{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496d79fb-c903-4145-9b40-d36d17edb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af97c6a-329c-404c-a440-ed775301e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from energyclustering.sampling.preprocessing import DataPreprocessor\n",
    "from dask.distributed import Client\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c716b-e34d-4ac5-ba5c-7e4e30a62c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887bbf1b-81e1-476d-97be-96814ef3a7ae",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a29e4fe-6d82-4fe1-b8de-e18bbe923600",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data_df, data_df, daily_info_df, weather_df = (\n",
    "    DataPreprocessor()\n",
    "    .preprocess_info_df('baseline')\n",
    "    .preprocess_weather_df('baseline')\n",
    "    .drop_days_with_nan(True)\n",
    "    .subsample_days(week_reduction_factor = None)\n",
    "    # for testing only!\n",
    "    .subsample_years(1000)\n",
    "    .get_data()\n",
    ")\n",
    "daily_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d320c4-31f7-443d-8087-a16f5fe8f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_info_df.loc[:, 'household_info']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d504fd5-1451-42bd-9b85-7c1fa53cf132",
   "metadata": {},
   "source": [
    "# Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ba479-37ee-49c4-8e4a-44b70d7b9782",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = np.random.default_rng(1)\n",
    "shuffled = data_df.index.to_numpy(copy=True)\n",
    "generator.shuffle(shuffled)\n",
    "folds = np.array_split(shuffled, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8436cea-403b-41de-a50e-2cfbb2a26507",
   "metadata": {},
   "source": [
    "## Clusterers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315823d2-2e19-4a02-98f1-60f13045a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from numba import jit, float64\n",
    "from dtaidistance import dtw\n",
    "import kmedoids\n",
    "from energyclustering.sampling.samplers import ConsumptionDataSampler, MetadataSampler, EnergyvilleDaySelectionBaseline, RandomSamplerBaseline\n",
    "from energyclustering.sampling.day_of_year_samplers import DailySamplerFromClusterSampler, GenerateSampleDecorator\n",
    "from energyclustering.clustering.preclustering import PreClusteringClusterer\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from pathlib import Path\n",
    "from energyclustering.sampling.evaluation.evaluation import SamplerEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25c52e-0c9e-421d-888c-8868ecbc533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(float64(float64[:], float64[:]), nogil = True, nopython = True)\n",
    "def euc_dist_missing(a1, a2): \n",
    "    return np.nanmean((a1-a2)**2)\n",
    "\n",
    "euc_distance_matrix_missing = lambda x: pairwise_distances(x, metric = euc_dist_missing)\n",
    "euc_distance_matrix = lambda x: euclidean_distances(x.fillna(0))\n",
    "dtw_distance_matrix = lambda x: dtw.distance_matrix_fast(x.to_numpy(), window = 4)\n",
    "\n",
    "class CustomKMedoids: \n",
    "    def __init__(self, nb_clusters, metric, random_state = None): \n",
    "        self.nb_clusters = nb_clusters\n",
    "        self.metric = metric\n",
    "        self.random_state = random_state\n",
    "        self.labels_ = None\n",
    "    \n",
    "    def fit(self, data):\n",
    "        matrix = self.metric(data) \n",
    "        km = kmedoids.KMedoids(self.nb_clusters, method = 'fasterpam', random_state = self.random_state) \n",
    "        c = km.fit(matrix)\n",
    "        self.labels_ = c.labels_.astype('int')\n",
    "        return self\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7beaca-63ce-4af0-8e9c-9f5d9bbd2894",
   "metadata": {},
   "source": [
    "## Models to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582b4f0-0249-4cc3-b893-64729705c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_YEARLY_CLUSTERS = 40\n",
    "NB_DAILY_CLUSTERS = 30 \n",
    "NB_SAMPLES = 250 \n",
    "\n",
    "models = dict() \n",
    "\n",
    "yearly_clustering = dict(\n",
    "    kmeans = KMeans(NB_YEARLY_CLUSTERS, random_state = 0), \n",
    "    euclidean = CustomKMedoids(NB_YEARLY_CLUSTERS, euc_distance_matrix, random_state = 0), \n",
    ")\n",
    "\n",
    "daily_clustering = dict(\n",
    "    kmeans = KMeans(NB_DAILY_CLUSTERS, random_state = 0), \n",
    "    euclidean = CustomKMedoids(NB_DAILY_CLUSTERS, euc_distance_matrix, random_state = 0),\n",
    "    euclidean_overcluster = PreClusteringClusterer(\n",
    "        pre_clusterer = MiniBatchKMeans(n_clusters = 5000, batch_size = 500*40), \n",
    "        post_clusterer = CustomKMedoids(NB_DAILY_CLUSTERS, euc_distance_matrix, random_state = 0)\n",
    "    ), \n",
    "    dtw = CustomKMedoids(NB_DAILY_CLUSTERS, dtw_distance_matrix, random_state = 0), \n",
    "    \n",
    ")\n",
    "for y_name, y_cluster in yearly_clustering.items(): \n",
    "    for d_name, d_cluster in daily_clustering.items():\n",
    "        models[f'y={y_name}, d={d_name}'] = (\n",
    "            GenerateSampleDecorator(\n",
    "                DailySamplerFromClusterSampler(\n",
    "                        yearly_sampler = ConsumptionDataSampler(\n",
    "                            classifier = RandomForestClassifier(), \n",
    "                            clusterer = y_cluster, \n",
    "                            info_preprocessing = None\n",
    "                        ), \n",
    "                        daily_sampler = ConsumptionDataSampler(\n",
    "                            classifier = RandomForestClassifier(), \n",
    "                            clusterer = d_cluster, \n",
    "                            info_preprocessing = None\n",
    "                        )\n",
    "                    ), \n",
    "                n_samples = NB_SAMPLES)\n",
    "        )\n",
    "\n",
    "# manually add an entry for euclidean missing with euclidean distance\n",
    "models[f'y=euclidean_missing, d=euclidean'] = (\n",
    "            GenerateSampleDecorator(\n",
    "                DailySamplerFromClusterSampler(\n",
    "                        yearly_sampler = ConsumptionDataSampler(\n",
    "                            classifier = RandomForestClassifier(), \n",
    "                            clusterer = CustomKMedoids(NB_YEARLY_CLUSTERS, euc_distance_matrix_missing, random_state = 0), \n",
    "                            info_preprocessing = None\n",
    "                        ), \n",
    "                        daily_sampler = ConsumptionDataSampler(\n",
    "                            classifier = RandomForestClassifier(), \n",
    "                            clusterer =  CustomKMedoids(NB_DAILY_CLUSTERS, euc_distance_matrix, random_state = 0), \n",
    "                            info_preprocessing = None\n",
    "                        )\n",
    "                    ), \n",
    "                n_samples = NB_SAMPLES)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f2b81-d639-4e21-94f9-e58efb9433b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc809d-4179-47a7-8807-031090380a64",
   "metadata": {},
   "source": [
    "## Do the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8ccc5-bc56-4ec1-b773-cb723bb05dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "energy_scores = []\n",
    "\n",
    "result_path = Path()/'results'/'daily_sampling'\n",
    "result_path.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "with Client(local_directory = '/cw/dtailocal/', n_workers=20, threads_per_worker = 1) as client:\n",
    "    evaluator = SamplerEvaluator(folds, daily_data_df, daily_info_df, data_df, client, 400, crossval = False)\n",
    "    for key, model in models.items():\n",
    "        energy_score = evaluator.evaluate_and_save(model, result_path/f\"{key}.pkl\")\n",
    "        energy_scores.append(energy_score)\n",
    "    energy_scores = pd.concat(energy_scores, axis = 1, keys = models.keys())\n",
    "\n",
    "# aggregate energy scores\n",
    "plot_df = energy_scores.agg(['mean', 'std'], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7b859-adb2-4500-a117-712f66afda4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e670b98-944d-4956-892f-26df671e92b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energyville",
   "language": "python",
   "name": "energyville"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
