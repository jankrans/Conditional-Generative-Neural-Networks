{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Eandis 2017 data\n",
    "in the folder \"**_DATA Eandis 20170712 VREG study complete_**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import tqdm\n",
    "import pyxlsb #Excel 2007-2010 Binary Workbook (xlsb) parser for Python.\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import zlib #this module allow data compression and decompression,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH to the profile directory in the fluvius data\n",
    "DATA_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/Data-2020-11/FluviusData/profiles')\n",
    "# DATA_PATH = Path('/Users/lolabotman/PycharmProjects/FluviusFullData/profiles') #Path Lola\n",
    "\n",
    "PATH_EANDIS2017 = DATA_PATH / 'DATA Eandis 20170712 VREG study complete/'\n",
    "\n",
    "# PATH to where the preprocessed files should be appear\n",
    "PREPROCESSED_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/new_preprocessed/eandis2017')\n",
    "# PREPROCESSED_PATH = Path('/Users/lolabotman/PycharmProjects/FluviusFullData/profiles/preprocessed/eandis2017') #Path Lola\n",
    "PREPROCESSED_PATH.mkdir(mode = 0o770, parents = True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the contents of the zipped files and the other files are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_zip = 'VITO_VREG_EXPORT.zip'\n",
    "\n",
    "def crc32(fileName):\n",
    "    with open(fileName, 'rb') as fh:\n",
    "        hash = 0\n",
    "        while True:\n",
    "            s = fh.read(65536)\n",
    "            if not s:\n",
    "                break\n",
    "            hash = zlib.crc32(s, hash)\n",
    "        # return \"%08X\" % (hash & 0xFFFFFFFF)\n",
    "        return hash\n",
    "\n",
    "with ZipFile(PATH_EANDIS2017 / file_zip, 'r') as zp:\n",
    "    print('Contents of the .ZIP file:\\n')\n",
    "    zp.printdir()\n",
    "    print('')\n",
    "    \n",
    "    for fl in zp.filelist:\n",
    "        if crc32(PATH_EANDIS2017 / fl.filename) == fl.CRC:\n",
    "            print(f'✓ The file {fl.filename} is consistent.')\n",
    "        else:\n",
    "            print(f'✘ The file {fl.filename} is different in the .ZIP file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_info = 'MASTER_TABLE_METERS.csv'\n",
    "file_data = 'READINGS_2016.csv'\n",
    "\n",
    "info_df = pd.read_csv(PATH_EANDIS2017 / file_info, sep=';', decimal=',')\n",
    "data_df = pd.read_csv(PATH_EANDIS2017 / file_data, sep=';', decimal=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format of the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.iloc[1250,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the dates are in day/month/year format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df.replace({'Ja':True, 'Nee':False}, inplace=True)\n",
    "\n",
    "data_df.replace({'Elektriciteit':'electricity', 'Injectie':'injection', 'Afname':'offtake'}, inplace=True)\n",
    "data_df.columns = ['type', 'meterID', 'measurement type', 'timestamp', 'unit', 'measurement', 'status']\n",
    "\n",
    "# data_df['timestamp'] = pyxlsb.convert_date(data_df['timestamp']) # does not work\n",
    "data_df['timestamp'] = pd.to_datetime(data_df['timestamp'], format='%d/%m/%Y %H:%M:%S') # too slow when format is not specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove redundant parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop(columns=['type', 'status', 'unit'], inplace=True) # values in these columns are constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summer/Winter time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30 Oktober an hour passes twice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = data_df[data_df['measurement type'] == 'offtake'] #Select only offtake values \n",
    "\n",
    "occurence_count = temp_df.groupby('meterID')['timestamp'].value_counts() #count the number of time each timestamp is present (per meterid)\n",
    "duplicate_values = occurence_count[occurence_count > 1] #select the rows for which the count is above 1 (meaning duplicates)\n",
    "\n",
    "duplicate_values.to_frame('count').reset_index().drop_duplicates(subset = ['timestamp', 'count']) #show only the duplicates ones (same dates/hour for each smart meterid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise the duplicate values in offtake (visual check for the next ste[ in the handling of the duplicates])\n",
    "visualize_dup = data_df.loc[data_df.meterID=='Sl2clpW0mYpJ3w'].set_index(['measurement type','timestamp']).loc['offtake'].loc['2016-10-30 2:00':'2016-10-30 3:00']\n",
    "visualize_dup.sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for all of the smart meters, on the 30th of october 2016, the timestamps 2:00, 1:15, 2:30 and 2:45 occur twice. It is due to the clock change, at 3am, we decide to go back to 2am. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27 March an hour is skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = data_df[data_df.meterID == data_df.meterID.iloc[0]] #select only one smart meter (the first one)\n",
    "temp_df = temp_df[temp_df['measurement type'] == 'offtake'] #select only the offtake\n",
    "temp_df = temp_df.set_index('timestamp') #set the index to the timestamps\n",
    "temp_df.loc['2016-3-27 1:00': '2016-3-27 4:00'] #show the values between 1 and 4 am of the 27th of march "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that we have no rows for timestamps 2:00, 2:15, 2:30 and 2:45. It is due to the clock change forward, at 2 am, we change the clock to 3 am. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle duplicate measurements and calculate consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processed_df = pd.pivot_table(data_df, index=['meter ID', 'timestamp'], \\\n",
    "#                                    columns='measurement type', values='measurement', aggfunc=np.sum) # JONAS This is a sum but should be mean? \n",
    "\n",
    "#pivot table and averages out duplicates, missing timestamps are still missing \n",
    "data_processed_df = pd.pivot_table(data_df, index=['meterID', 'timestamp'], \\\n",
    "                                   columns='measurement type', values='measurement', aggfunc=np.mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed_df.loc['Sl2clpW0mYpJ3w'].loc['2016-3-27 1:00':'2016-3-27 4:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed_df.loc['Sl2clpW0mYpJ3w'].loc['2016-10-30 2:00':'2016-10-30 3:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in zeros because Nan - x = NaN and compute the consumption value = offtake - injection\n",
    "data_processed_df['consumption'] = data_processed_df['offtake'].fillna(0) - data_processed_df['injection'].fillna(0)\n",
    "\n",
    "# make consumption NaN when there are no measurements\n",
    "data_processed_df['consumption'].loc[data_processed_df['offtake'].isna() & data_processed_df['injection'].isna()] = np.nan\n",
    "\n",
    "# add rows with NaN for missing timestamps\n",
    "data_processed_df = data_processed_df.reindex(pd.MultiIndex.from_product([data_processed_df.index.levels[0], pd.date_range('2016-01-01', '2017-01-01', freq = '15min')[:-1]]))\n",
    "data_processed_df.index.names = ['meterID', 'timestamp']\n",
    "data_processed_df.sort_index(inplace = True)\n",
    "data_processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed_df.loc['Sl2clpW0mYpJ3w'].loc['2016-3-27 1:00':'2016-3-27 4:00'] #visual inspection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed_df.loc['Sl2clpW0mYpJ3w'].loc['2016-10-30 2:00':'2016-10-30 3:00'] #visual inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make nice info table (consistent with the rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_info_df = info_df.drop(columns = ['YEARLY_INJECTION', 'YEARLY_CONSUMPTION', 'YEARLY_BALANCE'])\n",
    "clean_info_df.rename(columns = {'EQUIPMENT_ID':'meterID', 'CONTRACTED_POWER':'connection_power','TIME_OF_USE':'tarif_type', 'LOCAL_PROD_POWER':'PV_power', 'LOCAL_PROD': 'PV', 'CATEGORY': 'category', 'METERS_INSTALLED':'installed_meters', 'LOCATION_ID':'locationID'}, inplace = True)\n",
    "clean_info_df['tarif_type'] = clean_info_df['tarif_type'].replace({'THNUTHNUTE':'single tarif', 'THNUTHNU':'single tarif', 'HILOHILO':'two tarifs', 'HILOHILOTE': 'two tarifs', 'EXNUEXNU':'ex_night', 'EXNUEXNUTE':'ex_night'})\n",
    "clean_info_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are exclusive night meters at the same location as another meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_night_meters = clean_info_df.loc[clean_info_df.tarif_type == 'ex_night']\n",
    "ex_night_meter_locations = ex_night_meters['locationID']\n",
    "ex_night_location_meters = clean_info_df.loc[clean_info_df['locationID'].isin(ex_night_meter_locations)].set_index(['locationID', 'meterID']).sort_index()\n",
    "ex_night_location_meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignore locations with exclusive night meters for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_info_df = clean_info_df[~ clean_info_df['locationID'].isin(ex_night_meter_locations)]\n",
    "clean_info_df.set_index('meterID', inplace = True)\n",
    "clean_info_df.drop(columns = ['installed_meters', 'locationID'],  inplace = True)\n",
    "clean_info_df['data_source'] = 'EandisVREG'\n",
    "clean_info_df['year'] = 2016\n",
    "clean_info_df.set_index('year', append = True, inplace = True)\n",
    "clean_info_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make nice pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_df = data_processed_df.drop(columns = ['injection', 'offtake'])\n",
    "clean_data_df = clean_data_df.reset_index()\n",
    "clean_data_df = pd.pivot_table(clean_data_df, index = 'meterID', columns = 'timestamp', values = 'consumption')\n",
    "# here as well exclude the exclusive night meters\n",
    "clean_data_df = clean_data_df.loc[clean_data_df.index.isin(clean_info_df.index.levels[0])]\n",
    "# make the index consistent with the other df's\n",
    "clean_data_df['year'] = 2016\n",
    "clean_data_df.set_index('year', append = True, inplace = True)\n",
    "# make sure the columns are datetime\n",
    "clean_data_df.columns = pd.to_datetime(clean_data_df.columns)\n",
    "clean_data_df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The timestamps of the missing hour are skipped in the clean data_df!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_df.loc[:, '2016-03-27 1:00': '2016-03-27 4:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df.to_csv(PREPROCESSED_PATH / 'full_info.csv')\n",
    "data_df.to_csv(PREPROCESSED_PATH / 'raw_data.csv')\n",
    "data_processed_df.to_csv(PREPROCESSED_PATH/ 'processed_data.csv')\n",
    "clean_info_df.to_csv(PREPROCESSED_PATH / 'clean_info_no_night.csv')\n",
    "clean_data_df.to_csv(PREPROCESSED_PATH / 'clean_data_no_night.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
