{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesses infrax part of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The appartement part of this dataset has a missing hour due to summer winter time  \n",
    "The other parts are in UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import tqdm\n",
    "import pyxlsb\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOLA = False\n",
    "if LOLA: \n",
    "    DATA_PATH = Path('/Users/lolabotman/PycharmProjects/FluviusFullData/profiles') #Path Lola\n",
    "    PREPROCESSED_PATH = Path('/Users/lolabotman/PycharmProjects/FluviusFullData/profiles/preprocessed/infrax')#Path Lola\n",
    "else: \n",
    "    # PATH to the profile directory in the fluvius data\n",
    "    DATA_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/Data-2020-11/FluviusData/profiles')\n",
    "    # PATH to where the preprocessed files should be appear\n",
    "    PREPROCESSED_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/new_preprocessed/infrax')\n",
    "\n",
    "PREPROCESSED_PATH.mkdir(mode = 0o770, parents = True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse code\n",
    "This is simply all the code to parse every kind of dataset (not so clean I know)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting a value to a datetime format  \n",
    "def to_timestamp(index): \n",
    "#     return index.apply(lambda value: pyxlsb.convert_date(value) if not np.isnan(value) else value).round('1min')\n",
    "    return [pd.to_datetime(pyxlsb.convert_date(value)).round('1min') if not np.isnan(value) else value for value in index]\n",
    "\n",
    "# transform the data and save the transformed data using the functions according to the parse data dict\n",
    "def transform_and_save(source_path, name, parse_function): \n",
    "    info_path = PREPROCESSED_PATH / f\"{name}_info.csv\"\n",
    "    data_path = PREPROCESSED_PATH / f\"{name}_data.csv\"\n",
    "    if not( info_path.exists() and data_path.exists()):\n",
    "        try:\n",
    "            info_df, data_df = parse_function(source_path)\n",
    "            info_df.to_csv(info_path)\n",
    "            data_df.to_csv(data_path)\n",
    "            print(data_df.shape)\n",
    "            assert info_path.exists() \n",
    "            assert data_path.exists()\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    else: \n",
    "        print(f'{name} already preprocessed')\n",
    "\n",
    "# In these files, the 24 first rows are meta data and the time series starts on row 26\n",
    "# There are 30 EANs and hourly measurements for a year (2015 file and 2014 file)\n",
    "# Hypothesis : measurement values are in kW\n",
    "def read_infrax_gas(path): \n",
    "    df = pd.read_excel(path, header = None, parse_dates = True)\n",
    "    df = df.set_index(df.columns[0])\n",
    "    smart_meter_df = df.T\n",
    "    smart_meter_df = smart_meter_df.set_index('EAN_coded')\n",
    "    \n",
    "    # info_df (we get the meta data from the first 24 columns - table has been transposed)\n",
    "    info_df = smart_meter_df.iloc[:,:24]\n",
    "    info_df = info_df.loc[:, ~ info_df.columns.isna()]\n",
    "    info_df = info_df.dropna(how='all', axis = 1) # drop columns with all NaN's\n",
    "    info_df = info_df.set_index('Jaar', append=True)\n",
    "    \n",
    "    #data_df (we get the actual value data from after column 24 )\n",
    "    data_df = smart_meter_df.iloc[:,24:].copy()\n",
    "    data_df.columns = pd.to_datetime(data_df.columns).round('1min')\n",
    "    data_df['Jaar'] = info_df.Jaar\n",
    "    data_df = data_df.set_index(['Jaar'], append = True)\n",
    "    data_df = data_df.sort_index()\n",
    "    \n",
    "    assert info_df.index.is_unique, 'info_df index should be unique'\n",
    "    assert data_df.index.is_unique, 'data_df index should be unique'\n",
    "\n",
    "    # sort on index\n",
    "    info_df = info_df.sort_index()\n",
    "    data_df = data_df.sort_index()\n",
    "    \n",
    "    \n",
    "    return info_df, data_df \n",
    "\n",
    "# In these files, the 24 first rows are meta data and the time series starts on row 27\n",
    "# there are 8 smart meter ids (EAN)\n",
    "# yearly total is in kWh \n",
    "# measurement values are in kW\n",
    "def read_infrax_heatpump(path): \n",
    "    df = pd.read_excel(path, header = None, parse_dates = True)\n",
    "    df = df.set_index(df.columns[0])\n",
    "    smart_meter_df = df.T\n",
    "    smart_meter_df = smart_meter_df.set_index('EAN_coded')\n",
    "\n",
    "    # info_df \n",
    "    info_df = smart_meter_df.iloc[:,:25]\n",
    "    info_df = info_df.loc[:, ~ info_df.columns.isna()]\n",
    "    info_df = info_df.dropna(how='all', axis = 1) # drop columns with all NaN's\n",
    "    info_df = info_df.set_index('Jaar', append=True)\n",
    "    \n",
    "    #data_df \n",
    "    data_df = smart_meter_df.iloc[:,25:].copy()\n",
    "#     print(\"doing a possibly WRONG conversion to date time\")\n",
    "    data_df.columns = pd.to_datetime(data_df.columns, format = \"%d/%b %H:%M\").round('1min')\n",
    "    data_df['Jaar'] = info_df.index.get_level_values('Jaar')\n",
    "    data_df = data_df.set_index(['Jaar'], append = True)\n",
    "    data_df = data_df.sort_index()\n",
    "    \n",
    "    assert info_df.index.is_unique, 'info_df index should be unique'\n",
    "    assert data_df.index.is_unique, 'data_df index should be unique'\n",
    "\n",
    "    # sort on index\n",
    "    info_df = info_df.sort_index()\n",
    "    data_df = data_df.sort_index()\n",
    "    \n",
    "    return info_df, data_df \n",
    "\n",
    "# Yearly total in kwh \n",
    "# hyp : Measurement values in kw >> deduced from the fact that (sum of the measurement) = 4*(jaarverbuik in kwh) + max value given in kW\n",
    "def read_infrax_app_xlsb(path): \n",
    "    # no useful index! \n",
    "    df = pd.read_excel(path, header = None, engine='pyxlsb')\n",
    "    df.set_index(df.columns[0], inplace = True)\n",
    "    smart_meter_df = df.T\n",
    "   \n",
    "    #info df \n",
    "    info_df = (\n",
    "    smart_meter_df\n",
    "        .iloc[:,:5] # info columns\n",
    "        .loc[:,~smart_meter_df.columns[:5].isna()] # drop nan columns\n",
    "    )\n",
    "    info_df = info_df.rename(index=lambda s: 'app2_'+ str(s)) #changing to 'unique' index to not mix up with the app1\n",
    "    info_df['Jaar']=[2014]*len(info_df) ##extract 2014 anoher way ? more generic ?\n",
    "    info_df = info_df.set_index('Jaar', append=True)\n",
    "    info_df = info_df.dropna(how = 'all') # for some reason there are some NaN rows\n",
    "    info_df = info_df.rename(columns={'Max (kW)':'Piek P (kW)'}) #rename such as to have the same column title as the other dfs\n",
    "    \n",
    "    # data_df\n",
    "    data_df = smart_meter_df.iloc[:,5:].copy()\n",
    "    data_df.columns = to_timestamp(data_df.columns)\n",
    "    data_df = data_df.dropna(how = 'all') # for some reason there are some NaN rows\n",
    "    data_df.columns = data_df.columns.round('1min')\n",
    "    data_df = data_df.rename(index=lambda s: 'app2_'+ str(s))\n",
    "    data_df['Jaar'] = [int(2014)]*len(data_df)\n",
    "    data_df = data_df.set_index(['Jaar'], append = True)  \n",
    "    data_df = data_df.loc[:,pd.to_datetime(data_df.columns).year == 2014] #there is one day of 2015 that we don't want to keep\n",
    "    data_df.columns = data_df.columns.map(lambda t: t.replace(year=2016)) #set the columns to 2016 for the final merge \n",
    "    data_df.columns = data_df.columns.round('1min')\n",
    "    data_df = data_df.astype('float')\n",
    "    \n",
    "    # duplicate hour take the mean\n",
    "    data_df = data_df.groupby(by=data_df.columns, axis = 1).mean()\n",
    "    \n",
    "    assert info_df.index.is_unique, 'info_df index should be unique'\n",
    "    assert data_df.index.is_unique, 'data_df index should be unique'\n",
    "    assert info_df.columns.is_unique, 'info_df columns should be unique'\n",
    "    assert data_df.columns.is_unique, 'data_df columns should be unique'\n",
    "\n",
    "    # sort on index\n",
    "    info_df = info_df.sort_index()\n",
    "    data_df = data_df.sort_index()\n",
    "    return info_df, data_df\n",
    "\n",
    "# hyp : all measurement values in kW >> deduced from the fact that (sum of the measurement) = 4*(jaarverbuik in kwh) \n",
    "# jaar verbruik in kWh\n",
    "def read_infrax_app_xlsx(path): \n",
    "    df = pd.read_excel(path, header = None, parse_dates = True)\n",
    "    df = df.set_index(df.columns[0])\n",
    "    smart_meter_df = df.T\n",
    "\n",
    "    # info_df\n",
    "    info_df = smart_meter_df.iloc[:,:7]\n",
    "    info_df = info_df.loc[:, ~ info_df.columns.isna()]\n",
    "    info_df = info_df.rename(index=lambda s: 'app1_'+ str(s)) #changing to 'unique' index to not mix up with the app1\n",
    "    info_df['Jaar']=[int(2014)]*len(info_df) ##extract 2014 anoher way ? more generic ?\n",
    "    info_df = info_df.set_index('Jaar', append=True)\n",
    "    info_df = info_df.rename(columns={'Max (kW)':'Piek P (kW)'})#rename such as to have the same column title as the other dfs\n",
    "    \n",
    "    # data_df\n",
    "    data_df = smart_meter_df.iloc[:,7:].copy()\n",
    "    data_df = data_df.dropna(how = 'all')\n",
    "    data_df.columns = pd.to_datetime(data_df.columns, format = '%m/%d/%Y %H:%M').round('1min')\n",
    "    data_df = data_df.rename(index=lambda s: 'app1_'+ str(s))\n",
    "    data_df['Jaar'] = [2014]*len(data_df)\n",
    "    data_df = data_df.set_index(['Jaar'], append = True)\n",
    "    data_df = data_df.loc[:,pd.to_datetime(data_df.columns).year == 2014] #there is one day of 2015 that we don't want to keep\n",
    "    data_df.columns = data_df.columns.map(lambda t: t.replace(year=2016)) #set the columns to 2016 for the final merge \n",
    "    data_df = data_df.astype('float')\n",
    "    \n",
    "    # duplicate hour take the mean\n",
    "    data_df = data_df.groupby(by=data_df.columns, axis = 1).mean()\n",
    "    \n",
    "    assert info_df.index.is_unique, 'info_df index should be unique'\n",
    "    assert data_df.index.is_unique, 'data_df index should be unique'\n",
    "    assert info_df.columns.is_unique, 'info_df columns should be unique'\n",
    "    assert data_df.columns.is_unique, 'data_df columns should be unique'\n",
    "\n",
    "    # sort on index\n",
    "    info_df = info_df.sort_index()\n",
    "    data_df = data_df.sort_index()\n",
    "    return info_df, data_df\n",
    "\n",
    "# all other files ending in _coded.xlsb \n",
    "# measurment values in kW\n",
    "# yearly total in kWh\n",
    "def read_infrax_data(path):\n",
    "    df = pd.read_excel(path, engine='pyxlsb')\n",
    "    df.set_index(df.columns[0], inplace = True)\n",
    "    smart_meter_df = df.T\n",
    "    smart_meter_df.set_index('EAN_coded', inplace = True)\n",
    "\n",
    "\n",
    "    # info df \n",
    "    info_df = (\n",
    "    smart_meter_df\n",
    "        .iloc[:,:21] # info columns\n",
    "        .loc[:,~smart_meter_df.columns[:21].isna()] # drop nan columns\n",
    "        .drop(columns = ['Info installatie', 'Info profiel'])\n",
    "    )\n",
    "\n",
    "    info_df['PV vermogen (kW)'] = info_df['PV vermogen (kW)'].replace('/', np.nan)\n",
    "    info_df = info_df[~ info_df.index.isna()] #remove row with nan index\n",
    "    info_df = info_df.reset_index()\n",
    "    info_df['EAN_coded'] = info_df['EAN_coded'].astype('int')\n",
    "    info_df = info_df.set_index(['EAN_coded', 'Jaar'])\n",
    "\n",
    "    # data df \n",
    "\n",
    "    data_df = smart_meter_df.iloc[:,23:].copy()\n",
    "    data_df.columns = to_timestamp(data_df.columns)\n",
    "    # drop the columns with NaT\n",
    "    data_df = data_df.loc[:,~data_df.columns.isna()]\n",
    "    data_df.columns = data_df.columns.round('1min')\n",
    "    data_df = data_df[~ data_df.index.isna()] #remove row with nan index\n",
    "    data_df = data_df.reset_index()\n",
    "    data_df['EAN_coded'] = data_df['EAN_coded'].astype('int')\n",
    "    data_df['Jaar'] = info_df.index.get_level_values(1)\n",
    "    data_df = data_df.set_index(['EAN_coded','Jaar'])\n",
    "\n",
    "    # Handle the ids 1290 en 1299 that have year 2013 twice \n",
    "    if (1290, 2013) in info_df.index: \n",
    "        new_info_df = info_df.reset_index()\n",
    "        new_info_df.loc[new_info_df['EAN_coded'].isin([1290,1299]) & new_info_df.duplicated(subset = ['EAN_coded', 'Jaar'], keep = 'first'), 'Jaar'] = 2012\n",
    "        info_df = new_info_df.set_index(['EAN_coded', 'Jaar'])\n",
    "        \n",
    "        new_data_df = data_df.reset_index()\n",
    "        new_data_df.loc[new_data_df['EAN_coded'].isin([1290,1299]) & new_data_df.duplicated(subset = ['EAN_coded', 'Jaar'], keep = 'first'), 'Jaar'] = 2012\n",
    "        data_df = new_data_df.set_index(['EAN_coded', 'Jaar'])\n",
    "        \n",
    "   \n",
    "    assert info_df.index.is_unique, 'info_df index should be unique'\n",
    "    assert data_df.index.is_unique, 'data_df index should be unique'\n",
    "\n",
    "    # sort on index\n",
    "    info_df = info_df.sort_index()\n",
    "    data_df = data_df.sort_index()\n",
    "    \n",
    "    return info_df, data_df \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# file information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is which parser function to use for which file \n",
    "parser_functions = {   \n",
    "    'Appartement1': read_infrax_app_xlsx,\n",
    "    'Appartement2': read_infrax_app_xlsb,\n",
    "#     'SLP_profiel S41 2014 (30)_coded': read_infrax_gas,\n",
    "#     'SLP_profiel S41 2015 (30)_coded': read_infrax_gas,\n",
    "    'SLPs_professionelen(348)_coded': read_infrax_data,\n",
    "    'SLPs_residentielen(1675)_coded': read_infrax_data,\n",
    "    'SLPs_residentiëlen(1675)_coded': read_infrax_data,\n",
    "    'Slimme meters met WP (en eventueel PV)_coded': read_infrax_heatpump,\n",
    "    'Slimme meters_professionelen(141)_coded': read_infrax_data,\n",
    "    'Slimme meters_prosumers(123)_coded': read_infrax_data,\n",
    "    'Slimme meters_residentielen(1080)_coded': read_infrax_data, \n",
    "    'Slimme meters_residentiëlen(1080)_coded': read_infrax_data\n",
    "}\n",
    "\n",
    "# this is which preprocessed file name to use \n",
    "new_filename = { \n",
    "    'Appartement1': 'app1',\n",
    "    'Appartement2': 'app2',\n",
    "    'SLP_profiel S41 2014 (30)_coded': 'SLP_gas_2014',\n",
    "    'SLP_profiel S41 2015 (30)_coded': 'SLP_gas_2015',\n",
    "    'SLPs_professionelen(348)_coded': 'SLP_prof',\n",
    "    'SLPs_residentielen(1675)_coded': 'SLP_resid',\n",
    "    'SLPs_residentiëlen(1675)_coded': 'SLP_resid',\n",
    "    'Slimme meters met WP (en eventueel PV)_coded': 'M_heatpump',\n",
    "    'Slimme meters_professionelen(141)_coded': 'M_prof',\n",
    "    'Slimme meters_prosumers(123)_coded': 'M_prosumers',\n",
    "    'Slimme meters_residentielen(1080)_coded': 'M_resid', \n",
    "    'Slimme meters_residentiëlen(1080)_coded': 'M_resid'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse it all :D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infrax_path = DATA_PATH/ \"20171219 Profielen Infrax\"\n",
    "translate = dict()\n",
    "for path in tqdm.tqdm(list(infrax_path.glob('**/*.xlsb'))+ list(infrax_path.glob('**/*.xlsx'))):\n",
    "    print(path)\n",
    "    if path.stem in parser_functions:\n",
    "        new_name = new_filename[path.stem]\n",
    "        parser = parser_functions[path.stem]\n",
    "        transform_and_save(path, new_name, parser)\n",
    "    else:\n",
    "        print('error:'+path.stem)\n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make combined dataframe of relevant profiles\n",
    "So these profiles are all in the same format so we can easily combine these!  \n",
    "I add some extra columns to the info dataframe to ensure that we can later recover the different groups if necessary.  \n",
    "Appartement is excluded and the gas information is excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_info_df():\n",
    "    files = ['M_resid_info.csv', 'SLP_resid_info.csv', 'M_prof_info.csv', 'SLP_prof_info.csv', 'M_prosumers_info.csv', 'M_heatpump_info.csv', 'app1_info.csv', 'app2_info.csv']\n",
    "    files = [PREPROCESSED_PATH/file for file in files]\n",
    "\n",
    "\n",
    "    M_heatpump = pd.read_csv(files[5], index_col = [0,1])\n",
    "    M_heatpump['heatpump'] = True\n",
    "\n",
    "\n",
    "    M_prosumers = pd.read_csv(files[4], index_col = [0,1])\n",
    "    M_prosumers = M_prosumers.dropna(how='all')\n",
    "    M_prosumers['prosumer'] = True\n",
    "\n",
    "    \n",
    "    M_prof_df = pd.read_csv(files[2], index_col = [0,1])\n",
    "    SLP_prof_df = pd.read_csv(files[3], index_col = [0,1])\n",
    "\n",
    "\n",
    "    M_resid_df = pd.read_csv(files[0], index_col = [0,1])\n",
    "    SLP_resid_df = pd.read_csv(files[1], index_col = [0,1])\n",
    "    \n",
    "    app1_df = pd.read_csv(files[6], index_col =[0,1])\n",
    "    app1_df['R/P']=['app1']*len(app1_df.index)\n",
    "    \n",
    "    app2_df = pd.read_csv(files[7], index_col = [0,1])\n",
    "    app2_df['R/P']=['app2']*len(app2_df.index)\n",
    "\n",
    "    infrax = pd.concat([M_resid_df, SLP_resid_df, M_prof_df, SLP_prof_df, M_heatpump, M_prosumers, app1_df, app2_df]).sort_index()\n",
    "    infrax.to_csv(PREPROCESSED_PATH/'combined_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE = True\n",
    "if not (PREPROCESSED_PATH/'combined_info.csv').exists() or OVERWRITE: \n",
    "    combined_info_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = pd.read_csv(PREPROCESSED_PATH/'combined_info.csv')\n",
    "comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE = True\n",
    "if not (PREPROCESSED_PATH/'combined_data.csv').exists() or OVERWRITE: \n",
    "    files = ['M_resid_info.csv', 'SLP_resid_info.csv', 'M_prof_info.csv', 'SLP_prof_info.csv', 'M_prosumers_info.csv', 'M_heatpump_info.csv','app1_info.csv', 'app2_info.csv']\n",
    "    profile_files = [PREPROCESSED_PATH/ f'{file[:-8]}data.csv' for file in files]\n",
    "    combined_data_df = pd.concat([pd.read_csv(file, index_col = [0,1]) for file in profile_files] )\n",
    "    combined_data_df = combined_data_df.dropna(how='all', axis = 0).sort_index()\n",
    "    combined_data_df = combined_data_df.reset_index()\n",
    "    combined_data_df['Jaar'] = combined_data_df['Jaar'].astype('int')\n",
    "    combined_data_df = combined_data_df.set_index(['EAN_coded','Jaar'])\n",
    "    combined_data_df.to_csv(PREPROCESSED_PATH/'combined_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the 2 appartment files have not been treated, because we can still notice the artefacts (duplicate values in october and missing values in march) >> let's remove this before checking the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make clean info df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = pd.read_csv(PREPROCESSED_PATH/'combined_info.csv')\n",
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_info_df = (\n",
    "    info_df\n",
    "    # remove two NaN rows\n",
    "    .dropna(axis = 0, how = 'all', subset = ['EAN_coded'])\n",
    "    # drop unnecessary columns \n",
    "    .drop(columns = ['DNB', 'Automaat', 'Bron SLP of SM?', 'Meting of synthetisch?', 'nacht/dag ratio', 'Piek P (kW)', 'Jaarverbruik (kWh)', 'Gebruiksduur'])\n",
    "    # rename some columns\n",
    "    .rename(columns = {'EAN_coded':'meterID', 'R/P':'consumer_type', 'Jaar':'year', 'Gemeente':'town', 'Postcode':'postal_code', 'SLP cat': 'SLP_cat','#gezinsleden':'#family_members', 'Aansluitvermogen (kVA)':'connection_power', 'PV?':'PV', 'PV vermogen (kW)':'PV_power'})\n",
    "    .astype({'year':'int'})\n",
    "    .set_index(['meterID', 'year'])\n",
    "    .fillna({'prosumer': False, 'heatpump':False})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_info_df['PV'] = clean_info_df['PV'].replace({'N':False, 'Y': True})\n",
    "clean_info_df['consumer_type'] = clean_info_df['consumer_type'].replace({'R':'residential', 'P':'professional'})\n",
    "clean_info_df['data_source'] = 'Infrax'\n",
    "clean_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change connection capacity from kVA to kWh\n",
    "**TODO: to do this conversion we would need a power factor**: https://www.adeltd.co.uk/info/what-is-kva.php \n",
    "\n",
    "\n",
    "assumption 1kVa = 1kW (CONFIRMED by reinhilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change PV power from kW to kWh? \n",
    "Is this necessary? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_info_df.to_csv(PREPROCESSED_PATH/ 'clean_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make clean data df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(PREPROCESSED_PATH/'combined_data.csv')\n",
    "# data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_df = (\n",
    "    data_df\n",
    "    .rename(columns = {'EAN_coded': 'meterID', 'Jaar':'year'})\n",
    "    .astype({'year': 'int'})\n",
    "    .set_index(['meterID', 'year'])\n",
    ")\n",
    "clean_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_df.columns = pd.to_datetime(clean_data_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for summer/winter time issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_df.loc[:, '2016-3-29 1:00': '2016-3-29 4:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_df.loc[:, '2016-10-10 1:00': '2016-10-10 4:00' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert measurements from kW to kWh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_df = clean_data_df / 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_df.to_csv(PREPROCESSED_PATH/'clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_df.iloc[:,-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate duplicates issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/Data-2020-11/FluviusData/profiles/20171219 Profielen Infrax/1. Consumenten/1. Residentiële profielen/SLPs_residentiëlen(1675)_coded.xlsb')\n",
    "df = pd.read_excel(path, engine='pyxlsb')\n",
    "df.set_index(df.columns[0], inplace = True)\n",
    "\n",
    "smart_meter_df = df.T\n",
    "smart_meter_df.set_index('EAN_coded', inplace = True)\n",
    "\n",
    "smart_meter_df\n",
    "\n",
    "# info df \n",
    "info_df = (\n",
    "smart_meter_df\n",
    "    .iloc[:,:21] # info columns\n",
    "    .loc[:,~smart_meter_df.columns[:21].isna()] # drop nan columns\n",
    "    .drop(columns = ['Info installatie', 'Info profiel'])\n",
    ")\n",
    "info_df['PV vermogen (kW)'] = info_df['PV vermogen (kW)'].replace('/', np.nan)\n",
    "info_df = info_df[~ info_df.index.isna()] #remove row with nan index\n",
    "info_df = info_df.reset_index()\n",
    "info_df['EAN_coded'] = info_df['EAN_coded'].astype('int')\n",
    "info_df = info_df.set_index(['EAN_coded', 'Jaar'])\n",
    "info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indices that occur twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = info_df.index.value_counts()\n",
    "duplicate_counts = counts[counts>1]\n",
    "duplicate_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_info_df = info_df.loc[duplicate_counts.index]\n",
    "duplicate_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df.loc[[1290, 1299],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df.index.get_level_values(1).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at corresponding profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data df \n",
    "data_df = smart_meter_df.iloc[:,23:].copy()\n",
    "data_df.columns = to_timestamp(data_df.columns)\n",
    "# drop the columns with NaT\n",
    "data_df = data_df.loc[:,~data_df.columns.isna()]\n",
    "data_df.columns = data_df.columns.round('1min')\n",
    "data_df = data_df[~ data_df.index.isna()] #remove row with nan index\n",
    "data_df = data_df.reset_index()\n",
    "data_df['EAN_coded'] = data_df['EAN_coded'].astype('int')\n",
    "data_df['Jaar'] = info_df.index.get_level_values(1)\n",
    "data_df = data_df.set_index(['EAN_coded','Jaar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_data_df = data_df.loc[duplicate_counts.index]\n",
    "duplicate_data_df\n",
    "\n",
    "temp_df = duplicate_data_df.reset_index().astype({'EAN_coded':'str'})\n",
    "temp_df.loc[temp_df.duplicated(subset = ['EAN_coded', 'Jaar'], keep = 'last'), 'EAN_coded'] = temp_df.loc[temp_df.duplicated(subset = ['EAN_coded', 'Jaar'], keep = 'last'), 'EAN_coded'] + 'duplicate'\n",
    "temp_df.set_index(['EAN_coded', 'Jaar'], inplace = True)\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_df = temp_df.stack().to_frame('value').reset_index().rename(columns = {'level_2':'timestamp'})\n",
    "vis_df.timestamp\n",
    "\n",
    "alt.Chart(vis_df, width = 2200).mark_line().encode(\n",
    "    x = 'timestamp:T',\n",
    "    y  = 'value:Q', \n",
    "    color = 'EAN_coded:N'\n",
    ").interactive(bind_y = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
