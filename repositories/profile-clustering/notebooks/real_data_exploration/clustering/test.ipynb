{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surprised-privacy",
   "metadata": {},
   "source": [
    "# NMF matching to compare profiles\n",
    "The steps of this algorithm\n",
    "## Figuring out the representation of each profile\n",
    "- Raw components are extracted using NMF on the days of each profile separately  \n",
    "- Problem: the components don't have a scale\n",
    "    - look into the coefficient matrix at all the non-zero coefficients that are used with this component \n",
    "    - make a kernel density estimation of these coefficients (just fixed bandwidth) and look for the local maxima in this density estimation \n",
    "    - these local maxima will be the scales that we use (so here we introduce an error) \n",
    "    - now each component that has multiple local maxima will be split up in different components each with one local maxima \n",
    "    - in this way we can solve the scale issue \n",
    "- So now we have the representation a set of (scaled) components, each component also keeps track of how much it is used in the profile \n",
    "\n",
    "## Comparing different profiles \n",
    "To compare different profiles, we are going to try to match the consumption with each other.  \n",
    "The distance between two profiles is then the total consumption that couldn't be matched under time warping.  \n",
    "This is implemented by greedily matching consumption untill no matches can be made anymore.  \n",
    "\n",
    "\n",
    "_note: the implementation is pretty expensive, it is iterative and lots of DTW distances need to be calculated_  \n",
    "But these are worries for later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-japanese",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this reloads code from external modules automatically \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from KDEpy import TreeKDE\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import datetime\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.decomposition import NMF\n",
    "idx = pd.IndexSlice\n",
    "alt.data_transformers.disable_max_rows()\n",
    "import warnings\n",
    "from component_matching import (\n",
    "    get_component_similarity,\n",
    "    get_day_df,\n",
    "    get_NMF,\n",
    "    scale_components_simple,\n",
    "    scale_components_discrete, \n",
    "    add_date\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-uzbekistan",
   "metadata": {},
   "source": [
    "# Read (subset of) the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/preprocessed/combined')\n",
    "info_path = PRE_PATH/'reindexed_info.csv'\n",
    "data_path = PRE_PATH/'reindexed_DST_data.csv'\n",
    "info_df = pd.read_csv(info_path, index_col = [0,1])\n",
    "data_df = pd.read_csv(data_path, index_col = [0,1], nrows = 100)\n",
    "data_df.columns = pd.to_datetime(data_df.columns)\n",
    "data_df.columns.name = 'timestamp'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-thriller",
   "metadata": {},
   "source": [
    "## Data subset to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.sample(20, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-rugby",
   "metadata": {},
   "source": [
    "## Calculate the distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_meters = data_df.shape[0]\n",
    "distance_matrix = np.zeros((nb_meters, nb_meters))\n",
    "for i,j in tqdm(itertools.product(range(0,nb_meters), range(0, nb_meters)), total = nb_meters*nb_meters): \n",
    "    distance_matrix[i,j]= get_component_similarity(data_df.iloc[i], data_df.iloc[j])\n",
    "distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-passenger",
   "metadata": {},
   "source": [
    "## Cluster the profiles using this metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = KMedoids(n_clusters = 2, metric = 'precomputed').fit(distance_matrix)\n",
    "labels = pd.Series(clusterer.labels_, index = data_df.index)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-roads",
   "metadata": {},
   "source": [
    "## Cluster sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    labels\n",
    "    .value_counts()\n",
    "    .to_frame('count')\n",
    "    .rename_axis(index = 'cluster_nb')\n",
    "    .sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-awareness",
   "metadata": {},
   "source": [
    "## Show the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_to_show = labels.index[labels == 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_profiles_of_cluster(cluster_idx): \n",
    "    profiles_to_show = labels.index[labels == cluster_idx]\n",
    "    plot_df = (\n",
    "        data_df.loc[profiles_to_show]\n",
    "        # drop the year level\n",
    "        .droplevel(1)\n",
    "        .stack()\n",
    "        .to_frame('value')\n",
    "        .reset_index()\n",
    "        .assign(\n",
    "            time = lambda x: add_date(x.timestamp.dt.time),\n",
    "            date = lambda x: x.timestamp.dt.date.astype('str')\n",
    "        )\n",
    "    )\n",
    "    return alt.Chart(plot_df, width = 600, height = 300).mark_line(size = 0.3).encode(\n",
    "        x = 'time:T', \n",
    "        y= 'value', \n",
    "        color = alt.Color('date', scale = alt.Scale(scheme = 'rainbow'), legend = None),\n",
    "    ).facet(row = 'meterID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_profiles_of_cluster(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_profiles_of_cluster(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-projector",
   "metadata": {},
   "source": [
    "# Choose two example profiles to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDX1, IDX2 = 0,1 #pretty similar\n",
    "# IDX1, IDX2 = 1,2 # similar\n",
    "IDX1, IDX2 = 1,4 # similar\n",
    "\n",
    "profile1 = data_df.iloc[IDX1]\n",
    "profile2 = data_df.iloc[IDX2]\n",
    "day_df1 = get_day_df(profile1)\n",
    "day_df2 = get_day_df(profile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_day_chart(day_df): \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_day_chart(day_df1).properties(title = 'Profile 1') & all_day_chart(day_df2).properties(title = 'Profile 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-campbell",
   "metadata": {},
   "source": [
    "# Calculate the NMF of each profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_component_similarity(profile1, profile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_component_similarity(profile2, profile1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-salmon",
   "metadata": {},
   "source": [
    "# Handle the scale "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-obligation",
   "metadata": {},
   "source": [
    "### Distribution plot of the coefficients of each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each component is used a couple of times with a large coefficient\n",
    "dfs = []\n",
    "for component_nb in components_df1.index:\n",
    "    component_values = representation_df1[component_nb].pipe(lambda x: x[x>0.03])\n",
    "    x, y = TreeKDE(bw = 0.01).fit(component_values.values).evaluate()\n",
    "    kde_df = (\n",
    "        pd.DataFrame()\n",
    "        .assign(\n",
    "            x = x, \n",
    "            y = y, \n",
    "            component_nb = component_nb\n",
    "        )\n",
    "    )\n",
    "    dfs.append(kde_df)\n",
    "    \n",
    "all_kde_dfs = pd.concat(dfs, axis = 0)\n",
    "alt.Chart(all_kde_dfs, title = 'distribution of the coefficients of each component').mark_line().encode(\n",
    "    x = 'x', \n",
    "    y = 'y',\n",
    "    color = 'component_nb:N'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-stack",
   "metadata": {},
   "source": [
    "### For now just use the maximum of the KDE (in some way the most common coefficient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_components(representation_df, components_df): \n",
    "    dfs = []\n",
    "    for component_nb in components_df.index:\n",
    "        component_values = representation_df[component_nb].pipe(lambda x: x[x>0.03])\n",
    "        x, y = TreeKDE(bw = 0.01).fit(component_values.values).evaluate()\n",
    "        kde_df = (\n",
    "            pd.DataFrame()\n",
    "            .assign(\n",
    "                x = x, \n",
    "                y = y, \n",
    "                component_nb = component_nb\n",
    "            )\n",
    "        )\n",
    "        dfs.append(kde_df)\n",
    "\n",
    "    all_kde_dfs = pd.concat(dfs, axis = 0)\n",
    "    most_common_coefficients = all_kde_dfs.groupby('component_nb')[['y', 'x']].max()['x']\n",
    "    scaled_components = components_df.multiply(most_common_coefficients, axis = 0)\n",
    "    times_used = (representation_df > 0.03).sum(axis = 0)\n",
    "    return scaled_components, times_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_components_df1, times_used1 = scale_components(representation_df1, components_df1)\n",
    "scaled_components_df2, times_used2 = scale_components(representation_df2, components_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.hconcat(all_day_chart(day_df1).properties(title = 'all days'), NMF_component_chart(scaled_components_df1).properties(title = 'scaled components'), NMF_component_chart(components_df1).properties(title = 'non scaled components')).resolve_scale(color = 'independent', y = 'shared').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.hconcat(all_day_chart(day_df2).properties(title = 'all days'), NMF_component_chart(scaled_components_df2).properties(title = 'scaled components'), NMF_component_chart(components_df2).properties(title = 'non scaled components')).resolve_scale(color = 'independent', y = 'shared').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-processor",
   "metadata": {},
   "source": [
    "# Matching algorithm\n",
    "The goal here is to compare two sets of scaled components which each have a time used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "(NMF_component_chart(scaled_components_df1) | NMF_component_chart(scaled_components_df2)).resolve_scale(color = 'independent', y = 'shared')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-provider",
   "metadata": {},
   "source": [
    "## Calculate all the aligned sequences and DTW distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dtaidistance import dtw\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComponentMatcher:\n",
    "    \"\"\"\n",
    "        A class to help keep track of all the necessary information to make the matching algorithm pretty simple\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,scaled_components_df1, times_used1, scaled_components_df2, times_used2): \n",
    "        # keeps track of the times used attribute of each component\n",
    "        self.times_used_dict = dict()\n",
    "        # keeps track of all the aligned sequences\n",
    "        self.aligned_sequence_dict = dict()\n",
    "        # keeps track of all the dtw distances\n",
    "        self.dtw_distances_dict = dict()\n",
    "        \n",
    "        # keeps track of the original components \n",
    "        self.original_components1 = {key: value.to_numpy() for key, value in scaled_components_df1.iterrows()}\n",
    "        self.original_components2 = {key: value.to_numpy() for key, value in scaled_components_df2.iterrows()}\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # integers to use for the next components \n",
    "        self.current_component_nb1 = scaled_components_df1.index.max()\n",
    "        self.current_component_nb2 = scaled_components_df2.index.max()\n",
    "        \n",
    "        # all components in set1\n",
    "        self.current_components1 = set(scaled_components_df1.index)\n",
    "        self.current_components2 = set(scaled_components_df2.index)\n",
    "        \n",
    "        # initialise the aligned sequence dict and dtw distances dict\n",
    "        for comp_nb1, comp_nb2 in itertools.product(scaled_components_df1.index, scaled_components_df2.index):\n",
    "            component1 = scaled_components_df1.loc[comp_nb1].to_numpy()\n",
    "            component2 = scaled_components_df2.loc[comp_nb2].to_numpy()\n",
    "            # added penalty to ensure no warping is preffered over warping \n",
    "            aligned_component1, best_path = dtw.warp(component1, component2, window = 4, penalty = 0.1)\n",
    "            dist = dtw.distance(component1, component2, window = 4, penalty = 0.1)\n",
    "            self.aligned_sequence_dict[(comp_nb1, comp_nb2)] = (aligned_component1, component2, best_path)\n",
    "            self.dtw_distances_dict[(comp_nb1, comp_nb2)] = dist\n",
    "        \n",
    "        # initialise the times_used_dict \n",
    "        for index, value in times_used1.iteritems(): \n",
    "            self.times_used_dict[(1,index)] = value \n",
    "        for index, value in times_used2.iteritems(): \n",
    "            self.times_used_dict[(2,index)] = value \n",
    "    \n",
    "    def one_set_empty(self): \n",
    "        return len(self.current_components1)==0 or len(self.current_components2)==0\n",
    "    \n",
    "    def get_non_empty_component_set(self): \n",
    "        assert self.one_set_empty()\n",
    "        if len(self.current_components1) == 0: \n",
    "            return [self.original_components2[idx] for idx in self.current_components2]\n",
    "        return [self.original_components1[idx] for idx in self.current_components1]\n",
    "        \n",
    "    def get_best_aligned_pair(self): \n",
    "        best_pair = min(self.dtw_distances_dict, key = self.dtw_distances_dict.get)\n",
    "        component1, component2, best_path = self.aligned_sequence_dict[best_pair]\n",
    "        times_used1 = self.times_used_dict[(1, best_pair[0])]\n",
    "        times_used2 = self.times_used_dict[(2, best_pair[1])]\n",
    "        return (best_pair[0], component1, times_used1), (best_pair[1], component2, times_used2), best_path\n",
    "    \n",
    "    def remove_component_from_set1(self,comp1): \n",
    "        component1_pairs = [(comp1, other_comp2) for other_comp2 in self.current_components2]\n",
    "        for pair_to_remove in component1_pairs: \n",
    "            self.aligned_sequence_dict.pop(pair_to_remove)\n",
    "            self.dtw_distances_dict.pop(pair_to_remove)\n",
    "        \n",
    "        self.times_used_dict.pop((1, comp1))\n",
    "        \n",
    "        self.current_components1.remove(comp1)\n",
    "        \n",
    "    def remove_component_from_set2(self,comp2): \n",
    "        component2_pairs = [(other_comp1, comp2) for other_comp1 in self.current_components1]\n",
    "        for pair_to_remove in component2_pairs: \n",
    "            self.aligned_sequence_dict.pop(pair_to_remove)\n",
    "            self.dtw_distances_dict.pop(pair_to_remove)\n",
    "            \n",
    "        self.times_used_dict.pop((2, comp2))\n",
    "        \n",
    "        self.current_components2.remove(comp2)\n",
    "        \n",
    "    def add_component_to_set1(self,component1, times_used): \n",
    "        comp1_idx = self.current_component_nb1 + 1\n",
    "        self.current_component_nb1 += 1\n",
    "        self.current_components1.add(comp1_idx)\n",
    "        \n",
    "        self.original_components1[comp1_idx] = component1\n",
    "        \n",
    "        self.times_used_dict[(1,comp1_idx)] = times_used\n",
    "        \n",
    "        for comp2_idx in self.current_components2: \n",
    "            component2 = self.original_components2[comp2_idx] \n",
    "            aligned_component1, _ = dtw.warp(component1, component2, window = 4, penalty = 0.1)\n",
    "            dist = dtw.distance(component1, component2, window = 4, penalty = 0.1)\n",
    "            self.aligned_sequence_dict[(comp1_idx, comp2_idx)] = (aligned_component1, component2)\n",
    "            self.dtw_distances_dict[(comp1_idx, comp2_idx)] = dist\n",
    "            \n",
    "   \n",
    "        \n",
    "    def add_component_to_set2(self,component2, times_used): \n",
    "        comp2_idx = self.current_component_nb2 + 1\n",
    "        self.current_component_nb2 += 1\n",
    "        self.current_components2.add(comp2_idx)\n",
    "        \n",
    "        self.original_components2[comp2_idx] = component2\n",
    "        \n",
    "        self.times_used_dict[(2,comp2_idx)] = times_used\n",
    "        \n",
    "        for comp1_idx in self.current_components1: \n",
    "            component1 = self.original_components1[comp1_idx] \n",
    "            aligned_component1, _ = dtw.warp(component1, component2, window = 4, penalty = 0.1)\n",
    "            dist = dtw.distance(component1, component2, window = 4, penalty = 0.1)\n",
    "            self.aligned_sequence_dict[(comp1_idx, comp2_idx)] = (aligned_component1, component2)\n",
    "            self.dtw_distances_dict[(comp1_idx, comp2_idx)] = dist\n",
    "    \n",
    "    def change_times_used_set1(self,comp1_idx, times_used): \n",
    "        self.times_used_dict[(1, comp1_idx)] = times_used\n",
    "        \n",
    "    def change_times_used_set2(self,comp2_idx, times_used): \n",
    "        self.times_used_dict[(2, comp2_idx)] = times_used \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "component_matcher = ComponentMatcher(scaled_components_df1, times_used1, scaled_components_df2, times_used2)\n",
    "while not component_matcher.one_set_empty(): \n",
    "    \n",
    "    (comp1_idx, component1, used1), (comp2_idx, component2, used2), warping_path = component_matcher.get_best_aligned_pair()\n",
    "\n",
    "    print(f\"Pair {comp1_idx}, {comp2_idx} used {used1}, {used2}\")\n",
    "    if used1 > used2: \n",
    "        diff = used1-used2\n",
    "        # add component 1 with times_used diff \n",
    "        component_matcher.change_times_used_set1(comp1_idx, diff)\n",
    "        component_matcher.remove_component_from_set2(comp2_idx)\n",
    "    elif used2 < used1: \n",
    "        diff = used2-used1\n",
    "        # add component 2 with times_used diff \n",
    "        component_matcher.change_time_used_set2(comp2_idx, diff)\n",
    "        component_matcher.remove_component_from_set1(comp1_idx)\n",
    "    else: \n",
    "        component_matcher.remove_component_from_set1(comp1_idx)\n",
    "        component_matcher.remove_component_from_set2(comp2_idx)\n",
    "        \n",
    "    used = min(used1, used2)\n",
    "\n",
    "    difference = component1 - component2\n",
    "    positive_sum = np.sum(difference[difference>0])\n",
    "    negative_sum = -np.sum(difference[difference<0])\n",
    "    if positive_sum > negative_sum: \n",
    "        # comp1 is the one to use \n",
    "        new_comp1 = difference\n",
    "        new_comp1[new_comp1 < 0] = 0\n",
    "        # add new comp1 with times_used used \n",
    "        component_matcher.add_component_to_set1(new_comp1, used)\n",
    "        distance += negative_sum\n",
    "    else: \n",
    "        new_comp2 = -difference\n",
    "        new_comp2[new_comp2 < 0] = 0\n",
    "        # add new comp2 with times_used used \n",
    "        component_matcher.add_component_to_set2(new_comp2, used)\n",
    "        distance += positive_sum \n",
    "left_over = component_matcher.get_non_empty_component_set()\n",
    "for component in left_over:   \n",
    "    distance += np.sum(left_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-clearance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
