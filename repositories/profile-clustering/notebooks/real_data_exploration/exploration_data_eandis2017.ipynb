{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Eandis 2017 data\n",
    "in the folder \"**_DATA Eandis 20170712 VREG study complete_**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/preprocessed/eandis2017')\n",
    "info_path = PREPROCESSED_PATH / 'info.csv'\n",
    "data_path = PREPROCESSED_PATH / 'data.csv'\n",
    "data_processed_path = PREPROCESSED_PATH / 'data_processed.csv'\n",
    "\n",
    "info_df = pd.read_csv(info_path, index_col=[0])\n",
    "print('info_df loaded.')\n",
    "\n",
    "data_df = pd.read_csv(data_path, index_col=[0])\n",
    "data_df['timestamp'] = pd.to_datetime(data_df['timestamp'], format='%Y-%m-%d %H:%M:%S') # too slow when format is not specified\n",
    "print('data_df loaded.')\n",
    "\n",
    "data_processed_df = pd.read_csv(data_processed_path, index_col=[0, 1])\n",
    "data_processed_df.reset_index(inplace=True)\n",
    "data_processed_df['timestamp'] = pd.to_datetime(data_processed_df['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "data_processed_df.set_index(['meter ID', 'timestamp'], inplace=True)\n",
    "data_processed_df.columns.name = 'measurement type'\n",
    "print('data_processed_df loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate stats on the problems with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_consecutive_values(x, val): # number of consecutive values (val) in the array x, provided as a list\n",
    "    if np.isnan(val):\n",
    "        m1 = np.r_[False, np.isnan(x), False]\n",
    "    else:\n",
    "        m1 = np.r_[False, x==val, False]\n",
    "    idx = np.flatnonzero(m1[:-1] != m1[1:])\n",
    "    return (idx[1::2]-idx[::2])\n",
    "\n",
    "def check_data(data_dfx, data_processed_dfx):\n",
    "    # Duplicates:\n",
    "    data_df_counts = pd.pivot_table(data_dfx, index=['meter ID', 'timestamp'], \\\n",
    "                                    columns='measurement type', values='measurement', aggfunc='count')\n",
    "    print(f'Number of duplicate {data_df_counts.columns.to_list()} readings for the same meter ID and timestamp:' \\\n",
    "          f' {(data_df_counts > 1).sum().values}')\n",
    "    print('\\nDuplicate readings with their number of occurrences:')\n",
    "    display(data_df_counts[(data_df_counts > 1).all(axis=1)])\n",
    "    inds_duplicate = data_df_counts.index[(data_df_counts > 1).all(axis=1).eq(True)].values\n",
    "    data_df_duplicates = data_dfx[data_dfx.set_index(['meter ID', 'timestamp']).index \\\n",
    "                                  .isin(data_df_counts.index[(data_df_counts > 1).all(axis=1).eq(True)])] \\\n",
    "                         .sort_values(['meter ID', 'timestamp'])\n",
    "    data_df_duplicates_same = data_df_duplicates.groupby(['meter ID', 'timestamp', 'measurement type'])['measurement'].nunique().eq(1)\n",
    "    x1 = len(data_df_duplicates_same)\n",
    "    x2 = np.sum(data_df_duplicates_same == True)\n",
    "    x3 = np.sum(data_df_duplicates_same == False)\n",
    "    print(f'Out of {x1} duplicates, {x2} ({x2/x1*100}%) are consistent and {x3} are not.')\n",
    "    \n",
    "    # Missing values:\n",
    "    #time_first, time_last = data_processed_dfx.index.get_level_values(1).sort_values()[[0, -1]]\n",
    "    #time_indices_full = pd.date_range(start=time_first, end=time_last, freq=\"15min\")\n",
    "    print('\\nNumber of missing values (consumption is considered as missing when both injection and offtake are missing):')\n",
    "    display(data_processed_dfx.isna().sum())\n",
    "    print('Maximum number of missing values for a meter:')\n",
    "    display(data_processed_dfx.groupby(level=0).agg(lambda o: np.sum(pd.isna(o))).max())\n",
    "    print('Maximum number of consecutive missing values:')\n",
    "    print(data_processed_dfx.apply(lambda o: no_consecutive_values(o, np.nan).max()))\n",
    "    \n",
    "    # Zeros:\n",
    "    print('\\nNumber of zeros:')\n",
    "    print(data_processed_dfx.eq(0).mean()*100)\n",
    "    print('\\nMaximum number of consecutive zeros')\n",
    "    print(data_processed_dfx.apply(lambda o: no_consecutive_values(o, 0).max()))\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.hist(no_consecutive_values(data_processed_dfx['offtake'], 0)/(4*24), bins=200, log=True)\n",
    "    plt.xlabel('duration of zeros (days)')\n",
    "    plt.ylabel('count')\n",
    "    plt.title('histogram of duration of zero-offtake periods')\n",
    "\n",
    "meter_IDs_data_small = data_df['meter ID'].unique()[:3]\n",
    "data_df_small = data_df[data_df['meter ID'].isin(meter_IDs_data_small)]\n",
    "data_processed_df_small = data_processed_df[data_processed_df.index.isin(meter_IDs_data_small, level=0)]\n",
    "# check_data(data_df_small, data_processed_df_small); print(' ↑ for a small part of the data!!!')\n",
    "check_data(data_df, data_processed_df); print('\\n ↑ for the whole dataset\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
