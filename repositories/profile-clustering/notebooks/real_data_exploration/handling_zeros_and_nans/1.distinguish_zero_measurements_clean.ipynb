{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinguish measurement errors from real zero measurements\n",
    "\n",
    "So we build upon a couple of intuitions.  \n",
    "Indicators for an error: \n",
    "- **clear cumulative value** *Implemented not adapted*\n",
    "- **collective zero/NaN interval** *Implemented*  \n",
    "(it happens that in one profile an interval is zero and in another it is NaN) \n",
    "\n",
    "\n",
    "Indicators for normal behaviour: \n",
    "- **A single zero when there is a consumption sign change**  *Implemented*\n",
    "- **A single zero surrounded by low consumption** *Implemented*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Imports and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd #conda install dask\n",
    "from dask.distributed import Client\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import datetime\n",
    "import tqdm\n",
    "idx = pd.IndexSlice\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interval_information import get_interval_df\n",
    "from peak_detection import (\n",
    "    get_cumulative_value_detections, \n",
    "    get_connection_and_pv_power_peaks, \n",
    "    get_knn_similarity_based_peaks,\n",
    "    match_knn_then_assumption_parallel\n",
    ")\n",
    "from zero_intervals import (\n",
    "    sign_change_intervals, \n",
    "    low_consumption_on_both_sides_intervals, \n",
    "    collective_error_intervals\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/preprocessed/combined')\n",
    "RESULT_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/error_detection')\n",
    "RESULT_PATH.mkdir(mode = 0o770, parents = True, exist_ok=True)\n",
    "zero_error_path = RESULT_PATH / 'zero_interval_is_error.csv' \n",
    "zero_error_pkl_path = RESULT_PATH / 'zero_interval_is_error.pkl' \n",
    "# info_path = PRE_PATH/'info.csv'\n",
    "# data_path = PRE_PATH/'data.csv'\n",
    "info_path = PRE_PATH/'reindexed_info.pkl'\n",
    "data_path = PRE_PATH/'reindexed_DST_data.pkl'\n",
    "result_with_error_path = PRE_PATH/'reindexed_DST_data_w_errors.pkl'\n",
    "assert info_path.exists() and data_path.exists(), 'These paths should exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_summary(series): \n",
    "    count = series.value_counts(dropna=False).to_frame('count')\n",
    "    count['relative'] = count['count']/count['count'].sum()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_strategies(*args): \n",
    "    strategies = pd.concat(args, axis = 1)\n",
    "    normal = (strategies == False).any(axis = 1)\n",
    "    error = (strategies == True).any(axis = 1)\n",
    "    nan = (strategies.isna()).all(axis = 1)\n",
    "    result = pd.Series(index = strategies.index, dtype ='object')\n",
    "    result[error] = True\n",
    "    result[normal] = False\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "info_df = pd.read_pickle(info_path)\n",
    "data_df = pd.read_pickle(data_path)\n",
    "data_df.columns = pd.to_datetime(data_df.columns, exact = False)\n",
    "data_df.columns.name = 'timestamp'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_SOURCE = 'EandisVREG'\n",
    "# YEAR = 2016\n",
    "# # get the right subset based on the info df\n",
    "# info16_df = info_df.loc[idx[:, 2016],:]\n",
    "# info16_df = info16_df[info16_df.data_source == 'EandisVREG']\n",
    "\n",
    "# # read the corresponding data profiles \n",
    "# data16_df = data_df.loc[info16_df.index, :]\n",
    "\n",
    "# info_df.connection_power.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only investigate timeseries with data problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb of zeros for each profile\n",
    "nb_of_zeros = (data_df == 0).sum(axis = 1)\n",
    "nb_of_nan = data_df.isna().any(axis =1 )\n",
    "data16_df= data_df.loc[(nb_of_zeros>0) | nb_of_nan]\n",
    "# data16_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the intervals\n",
    "So in the rest of this code we simply construct the intervals as a dataset and add different attributes/features and investigate whether they could be useful or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# the DST missing intervals are still in here\n",
    "interval_df = get_interval_df(data_df, info_df, keep_zero = True, keep_nan = True)\n",
    "interval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign change intervals\n",
    "A single zero is normal if the consumption changes sign "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_change_detection = sign_change_intervals(interval_df)\n",
    "detection_summary(sign_change_detection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short low consumption intervals\n",
    "A single zero is normal if the consumption on both sides of the interval is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_consumption_detection = low_consumption_on_both_sides_intervals(interval_df)\n",
    "detection_summary(low_consumption_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_detection = combine_strategies(low_consumption_detection, sign_change_detection)\n",
    "detection_summary(current_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collective periods based on start time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't look at the intervals we have marked in the previous two steps\n",
    "rel_interval_df = interval_df.query('interval_value == 0')[current_detection != False]\n",
    "rel_interval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collective_data_problems = collective_error_intervals(rel_interval_df, threshold = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_result = combine_strategies(sign_change_detection,low_consumption_detection, collective_data_problems)\n",
    "detection_summary(current_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On the remaining zero intervals do cumulative value detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'there are {current_result.isna().sum()} intervals that are still unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "remaining_intervals = interval_df.query('interval_value == 0')[current_result.isna()]\n",
    "data_subset = data16_df.loc[remaining_intervals.index.get_level_values(0).unique()]\n",
    "\n",
    "cumulative_value_detection = get_cumulative_value_detections(data_subset, remaining_intervals, n_threads = 10, result_dir = Path()/'intermediate_results')\n",
    "cumulative_value_detection = cumulative_value_detection.reindex(interval_df.query('interval_value ==0').index)\n",
    "cumulative_value_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = combine_strategies(sign_change_detection,low_consumption_detection, collective_data_problems, cumulative_value_detection)\n",
    "detection_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_frame('is_error').to_csv(zero_error_path)\n",
    "result.to_frame('is_error').to_pickle(zero_error_pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include this result in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_with_error = interval_df.join(result.to_frame('is_error'))\n",
    "interval_with_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every detected data problem fill the interval with the end values with NaN's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_with_errors = data_df.copy()\n",
    "for index, row in tqdm.tqdm(interval_with_error[interval_with_error.is_error == True].iterrows()): \n",
    "    data_df_with_errors.loc[tuple(index[:2]),:].iloc[index[2]:index[3]+1] = np.NAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_with_errors.to_pickle(result_with_error_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
