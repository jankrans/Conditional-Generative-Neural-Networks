{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New similarity based cumulative value approach \n",
    "This approach first searched the nearest neighbors using the context as a guideline.  \n",
    "Each nearest neighbor checks which assumption fits the best.  \n",
    "If most nearest neighbors vote for the same assumption we mark the value as this.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import datetime\n",
    "import random\n",
    "from scipy.signal import find_peaks, find_peaks_cwt\n",
    "from kde_diffusion import kde1d\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import warnings\n",
    "from scipy.stats import norm\n",
    "idx = pd.IndexSlice\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this reloads code from external modules automatically if it is changed (without having to restart the kernel)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from interval_information import get_interval_df\n",
    "from peak_detection import (\n",
    "    get_connection_and_pv_power_peaks, \n",
    "    get_model_based_global_peaks,\n",
    "    get_similarity_based_peaks, \n",
    "    construct_search_intervals, \n",
    "    add_data_to_search_intervals, \n",
    "    sim_known_data, \n",
    "    match_knn_then_assumption, \n",
    "    get_knn_similarity_based_peaks\n",
    "    \n",
    ")\n",
    "from statistical_models import (\n",
    "    NormalDistribution, \n",
    "    AutoKDEDistribution, \n",
    "    KDEDistribution,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/preprocessed/combined')\n",
    "RESULT_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/error_detection')\n",
    "RESULT_PATH.mkdir(mode = 0o770, parents = True, exist_ok=True)\n",
    "result_path = RESULT_PATH / 'cumulative_value_detection.csv' \n",
    "info_path = PRE_PATH/'info.csv'\n",
    "data_path = PRE_PATH/'data.csv'\n",
    "assert info_path.exists() and data_path.exists(), 'These paths should exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_profile_with_intervals(meterID, year, period_type_column = None, data = None, daterange = None):\n",
    "    # plots the profile, using the period data in data \n",
    "    # the color can be determined using the period_type_column\n",
    "    if data is None : \n",
    "        data = profile_intervals\n",
    "    if daterange is not None: \n",
    "        start_time =  f'2016-{daterange[0]}-1 00:00:00'\n",
    "        end_time = f'2016-{daterange[1]}-1 00:00:00'\n",
    "        profile_df = data16_df.loc[(meterID, year),start_time:end_time]\n",
    "        periods_for_profile =data.loc[(meterID,year), :]\n",
    "        periods_for_profile = periods_for_profile[(periods_for_profile['end_time'] > start_time ) & (periods_for_profile['start_time'] < end_time)]\n",
    "    else: \n",
    "        profile_df = data16_df.loc[(meterID, year),:]\n",
    "        periods_for_profile =data.loc[(meterID,year), :]\n",
    "        \n",
    "#     print(periods_for_profile[['start_time', 'end_time']])\n",
    "#     print(zero_periods_for_profile[['start_time', 'end_time', 'is_disconnection_period']])\n",
    "    line = alt.Chart(profile_df.to_frame('value').reset_index()).mark_line().encode(\n",
    "        x = alt.X('timestamp:T'), \n",
    "        y = alt.Y('value:Q')\n",
    "    )\n",
    "    if period_type_column is None: \n",
    "        color_encoding = alt.ColorValue('blue') \n",
    "    else: \n",
    "        color_encoding = alt.Color(f'{period_type_column}:N')\n",
    "    plot_df =periods_for_profile.reset_index(drop=True)\n",
    "    rect = alt.Chart(plot_df).mark_rect(opacity = 0.6).encode(\n",
    "        x = 'start_time:T',\n",
    "        x2 = 'end_time:T', \n",
    "        color = color_encoding\n",
    "    ) + alt.Chart(plot_df).mark_circle(size = 100).encode(\n",
    "        x = 'start_time:T',\n",
    "        y = alt.YValue(profile_df.max()),\n",
    "#         x2 = 'end_time:T', \n",
    "        color = color_encoding\n",
    "    )\n",
    "    chart = rect + line\n",
    "    if 'connection_power' in periods_for_profile.columns: \n",
    "        connection_power = float(periods_for_profile.connection_power.iat[0])\n",
    "\n",
    "        connection_power_line = alt.Chart(periods_for_profile.reset_index()).mark_rule(color = 'black', opacity = 0.8).encode(\n",
    "            y =  'mean(connection_power):Q'\n",
    "        )\n",
    "        chart += connection_power_line\n",
    "    return chart.properties(width = 2200, title = f\"{meterID} in {year}\").interactive()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "Small table for convenience comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(name1, series1, name2, series2): \n",
    "    return pd.crosstab(series1, series2, rownames = [name1], colnames =[name2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "info_df = pd.read_csv(info_path, dtype={'meterID':'str'}).set_index(['meterID', 'year'], drop=True)\n",
    "print(f'#profiles = {info_df.shape[0]}')\n",
    "data_df = pd.read_csv(data_path, dtype={'meterID':'str'}).set_index(['meterID', 'year'], drop=True)\n",
    "data_df.columns = pd.to_datetime(data_df.columns)\n",
    "data_df.columns.name = 'timestamp'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For development look at subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = 'EandisVREG'\n",
    "YEAR = 2016\n",
    "# get the right subset based on the info df\n",
    "info16_df = info_df.loc[idx[:, 2016],:]\n",
    "info16_df = info16_df[info16_df.data_source == 'EandisVREG']\n",
    "info16_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the corresponding data profiles \n",
    "data16_df = data_df.loc[info16_df.index, :]\n",
    "data16_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the intervals with additional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data16_df = data16_df[data16_df.isna().any(axis = 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df = get_interval_df(data16_df, info16_df, keep_zero = True, keep_nan = True)\n",
    "interval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check peaks due to PV_power and connection_power\n",
    "If a value after an interval is larger than the connection power or lower than the negative PV_power/connection_power we know for sure it is a cumulative value.  \n",
    "**For now I assume a power_factor of 1 to convert kVA to kW ($kVA \\approx kW$), a lower power_factor will only work better because the threshold becomes 'tighter'!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_power_peaks = get_connection_and_pv_power_peaks(interval_df)\n",
    "connection_power_peaks.value_counts().to_frame('count').rename_axis(index = ['cumulative_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly this rule only helps to detect very few peaks but these peak detections are correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDE method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# takes around 11 minutes on pinac-d\n",
    "# this takes a while, we can later switch to faster KDE methods (sklearn is accurate but SLLLOOWWW)\n",
    "global_kde_peaks,global_kde_models = get_model_based_global_peaks(data16_df, interval_df, lambda: KDEDistribution(0.99, 0.07), return_models = True)\n",
    "global_kde_peaks.value_counts().to_frame('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_start_and_low_end = pd.Series(index = interval_df.index, dtype = 'bool')\n",
    "intervals_w_problems = interval_df[['0th_value_after_end', '1th_value_after_end', 'value_before_start']].isin(['start', 'end']).any(axis = 1)\n",
    "intervals_w_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_start_and_low_end.loc[~intervals_w_problems] = (interval_df.loc[~intervals_w_problems, ['0th_value_after_end', '1th_value_after_end', 'value_before_start']].abs() < 0.1).all(axis = 1)\n",
    "low_start_and_low_end[intervals_w_problems] = False\n",
    "low_start_and_low_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New similarity based method\n",
    "This one basically looks for the nearest neighbors first and is then going to check which assumption fits the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# reasonably fast takes 1min40s \n",
    "same_day_intervals = interval_df[interval_df.start_time.dt.date == interval_df.end_time.dt.date]\n",
    "similarity_peaks = get_knn_similarity_based_peaks(data16_df, same_day_intervals, context_size = '6H', reference_day_window = 50, k = 5)\n",
    "similarity_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_knn_then_assumption(row, data_df, reference_day_window = 50, context_size = '4H', k = 5):\n",
    "    meterID, year, _, _ = row.name \n",
    "    # start and end time of the interval INCLUSIVE\n",
    "    # so start_time is the first NaN value and end_time is the last NaN value\n",
    "    start_time, end_time = row['start_time']+pd.Timedelta('15min') , row['end_time']-pd.Timedelta('15min')\n",
    "    # later all timestamps will be put on the same date so do this here as well \n",
    "    start_time2, end_time2 = start_time.replace(year = 2016, month = 1, day = 1), end_time.replace(year = 2016, month = 1, day =1)\n",
    "   \n",
    "\n",
    "    # make the dataframe with all the relevant data\n",
    "    search_intervals_df = construct_search_intervals(start_time, end_time, reference_day_window, context_size, data16_df)\n",
    "    rel_data_df = add_data_to_search_intervals(meterID, year, search_intervals_df, data16_df)\n",
    "\n",
    "    # seperate the missing day from all the other days\n",
    "    missing_day = rel_data_df.loc[start_time - pd.Timedelta(context_size)/2]\n",
    "    reference_days = rel_data_df.drop(index = start_time-pd.Timedelta(context_size)/2)\n",
    "    \n",
    "    # stats on the missing day \n",
    "    min_value_missing_day, max_value_missing_day  = abs(missing_day.squeeze().min()), abs(missing_day.squeeze().max())\n",
    "    max_distance = max(min_value_missing_day, max_value_missing_day) / 2 \n",
    "    \n",
    "    # drop reference days with data problems\n",
    "    # TODO fix for zero days then this is not really correct :) \n",
    "    reference_days.dropna(inplace = True)\n",
    "\n",
    "    # calculate the distances between the missing day and the reference days \n",
    "    distances_known_data = reference_days.apply(sim_known_data, axis = 1, missing_day = missing_day.squeeze().to_numpy(), raw = True)\n",
    "    \n",
    "    # sort the distances from small to large\n",
    "    sorted_distances = distances_known_data.sort_values()\n",
    "       \n",
    "    # take the best k matches\n",
    "    best_matches = reference_days.loc[sorted_distances.iloc[:k].index]\n",
    "    \n",
    "    # for these matches calculate how well the cumulative and real value assumption fit \n",
    "    best_match_info = pd.DataFrame(index = best_matches.index)\n",
    "    peak_time = end_time2 + pd.Timedelta('15min')\n",
    "    # calculate the expected value after the interval using each assumption\n",
    "    best_match_info['cumulative'] = best_matches.apply(lambda x: np.sum(x.loc[start_time2: peak_time]), axis = 1)\n",
    "    best_match_info['real'] = best_matches[peak_time]\n",
    "    \n",
    "    # calculate the difference between the observed value and the expected value\n",
    "    observed_value =  missing_day.squeeze()[peak_time]\n",
    "    best_match_info = best_match_info.join(sorted_distances.to_frame('simularity'), how = 'left')\n",
    "    best_match_info['observed'] = observed_value\n",
    "    best_match_info['cumulative_distance'] = np.abs(best_match_info['cumulative'] - observed_value)\n",
    "    best_match_info['real_distance'] = np.abs(best_match_info['real'] - observed_value)\n",
    "    \n",
    "    # let each profile vote\n",
    "    real_votes = best_match_info.real_distance < best_match_info.cumulative_distance\n",
    "    cumulative_votes = best_match_info.cumulative_distance < best_match_info.real_distance\n",
    "    dont_know_votes = best_match_info[['cumulative_distance','real_distance']].min(axis = 1) > max_distance\n",
    "    best_match_info.loc[real_votes, 'vote']  = 'real'\n",
    "    best_match_info.loc[cumulative_votes, 'vote'] = 'cumulative'\n",
    "    best_match_info.loc[dont_know_votes, 'vote'] = 'dont_know'\n",
    "    \n",
    "    # count votes \n",
    "    votes = best_match_info[best_match_info.vote != 'dont_know']\n",
    "    vote_count = votes.vote.value_counts()\n",
    "    relative_vote_count = vote_count/ len(votes)\n",
    "    \n",
    "    decision_certainty = relative_vote_count.max()\n",
    "    if decision_certainty >= 0.80: \n",
    "        decision = relative_vote_count.idxmax()\n",
    "        if decision == 'dont_know': \n",
    "            decision = None\n",
    "    else: \n",
    "        decision = None\n",
    "    \n",
    "#     best_match_info = best_match_info[['real_distance', 'cumulative_distance']]\n",
    "    \n",
    "    return decision, relative_vote_count, missing_day, best_matches, best_match_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All info df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_df = (\n",
    "    interval_df\n",
    "        .drop(columns = ['0th_value_after_end', '1th_value_after_end', 'value_before_start', 'PV_power'])\n",
    "        .join(connection_power_peaks.to_frame('connection_peak'))\n",
    "        .join(global_kde_peaks.to_frame('kde_peak'))\n",
    "        .join(similarity_peaks.to_frame('similarity_peak'))\n",
    "        .join(low_start_and_low_end.to_frame('low_start_and_end'))\n",
    "        .fillna({'similarity_peak': np.nan})\n",
    "    )\n",
    "OVERWRITE = True\n",
    "if OVERWRITE or not result_path.exists(): \n",
    "    vis_df.to_csv(result_path)\n",
    "vis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_df = pd.read_csv(result_path)\n",
    "# vis_df['start_time'] = pd.to_datetime(vis_df['start_time'])\n",
    "# vis_df['end_time'] = pd.to_datetime(vis_df['end_time'])\n",
    "# vis_df = vis_df.set_index(['meterID', 'year', 'end', 'start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = vis_df[~vis_df.connection_peak & vis_df.similarity_peak]\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(name1, series1, name2, series2): \n",
    "    return pd.crosstab(series1, series2, rownames = [name1], colnames =[name2], dropna = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix('connection_power', connection_power_peaks, 'KDE', global_kde_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_connection_peaks = connection_power_peaks[similarity_peaks.index]\n",
    "confusion_matrix('connection_power', rel_connection_peaks.fillna('NA'), 'similarity based', similarity_peaks.fillna(\"NA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with global context kde peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_kde_peaks = global_kde_peaks[similarity_peaks.index]\n",
    "confusion_matrix('global_kde', rel_kde_peaks.fillna('NA'), 'similarity', similarity_peaks.fillna(\"NA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at some specific cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_similarity_approach(random_interval, reference_day_window = 50, context_size = '4H', k = 5): \n",
    "    decision, decision_info, missing_day, best_matches, best_match_info = match_knn_then_assumption(random_interval, data16_df, reference_day_window, context_size, k)\n",
    "    print(f\"decision = {decision}\")\n",
    "    display(decision_info.to_frame().T)\n",
    "    display(random_interval.to_frame().T)\n",
    "    start, end = random_interval.start_time - pd.Timedelta(days = reference_day_window //2+1), random_interval.end_time + pd.Timedelta(days = reference_day_window //2+1)\n",
    "    profile_data = data16_df.loc[random_interval.name[:2], start:end]\n",
    "\n",
    "    profile_data_vis = profile_data.to_frame('value').reset_index()\n",
    "    profile_data_vis\n",
    "\n",
    "    bars_df = best_matches.index.to_frame().reset_index(drop = True)\n",
    "    bars_df['type'] = 'reference_day'\n",
    "    start_missing, end_missing = random_interval[['start_time', 'end_time']]\n",
    "    start_missing -= pd.Timedelta(context_size)/2\n",
    "    end_missing += pd.Timedelta(context_size)/2\n",
    "\n",
    "    bars_df = bars_df.append({'start':start_missing, 'end':end_missing, 'type':'missing_day'}, ignore_index = True)\n",
    "    bars_df\n",
    "\n",
    "    full_chart = alt.Chart(bars_df).mark_rect(opacity = 0.6).encode(\n",
    "            x = 'start:T',\n",
    "            x2 = 'end:T', \n",
    "            color = 'type'\n",
    "        ) + alt.Chart(profile_data_vis, width = 2000).mark_line().encode(\n",
    "        x = 'timestamp:T', \n",
    "        y = 'value'\n",
    "    ) \n",
    "\n",
    "    missing_day_vis = missing_day.stack(dropna=False).to_frame('value').reset_index().rename(columns = {'level_1':'time'})\n",
    "\n",
    "    missing_chart = alt.Chart(missing_day_vis, title = 'the missing interval + context').mark_line().encode(\n",
    "        x='time:T', \n",
    "        y= 'value', \n",
    "        tooltip = [alt.Tooltip('time', format = '%H:%M'),'value']\n",
    "    )\n",
    "\n",
    "    matches_vis =best_matches.stack().to_frame('value').reset_index().rename(columns = {'level_2':'time'})\n",
    "    matches_chart = alt.Chart(matches_vis, title = 'best matches based on context').mark_line().encode(\n",
    "        x = 'time:T', \n",
    "        y = 'value', \n",
    "        color = 'start:N', \n",
    "        tooltip = [alt.Tooltip('time', format = '%H:%M'),'value']\n",
    "    )\n",
    "    (full_chart.interactive(bind_y =False) & (missing_chart.interactive(bind_x = False) | matches_chart.interactive(bind_x = False)).resolve_scale(y='shared')).resolve_scale(y='shared').display()\n",
    "    display(best_match_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connection peak is true but similarity peak doesn't know or dissagrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_df_same_day = vis_df[vis_df.start_time.dt.date == vis_df.end_time.dt.date]\n",
    "connection_sim_disagrees = vis_df_same_day[vis_df_same_day.connection_peak & (vis_df_same_day.similarity_peak == False)]\n",
    "kde_sim_disagrees = vis_df_same_day[vis_df_same_day.kde_peak & (vis_df_same_day.similarity_peak == False)]\n",
    "kde_false_sim_true = vis_df_same_day[~vis_df_same_day.kde_peak & (vis_df_same_day.similarity_peak)]\n",
    "real_detections = vis_df_same_day[vis_df_same_day.similarity_peak == False]\n",
    "dont_know_detections = vis_df_same_day[vis_df_same_day.similarity_peak.isna()]\n",
    "sim_dont_know_low = vis_df_same_day[vis_df_same_day.similarity_peak.isna() & vis_df_same_day.low_start_and_end]\n",
    "IDX = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX += 1\n",
    "# chose one of the options from above\n",
    "interval = kde_sim_disagrees.iloc[IDX]\n",
    "inspect_similarity_approach(interval, context_size = '6H', reference_day_window = 50, k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a single prediction per interval\n",
    "Sl2clpa0lIpO1Q this profile is an exception! The zeros after the Nan Interval should be replaced by NaNs and the value after the longer NaN interval should be marked as real\n",
    "- For the rest if connection_power_peak can give an answer use that answer\n",
    "- We won't use the kde results as similarity gives the same result but better \n",
    "- So then we apply similarity to the remaining NaNs \n",
    "- After the similarity all intervals where beginning and end are low are given a real value prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info = vis_df\n",
    "all_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = pd.Series(index = all_info.index, dtype = 'object')\n",
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection peak overwrites all the rest\n",
    "final_predictions[all_info.connection_peak] = True\n",
    "step1 = final_predictions.value_counts(dropna = False).to_frame('count')\n",
    "step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next use similarity \n",
    "final_predictions[final_predictions.isna()] = all_info.similarity_peak[final_predictions.isna()]\n",
    "step2 = final_predictions.value_counts(dropna = False).to_frame('count')\n",
    "step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next let KDE fill in the things that similarity couldn't solve\n",
    "final_predictions[final_predictions.isna() & all_info.kde_peak] = True\n",
    "step3 = final_predictions.value_counts(dropna = False).to_frame('count')\n",
    "step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after similarity mark intervals that are still NaN as real if both beginning and end value are low \n",
    "final_predictions[final_predictions.isna() & all_info.low_start_and_end] = False\n",
    "step4 = final_predictions.value_counts(dropna = False).to_frame('count')\n",
    "step4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose an interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_day_intervals = interval_df[interval_df.start_time.dt.date == interval_df.end_time.dt.date]\n",
    "IDX = 100\n",
    "# IDX +=100\n",
    "# IDX = 1 # a cumulative peak that would otherwise not be detected\n",
    "# IDX = 2 # a cumulative peak that would otherwise not be detected\n",
    "# IDX = 11 # a case where it is not super clear what to do\n",
    "# IDX = 316 # a case where it is not super clear what to do\n",
    "# IDX = 176 # a case where it is not super clear what to do but we don't actually carea\n",
    "random_interval = same_day_intervals.iloc[IDX]\n",
    "print(IDX)\n",
    "random_interval.to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the detection run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_day_window = 50\n",
    "context_size = '4H'\n",
    "k = 5\n",
    "decision, decision_info, missing_day, best_matches, best_match_info = match_knn_then_assumption(random_interval, reference_day_window, context_size, k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(best_match_info.mean()[['cumulative_distance','real_distance']].to_frame())\n",
    "best_match_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
