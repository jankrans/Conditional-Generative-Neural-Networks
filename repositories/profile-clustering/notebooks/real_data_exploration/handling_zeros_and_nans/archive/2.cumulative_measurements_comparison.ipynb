{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A try to consolidate all cumulative measurement approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import datetime\n",
    "import random\n",
    "from scipy.signal import find_peaks, find_peaks_cwt\n",
    "from kde_diffusion import kde1d\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import warnings\n",
    "from scipy.stats import norm\n",
    "idx = pd.IndexSlice\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this reloads code from external modules automatically if it is changed (without having to restart the kernel)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from interval_information import get_interval_df\n",
    "from peak_detection import (\n",
    "    get_connection_and_pv_power_peaks, \n",
    "    get_model_based_global_peaks,\n",
    "    get_similarity_based_peaks, \n",
    "    get_knn_similarity_based_peaks\n",
    ")\n",
    "from statistical_models import (\n",
    "    NormalDistribution, \n",
    "    AutoKDEDistribution, \n",
    "    KDEDistribution,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/preprocessed/combined')\n",
    "info_path = PRE_PATH/'info.csv'\n",
    "data_path = PRE_PATH/'data.csv'\n",
    "assert info_path.exists() and data_path.exists(), 'These paths should exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_profile_with_intervals(meterID, year, period_type_column = None, data = None, daterange = None):\n",
    "    # plots the profile, using the period data in data \n",
    "    # the color can be determined using the period_type_column\n",
    "    if data is None : \n",
    "        data = profile_intervals\n",
    "    if daterange is not None: \n",
    "        start_time =  f'2016-{daterange[0]}-1 00:00:00'\n",
    "        end_time = f'2016-{daterange[1]}-1 00:00:00'\n",
    "        profile_df = data16_df.loc[(meterID, year),start_time:end_time]\n",
    "        periods_for_profile =data.loc[(meterID,year), :]\n",
    "        periods_for_profile = periods_for_profile[(periods_for_profile['end_time'] > start_time ) & (periods_for_profile['start_time'] < end_time)]\n",
    "    else: \n",
    "        profile_df = data16_df.loc[(meterID, year),:]\n",
    "        periods_for_profile =data.loc[(meterID,year), :]\n",
    "        \n",
    "#     print(periods_for_profile[['start_time', 'end_time']])\n",
    "#     print(zero_periods_for_profile[['start_time', 'end_time', 'is_disconnection_period']])\n",
    "    line = alt.Chart(profile_df.to_frame('value').reset_index()).mark_line().encode(\n",
    "        x = alt.X('timestamp:T'), \n",
    "        y = alt.Y('value:Q')\n",
    "    )\n",
    "    if period_type_column is None: \n",
    "        color_encoding = alt.ColorValue('blue') \n",
    "    else: \n",
    "        color_encoding = alt.Color(f'{period_type_column}:N')\n",
    "    plot_df =periods_for_profile.reset_index(drop=True)\n",
    "    rect = alt.Chart(plot_df).mark_rect(opacity = 0.6).encode(\n",
    "        x = 'start_time:T',\n",
    "        x2 = 'end_time:T', \n",
    "        color = color_encoding\n",
    "    ) + alt.Chart(plot_df).mark_circle(size = 100).encode(\n",
    "        x = 'start_time:T',\n",
    "        y = alt.YValue(profile_df.max()),\n",
    "#         x2 = 'end_time:T', \n",
    "        color = color_encoding\n",
    "    )\n",
    "    chart = rect + line\n",
    "    if 'connection_power' in periods_for_profile.columns: \n",
    "        connection_power = float(periods_for_profile.connection_power.iat[0])\n",
    "\n",
    "        connection_power_line = alt.Chart(periods_for_profile.reset_index()).mark_rule(color = 'black', opacity = 0.8).encode(\n",
    "            y =  'mean(connection_power):Q'\n",
    "        )\n",
    "        chart += connection_power_line\n",
    "    return chart.properties(width = 2200, title = f\"{meterID} in {year}\").interactive()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "Small table for convenience comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(name1, series1, name2, series2): \n",
    "    return pd.crosstab(series1, series2, rownames = [name1], colnames =[name2], dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "info_df = pd.read_csv(info_path, dtype={'meterID':'str'}).set_index(['meterID', 'year'], drop=True)\n",
    "print(f'#profiles = {info_df.shape[0]}')\n",
    "data_df = pd.read_csv(data_path, dtype={'meterID':'str'}).set_index(['meterID', 'year'], drop=True)\n",
    "data_df.columns = pd.to_datetime(data_df.columns)\n",
    "data_df.columns.name = 'timestamp'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For development look at subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = 'EandisVREG'\n",
    "YEAR = 2016\n",
    "# get the right subset based on the info df\n",
    "info16_df = info_df.loc[idx[:, 2016],:]\n",
    "info16_df = info16_df[info16_df.data_source == 'EandisVREG']\n",
    "info16_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the corresponding data profiles \n",
    "data16_df = data_df.loc[info16_df.index, :]\n",
    "data16_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the intervals with additional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data16_df = data16_df[data16_df.isna().any(axis = 1)]\n",
    "data16_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df = get_interval_df(data16_df, info16_df, keep_zero = False, keep_nan = True)\n",
    "interval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect some random profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special profile: after a missing interval there is a zero \n",
    "In this part of the data there is one profile where values after a missing interval are always zero.  \n",
    "But these zero values are clearly wrong!  \n",
    "So this might be something we also need to take into account later.  \n",
    "**For now I just ignore this given that it is only one profile**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals_of_interest = interval_df[(interval_df['0th_value_after_end'] == 0) & (interval_df['1th_value_after_end'] != 0) ]\n",
    "zero_after_interval = pd.DataFrame(index = interval_df.index)\n",
    "zero_after_interval['special_interval'] = False\n",
    "zero_after_interval.loc[intervals_of_interest.index] = True\n",
    "zero_after_interval;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 2\n",
    "vis_df = pd.concat([interval_df, zero_after_interval], axis = 1)\n",
    "detected_peak_ids = vis_df[vis_df.special_interval].index.get_level_values(0).unique()\n",
    "print(f'{len(detected_peak_ids)} profiles with a detected peak')\n",
    "non_detected_peak_ids = vis_df[~ vis_df.special_interval].index.get_level_values(0).unique()\n",
    "profile_to_show = detected_peak_ids[IDX]\n",
    "profile_to_show = 'Sl2clpa0lIpO1Q' # profile where we have this weird scenario \n",
    "print(profile_to_show)\n",
    "# ';' supresses the output of the line. So if you want to see the plot remove the ; at the end of the next line\n",
    "plot_profile_with_intervals(profile_to_show, 2016, data = vis_df, period_type_column = \"special_interval\").properties(height = 400);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check peaks due to PV_power and connection_power\n",
    "If a value after an interval is larger than the connection power or lower than the negative PV_power/connection_power we know for sure it is a cumulative value.  \n",
    "**For now I assume a power_factor of 1 to convert kVA to kW ($kVA \\approx kW$), a lower power_factor will only work better because the threshold becomes 'tighter'!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_power_peaks = get_connection_and_pv_power_peaks(interval_df)\n",
    "connection_power_peaks.value_counts().to_frame('count').rename_axis(index = ['cumulative_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly this rule only helps to detect very few peaks but these peak detections are correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise some of the detected and non detected peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 1\n",
    "vis_df = pd.concat([interval_df, connection_power_peaks.to_frame('connection_power_peak')], axis = 1)\n",
    "detected_peak_ids = vis_df[vis_df.connection_power_peak].index.get_level_values(0).unique()\n",
    "non_detected_peak_ids = vis_df[~ vis_df.connection_power_peak].index.get_level_values(0).unique()\n",
    "injection_ids = data16_df[(data16_df < 0).any(axis = 1)].index.get_level_values(0).unique()\n",
    "profile_to_show = injection_ids[IDX]\n",
    "# profile_to_show = \"Sl2clpSwmYpN1Q\" # profile with clear cumulative measurement under connection capacity\n",
    "print(profile_to_show)\n",
    "plot_profile_with_intervals(profile_to_show, 2016, data = vis_df, period_type_column = 'connection_power_peak').properties(height = 400);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how this does for the nan intervals longer than 12 hours\n",
    "For long intervals, this methods seems to be enough to get a perfect detection  \n",
    "After such a long interval it seems that if there is a cumulative peak it will always exceed the connection capacity  \n",
    "This might be useful later on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOURS = 12\n",
    "periods = HOURS * 4 \n",
    "IDX = 21\n",
    "long_intervals = interval_df[interval_df.interval_length >= periods]\n",
    "vis_df = long_intervals.join(connection_power_peaks.to_frame('connection_power_peak'))\n",
    "detected_peak_ids = vis_df[vis_df.connection_power_peak].index.get_level_values(0).unique()\n",
    "non_detected_peak_ids = vis_df[~ vis_df.connection_power_peak].index.get_level_values(0).unique()\n",
    "print(f\"{len(detected_peak_ids)} profiles with a detected peak\")\n",
    "print(f\"{len(non_detected_peak_ids)} profiles with a undetected peak\")\n",
    "profile_to_show = non_detected_peak_ids[IDX]\n",
    "# profile_to_show = \"Sl2clpSwmYpN1Q\" # profile with clear cumulative measurement under connection capacity\n",
    "print(profile_to_show)\n",
    "plot_profile_with_intervals(profile_to_show, 2016, data = vis_df, period_type_column = 'connection_power_peak').properties(height = 400);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global context with statistical model \n",
    "For this method, a statistical model is fitted on all the measurements (except the measurements after an interval).  \n",
    "Using this statistical model, we look at the likelihood of a value after a nan interval.  \n",
    "If the likelihood is very low, the value is considered a cumulative value.  \n",
    "\n",
    "A normal distribution is not very good! Because a lot of values are close to zero but there are some peaks. The normal distribution cannot very well capture the distribution of the measurements.  \n",
    "\n",
    "As such I switched to a KDE, this does a better job because it can capture multi-model distribution (e.g. distributions with more than one peak, you will see an example later in this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learned_model(row, model): \n",
    "    meterID, year = row.name\n",
    "    interval_endings = nan_intervals.loc[(meterID, year), 'end_time']\n",
    "    row_normal_values = row.drop(interval_endings, errors = 'ignore').dropna()\n",
    "    model = model()\n",
    "    model.fit(row_normal_values.to_numpy().T)\n",
    "    return model\n",
    "\n",
    "def get_peaks_with_model(nan_intervals, model, return_models = False):\n",
    "    models = data16_df.apply(get_learned_model, model= model, axis = 1)\n",
    "    models = nan_intervals[nan_intervals.first_value_after_end != 'end'].join(models.to_frame('model'))\n",
    "    is_gauss_peak = models.apply(lambda row: row['model'].test_value(float(row['first_value_after_end'])), axis = 1)\n",
    "    if return_models: \n",
    "        return is_gauss_peak, models\n",
    "    return is_gauss_peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_gauss_peaks, global_gauss_models = get_model_based_global_peaks(data16_df, interval_df, lambda: NormalDistribution(0.99), return_models = True)\n",
    "global_gauss_peaks.value_counts().to_frame('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# takes around 11 minutes on pinac-d\n",
    "# this takes a while, we can later switch to faster KDE methods (sklearn is accurate but SLLLOOWWW)\n",
    "global_kde_peaks,global_kde_models = get_model_based_global_peaks(data16_df, interval_df, lambda: KDEDistribution(0.99, 0.07), return_models = True)\n",
    "global_kde_peaks.value_counts().to_frame('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Gauss with KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(global_gauss_peaks, global_kde_peaks, rownames = ['gauss_peaks'], colnames =['kde_peaks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Gauss detects more cumulative values than KDE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 1\n",
    "vis_df = pd.concat([interval_df, global_gauss_peaks.to_frame('gauss'), global_kde_peaks.to_frame('kde')], axis = 1).astype({'gauss':'bool', 'kde':'bool'})\n",
    "def convert(a): \n",
    "    return \"_\".join([s for s,i in zip(['gauss', 'kde'],a) if i])\n",
    "vis_df['detected_by'] = vis_df[['gauss','kde']].apply(convert, axis = 1)\n",
    "vis_df['gauss_not_kde'] = vis_df['gauss'] & (~vis_df['kde'])\n",
    "# display(vis_df)\n",
    "gauss_not_kde_ids = vis_df.query('gauss_not_kde').index.get_level_values(0).unique()\n",
    "# profile_to_show = 'Sl2clpSwmYpN1Q' # a profile with a clear peak that is not discovered using connection capacity\n",
    "profile_to_show = gauss_not_kde_ids[IDX]\n",
    "print(profile_to_show)\n",
    "plot_profile_with_intervals(profile_to_show, 2016, data = vis_df, period_type_column = 'detected_by').properties(height = 400).display()\n",
    "global_kde_models.loc[profile_to_show, 'model'].iloc[0].get_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local context with statistical model \n",
    "Instead of using all measurements to learn a statistical model, we could also only use measurements close to the missing interval.  \n",
    "However, this has some issues the context size has to be chosen well.  \n",
    "If the context is chosen to small, we might mark values as weird (cumulative) while they are not in reality.  \n",
    "This is why I think it is better to stick with the global context.  \n",
    "But feel free to disagree with me ðŸ˜‰  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity based method\n",
    "So this method is going to look for similar days using two distance metrics.  \n",
    "One distance metric assumes that the value after the interval is a cumulative peak, the other distance metric assumes that the value after the interval is a correct measurement.   \n",
    "The assumption of the distance metric that finds the best match is assumed to be correct.  \n",
    "\n",
    "**For now I think the code only works for intervals that start/end on the same day.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# reasonably fast takes 1min40s \n",
    "same_day_intervals = interval_df[interval_df.start_time.dt.date == interval_df.end_time.dt.date]\n",
    "similarity_peaks = get_knn_similarity_based_peaks(data16_df, same_day_intervals, context_size = '6H', reference_day_window = 100, k = 10)\n",
    "similarity_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_peaks.value_counts(dropna = False).to_frame('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with connection_power_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_connection_peaks = connection_power_peaks[similarity_peaks.index]\n",
    "confusion_matrix('connection_power', rel_connection_peaks.fillna('NA'), 'similarity based', similarity_peaks.fillna(\"NA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with global context kde peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_kde_peaks = global_kde_peaks[similarity_peaks.index]\n",
    "confusion_matrix('global_kde', rel_kde_peaks.fillna('NA'), 'similarity', similarity_peaks.fillna(\"NA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion of similarity\n",
    "So the similarity seems to detect even more peaks than the rest, but it does also detect the peaks they detect.  \n",
    "**Important note: for now this method can only handle intervals that start and end on the same day!**  \n",
    "This method might also work better for profiles that are repetitive!\n",
    "\n",
    "Special cases:  \n",
    "- it occurs that both real_distance and cumulative_distance are very low!  \n",
    "in this case we don't really know which one of the two but it doesn't really matter  \n",
    "This is often the case for very short intervals (e.g. length 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_peaks.fillna(\"NA\").value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_df = (\n",
    "    interval_df\n",
    "        .drop(columns = ['0th_value_after_end', '1th_value_after_end', 'value_before_start', 'PV_power'])\n",
    "        .join(connection_power_peaks.to_frame('connection_peak'))\n",
    "        .join(global_kde_peaks.to_frame('kde_peak'))\n",
    "        .join(similarity_peaks.to_frame('similarity_peak'))\n",
    "        .fillna({'similarity_peak': np.nan})\n",
    "    )\n",
    "vis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(a): \n",
    "    detections = []\n",
    "    if a.connection_peak: \n",
    "        detections.append('connection')\n",
    "    if a.kde_peak:\n",
    "        detections.append('kde')\n",
    "    if len(detections) == 0 and pd.isna(a.similarity_peak): \n",
    "        return \"don't know\"\n",
    "    if not pd.isna(a.similarity_peak) and a.similarity_peak: \n",
    "        detections.append('similarity')\n",
    "    return \"_\".join(detections)\n",
    "\n",
    "vis_df['detected_by'] = vis_df[['connection_peak', 'kde_peak', 'similarity_peak']].apply(convert, axis = 1)\n",
    "vis_df.detected_by.value_counts().to_frame('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detected by KDE but simularity says it is false\n",
    "First of all this seems to happen in the exceptional case that a missing interval is followed by a zero.  \n",
    "IDX = 2 seems to have kde correct but similarity is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = -1\n",
    "# IDX += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_df[vis_df.detected_by == 'kde'].similarity_peak.value_counts(dropna= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_only_kde = vis_df[vis_df.detected_by == 'kde'].index.get_level_values(0).unique()\n",
    "profiles_kde_sim_false = vis_df[(vis_df.detected_by == 'kde') & (vis_df.similarity_peak == False)].index.get_level_values(0).unique()\n",
    "profile_to_show = profiles_kde_sim_false[IDX]\n",
    "print(IDX, profile_to_show)\n",
    "display(vis_df.loc[profile_to_show])\n",
    "plot_profile_with_intervals(profile_to_show, 2016, data = vis_df, period_type_column = 'detected_by').properties(height = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 3\n",
    "temp_df = vis_df.dropna(axis = 0, subset = ['similarity', 'kde'])\n",
    "detected_sim_not_kde = temp_df[temp_df.similarity & ~temp_df.kde].index.get_level_values(0).unique()\n",
    "detected_kde_not_sim = temp_df[~temp_df.similarity & temp_df.kde].index.get_level_values(0).unique()\n",
    "profile_to_show = detected_kde_not_sim[IDX]\n",
    "print(profile_to_show)\n",
    "display(similarity_peaks_info.loc[profile_to_show])\n",
    "plot_profile_with_intervals(profile_to_show, 2016, data = vis_df, period_type_column = 'detected_by').properties(height = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all three techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
