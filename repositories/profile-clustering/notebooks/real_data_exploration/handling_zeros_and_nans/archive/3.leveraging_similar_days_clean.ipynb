{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging similar days: clean version\n",
    "The main idea is when filling in the missing values in a profile, use the most similar day. \n",
    "\n",
    "In this notebook I also try using this idea to figure out if the value after a peak is a cumulative measurement or not: \n",
    "\n",
    "1. search for the most similar day based on simularity metric that assumes that the value after the missing interval is a cumulative measurement  \n",
    "2. search the most similar day based on a simularity metric that assumes that the value after the missing interval is a normal measurement  \n",
    "Check if which of these similar days is the most similar and use that one.  \n",
    "I think this would result in using the similar day from 2. whenever the peak is not abnormal and using the day from 1. if it is abnormal  \n",
    "\n",
    "its a bit more adaptive then using a normal distribution learned on similar days because it takes into account context  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import datetime\n",
    "import tqdm\n",
    "idx = pd.IndexSlice\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/preprocessed/combined')\n",
    "info_path = PRE_PATH/'info.csv'\n",
    "data_path = PRE_PATH/'data.csv'\n",
    "assert info_path.exists() and data_path.exists(), 'These paths should exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = pd.read_csv(info_path, index_col = [0,1])\n",
    "info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(data_path, index_col = [0,1])\n",
    "data_df.columns = pd.to_datetime(data_df.columns)\n",
    "data_df.columns.name = 'timestamp'\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with a subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = 'EandisVREG'\n",
    "YEAR = 2016\n",
    "# get the right subset based on the info df\n",
    "info16_df = info_df.loc[idx[:, 2016],:]\n",
    "info16_df = info16_df[info16_df.data_source == 'EandisVREG']\n",
    "info16_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the corresponding data profiles \n",
    "data16_df = data_df.loc[info16_df.index, :]\n",
    "data16_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only use profiles with potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb of zeros for each profile\n",
    "nb_of_na = (data16_df.isna()).sum(axis = 1)\n",
    "nb_of_zeros = (data16_df == 0).sum(axis = 1)\n",
    "\n",
    "data16_df= data16_df.loc[(nb_of_na>0)| (nb_of_zeros>0), :]\n",
    "data16_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the intervals\n",
    "So in the rest of this code we simply construct the intervals as a dataset and add different attributes/features and investigate whether they could be useful or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to find intervals with only zeros\n",
    "def value_interval(meterID, year, a, value):\n",
    "    \"\"\"\n",
    "        Makes a dataframe containing the start and end of each interval (only the longest intervals) that only contains value\n",
    "    \"\"\"\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    if np.isnan(value):\n",
    "        iszero = np.concatenate(([0], np.isnan(a).view(np.int8), [0]))\n",
    "    else: \n",
    "        iszero = np.concatenate(([0], np.equal(a, value).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    df = pd.DataFrame(ranges, columns = ['start', 'end'])\n",
    "    df['meterID'] = meterID\n",
    "    df['year'] = year\n",
    "    df['interval_value'] = value\n",
    "    return df.set_index(['meterID', 'year'])\n",
    "\n",
    "def zero_nan_intervals_df(data_df): \n",
    "    dfs = []\n",
    "    for (meterID, year), row in data_df.iterrows(): \n",
    "        nan_df = value_interval( meterID, year,row, np.NaN)\n",
    "        zero_df = value_interval( meterID, year, row, 0)\n",
    "        dfs.append(nan_df)\n",
    "        dfs.append(zero_df)\n",
    "    full_df = pd.concat(dfs, axis = 0)\n",
    "#     full_df['start_time'] = data14_df.columns[full_df['start']]\n",
    "#     full_df['end_time'] = data14_df.columns[full_df['end']-1]\n",
    "    return full_df\n",
    "\n",
    "profile_intervals = zero_nan_intervals_df(data16_df)\n",
    "profile_intervals['interval_length'] = profile_intervals.end - profile_intervals.start\n",
    "# start time and end time are exclusive! (this plots well with altair that why we do it this way)\n",
    "profile_intervals['start_time'] = data16_df.columns[profile_intervals['start']] - pd.Timedelta('15min')\n",
    "profile_intervals['end_time'] = data16_df.columns[profile_intervals['end']-1] # doing it this way because the timestamp we need might not exist in the columns\n",
    "profile_intervals['end_time'] += pd.Timedelta('15min')\n",
    "profile_intervals = profile_intervals.set_index(['start', 'end'], append = True)\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*notes:*  \n",
    "- start is inclusive, end is exclusive so the interval is $[start, end[$  \n",
    "- start_time and end_time are both exclusive $]start\\_time, end\\_time[$  \n",
    "This works better for plotting in altair\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the missing hour on march 27 due to change from winter to summer time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_intervals = profile_intervals[~((profile_intervals.start_time == '2016-03-27 02:00:00') & (profile_intervals.end_time == '2016-03-27 03:00:00'))]\n",
    "profile_intervals = profile_intervals[~((profile_intervals.start_time == '2016-03-27 01:45:00') & (profile_intervals.end_time == '2016-03-27 03:00:00'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add two next values after each interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile_intervals['value_after_interval'] =\n",
    "def values_after_end(row):\n",
    "    meterID, year, start, end = row.name\n",
    "    # if end is to large\n",
    "    try:\n",
    "        first_value = data16_df.at[(meterID,year), data16_df.columns[end]]\n",
    "    except: \n",
    "        first_value = 'end'\n",
    "    try:\n",
    "        second_value = data16_df.at[(meterID,year), data16_df.columns[end+1]]\n",
    "    except: \n",
    "        second_value = 'end'\n",
    "    return first_value, second_value\n",
    "\n",
    "if 'first_value_after_end' not in profile_intervals.columns:\n",
    "    after_values_df = profile_intervals.apply(values_after_end, axis = 1, result_type = 'expand').rename(columns = {0:'first_value_after_end', 1:'second_value_after_end'})\n",
    "    profile_intervals = pd.concat([profile_intervals, after_values_df], axis = 1)\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first focus on NaN intervals that start and end on the same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_day_intervals = profile_intervals.start_time.dt.date == profile_intervals.end_time.dt.date\n",
    "nan_intervals = profile_intervals.interval_value.isna()\n",
    "nan_intervals = profile_intervals[same_day_intervals & nan_intervals].copy()\n",
    "nan_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The simularity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_as_real_measurement(full_day, missing_day): \n",
    "    iszero = np.concatenate(([0], np.isnan(missing_day).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    values_to_use = np.zeros(missing_day.shape).astype('int')\n",
    "    for start,end in ranges: \n",
    "        values_to_use[start:end] = 1\n",
    "    # euclidean distance of known values (without value after missing interval)\n",
    "    v1 = missing_day[values_to_use]\n",
    "    v2 = full_day[values_to_use]\n",
    "    euclidean = np.linalg.norm(v1-v2)\n",
    "    \n",
    "    # distances between values after missing intervals\n",
    "    other_vector = []\n",
    "    indices_to_use = ranges[:,1]\n",
    "    other_part = np.linalg.norm(missing_day[indices_to_use]- full_day[indices_to_use])\n",
    "    return euclidean + other_part\n",
    "\n",
    "def sim_as_cumulative_measurement(full_day, missing_day): \n",
    "    iszero = np.concatenate(([0], np.isnan(missing_day).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    values_to_use = np.zeros(missing_day.shape).astype('int')\n",
    "    for start,end in ranges: \n",
    "        values_to_use[start:end] = 1\n",
    "        \n",
    "    # euclidean distance of known part\n",
    "    v1 = missing_day[values_to_use]\n",
    "    v2 = full_day[values_to_use]\n",
    "    euclidean = np.linalg.norm(v1-v2)\n",
    "    \n",
    "    # distance between cumulative measurements and the sum of the measurement during missing interval \n",
    "    other_vector = []\n",
    "    for start, end in ranges: \n",
    "        consumption_during_missing = np.sum(full_day[start:end+1] )\n",
    "        cumulative_measurement = missing_day[end]\n",
    "        other_vector.append(consumption_during_missing - cumulative_measurement)\n",
    "    other_part = np.linalg.norm(other_vector)\n",
    "    return euclidean + other_part\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "def construct_search_intervals(start_time, end_time, reference_day_window, context_size): \n",
    "    \"\"\"\n",
    "        This function constructs all the parts of the timeseries that we can compare with \n",
    "        (same period as the missing period with context_size /2 added to both sides for every day in 'reference_day_window' around the missing day)\n",
    "    \"\"\"\n",
    "    reference_day_window_one_side = pd.Timedelta(days = ceil(reference_day_window / 2))\n",
    "    reference_day_window_size = reference_day_window_one_side * 2 + pd.Timedelta(days = 1) # one additional day for the day with the missing value\n",
    "    context_size = pd.Timedelta(context_size)\n",
    "    context_size_one_side = context_size / 2 \n",
    "    \n",
    "    reference_period_length = end_time - start_time + context_size\n",
    "    \n",
    "    # the first search interval starts at start_time of the missing interval - half the context size - half of the day window size \n",
    "    first_reference_period_start = start_time - context_size_one_side - reference_day_window_one_side\n",
    "    final_reference_period_start = first_reference_period_start + reference_day_window_size\n",
    "    search_interval_starts = pd.date_range(first_reference_period_start, final_reference_period_start, freq = 'D')\n",
    "    search_interval_ends = pd.date_range(first_reference_period_start + reference_period_length, final_reference_period_start + reference_period_length, freq = 'D')\n",
    "    df = search_interval_starts.to_frame(name = 'start').reset_index(drop = True)\n",
    "    df['end'] = search_interval_ends\n",
    "    # filter out intervals that fall outside of known range\n",
    "    min_date, max_date = data16_df.columns.min(), data16_df.columns.max()\n",
    "    before_start = df['start'] < min_date \n",
    "    after_end = df['end'] > max_date\n",
    "    return df[~before_start & ~after_end]\n",
    "\n",
    "def add_data_to_search_intervals(meterID,year, search_interval_df): \n",
    "    \"\"\"\n",
    "        Make a dataframe with the data from all the periods in search_interval_df\n",
    "    \"\"\"\n",
    "    def get_data(row): \n",
    "        start, end = row\n",
    "        return data16_df.loc[(meterID,year), start:end].values \n",
    "    \n",
    "    data_df = search_interval_df.apply(get_data, axis = 1, result_type = 'expand')\n",
    "    start, end = search_interval_df.iloc[0]\n",
    "    new_start = start.replace(year = 2016, month = 1, day = 1)\n",
    "    new_end = end.replace(year = 2016, month = 1, day = 1) +(end.date() - start.date())\n",
    "    data_df.columns = pd.date_range(new_start, new_end, freq = '15min')\n",
    "    data_df.index = pd.MultiIndex.from_frame(search_interval_df)\n",
    "    return data_df\n",
    "\n",
    "def match_interval_wrapper(row): \n",
    "    \"\"\" \n",
    "        Simple helper function to call match_interval from an apply call of pandas \n",
    "    \"\"\"\n",
    "    meterID, year, _, _ = row.name \n",
    "    start_time, end_time = row['start_time'] , row['end_time']\n",
    "    return match_interval(meterID, year, start_time, end_time).squeeze()\n",
    "\n",
    "def match_interval(meterID, year, start_time, end_time, reference_day_window = 30, context_size = '4H'):\n",
    "    \"\"\"\n",
    "        Function that will find the best match to the missing interval of meter meterID, year year between start_time and end_time\n",
    "    \"\"\"\n",
    "    # make the dataframe with all the relevant data\n",
    "    search_intervals_df = construct_search_intervals(start_time, end_time, reference_day_window, context_size)\n",
    "    data_df = add_data_to_search_intervals(meterID, year, search_intervals_df)\n",
    "    \n",
    "    # seperate the missing day from all the other days\n",
    "    missing_day = data_df.loc[start_time - pd.Timedelta(context_size)/2]\n",
    "    reference_days = data_df.drop(index = start_time-pd.Timedelta(context_size)/2)\n",
    "    \n",
    "    # drop reference days with data problems\n",
    "    reference_days.dropna(inplace = True)\n",
    "    \n",
    "    # calculate the similarity between missing day and each reference day\n",
    "    try:\n",
    "        distances_real_measurement = reference_days.apply(sim_as_real_measurement, axis = 1, missing_day = missing_day.squeeze().to_numpy(), raw = True)\n",
    "        distances_cum_measurement = reference_days.apply(sim_as_cumulative_measurement, axis = 1, missing_day = missing_day.squeeze().to_numpy(), raw = True)\n",
    "        distances = distances_real_measurement.to_frame('real')\n",
    "        distances['cumulative'] = distances_cum_measurement\n",
    "    except: \n",
    "        print(f\"error in profile {meterID}, {start_time}, {end_time}\")\n",
    "        return pd.DataFrame([[np.nan]*5], columns = ['real_distance', 'cumulative_distance', 'real_match', 'cumulative_match', 'best_match'])\n",
    "    \n",
    "    # calculate the smallest distances\n",
    "    best_real_distance, best_cumulative_distance = distances.min(axis = 0)\n",
    "    best_real_match_date = distances.index[np.argmin(distances['real'])][0] + pd.Timedelta(context_size)/2\n",
    "    best_cumulative_match_date = distances.index[np.argmin(distances['cumulative'])][0] + pd.Timedelta(context_size)/2\n",
    "    best_match = 'real' if best_real_distance < best_cumulative_distance else 'cumulative'\n",
    "    return pd.DataFrame([[best_real_distance, best_cumulative_distance, best_real_match_date, best_cumulative_match_date, best_match]], columns = ['real_distance', 'cumulative_distance', 'real_match', 'cumulative_match', 'best_match'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_interval = nan_intervals.iloc[0]\n",
    "\n",
    "meterID, year, _, _ = test_interval.name\n",
    "_, _, start_time, end_time, *_ = test_interval \n",
    "distances = match_interval(meterID, year, start_time, end_time) \n",
    "distances "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply to the all intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = nan_intervals\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = test_set.apply(match_interval_wrapper, axis = 1)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.concat([test_set, matches], axis = 1)\n",
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_profile_with_intervals(meterID, year, period_type_column = None, data = None, daterange = None):\n",
    "    # plots the profile, using the period data in data \n",
    "    # the color can be determined using the period_type_column\n",
    "    if data is None : \n",
    "        data = nan_intervals\n",
    "    if daterange is not None: \n",
    "        start_time =  f'2016-{daterange[0]}-1 00:00:00'\n",
    "        end_time = f'2016-{daterange[1]}-1 00:00:00'\n",
    "        profile_df = data16_df.loc[(meterID, year),start_time:end_time]\n",
    "        periods_for_profile =data.loc[(meterID,year), :]\n",
    "        periods_for_profile = periods_for_profile[(periods_for_profile['end_time'] > start_time ) & (periods_for_profile['start_time'] < end_time)]\n",
    "    else: \n",
    "        profile_df = data16_df.loc[(meterID, year),:]\n",
    "        periods_for_profile =data.loc[(meterID,year), :]\n",
    "        \n",
    "#     print(periods_for_profile[['start_time', 'end_time']])\n",
    "#     print(zero_periods_for_profile[['start_time', 'end_time', 'is_disconnection_period']])\n",
    "    line = alt.Chart(profile_df.to_frame('value').reset_index()).mark_line().encode(\n",
    "        x = alt.X('timestamp:T'), \n",
    "        y = alt.Y('value:Q')\n",
    "    )\n",
    "    if period_type_column is None: \n",
    "        color_encoding = alt.ColorValue('blue') \n",
    "    else: \n",
    "        color_encoding = alt.Color(f'{period_type_column}:N')\n",
    "    plot_df =periods_for_profile.reset_index(drop=True)\n",
    "    rect = alt.Chart(plot_df).mark_rect(opacity = 0.6).encode(\n",
    "        x = 'start_time:T',\n",
    "        x2 = 'end_time:T', \n",
    "        color = color_encoding\n",
    "    ) + alt.Chart(plot_df).mark_circle(opacity = 0.6).encode(\n",
    "        x = 'start_time:T',\n",
    "        y = alt.YValue(profile_df.max()),\n",
    "#         x2 = 'end_time:T', \n",
    "        color = color_encoding\n",
    "    )\n",
    "    chart = rect + line\n",
    "    if 'connection_power' in periods_for_profile.columns: \n",
    "        connection_power = float(periods_for_profile.connection_power.iat[0])\n",
    "\n",
    "        connection_power_line = alt.Chart(periods_for_profile.reset_index()).mark_rule(color = 'black', opacity = 0.8).encode(\n",
    "            y =  'mean(connection_power):Q'\n",
    "        )\n",
    "        chart += connection_power_line\n",
    "    return chart.properties(width = 2200, title = f\"{meterID} in {year}\").interactive()\n",
    "\n",
    "def plot_helper(profile_idx): \n",
    "    meterID = test_set.index.get_level_values(0).unique()[profile_idx]\n",
    "    return plot_profile_with_intervals(meterID, 2016, 'best_match', data = test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFILE_TO_CHECK = 28\n",
    "plot_helper(PROFILE_TO_CHECK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meterID = test_set.index.get_level_values(0).unique()[PROFILE_TO_CHECK]\n",
    "print(meterID)\n",
    "test_set.loc[meterID]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculating some statistics about real/cumulative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = test_set.best_match.value_counts().to_frame('count').reset_index()\n",
    "alt.Chart(temp).mark_bar().encode(\n",
    "    x = 'index', \n",
    "    y = 'count'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the real measurements into depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_measurements = test_set[test_set.best_match == 'real']\n",
    "real_measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the once that have a zero and then a high value \n",
    "real_measurements = real_measurements.query('~(first_value_after_end == 0 & second_value_after_end > 1)')\n",
    "real_measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_helper2(index): \n",
    "    meterID = real_measurements.index.get_level_values(0).unique()[index]\n",
    "    return plot_profile_with_intervals(meterID, 2016, 'best_match', data = test_set)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 5\n",
    "plot_helper2(IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meterID = real_measurements.index.get_level_values(0).unique()[IDX]\n",
    "print(meterID)\n",
    "test_set.loc[meterID]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at some of the longer intervals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_measurements_by_length = real_measurements.sort_values('interval_length', ascending = False)\n",
    "real_measurements_by_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_helper3(index): \n",
    "    meterID = real_measurements_by_length.index.get_level_values(0).unique()[index]\n",
    "    return plot_profile_with_intervals(meterID, 2016, 'best_match', data = test_set)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 1\n",
    "plot_helper3(IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meterID = real_measurements_by_length.index.get_level_values(0).unique()[IDX]\n",
    "print(meterID)\n",
    "test_set.loc[meterID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
