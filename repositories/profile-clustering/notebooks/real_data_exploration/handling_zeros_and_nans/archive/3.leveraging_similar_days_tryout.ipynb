{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging similar days: try-out\n",
    "The main idea is when filling in the missing values in a profile, use the most similar day. \n",
    "\n",
    "In this notebook I also try using this idea to figure out if the value after a peak is a cumulative measurement or not: \n",
    "\n",
    "1. search for the most similar day based on simularity metric that assumes that the value after the missing interval is a cumulative measurement  \n",
    "2. search the most similar day based on a simularity metric that assumes that the value after the missing interval is a normal measurement  \n",
    "Check if which of these similar days is the most similar and use that one.  \n",
    "I think this would result in using the similar day from 2. whenever the peak is not abnormal and using the day from 1. if it is abnormal  \n",
    "\n",
    "its a bit more adaptive then using a normal distribution learned on similar days because it takes into account context  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import datetime\n",
    "import tqdm\n",
    "idx = pd.IndexSlice\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/preprocessed/combined')\n",
    "info_path = PRE_PATH/'info.csv'\n",
    "data_path = PRE_PATH/'data.csv'\n",
    "assert info_path.exists() and data_path.exists(), 'These paths should exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = pd.read_csv(info_path, index_col = [0,1])\n",
    "info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(data_path, index_col = [0,1])\n",
    "data_df.columns = pd.to_datetime(data_df.columns)\n",
    "data_df.columns.name = 'timestamp'\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leap_years = [2012, 2016]\n",
    "non_leap_years = [year for year in info_df.index.levels[1] if year not in leap_years]\n",
    "print(f'leap years = {leap_years}')\n",
    "print(f'non leap years = {non_leap_years}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle all data sources and years seperately\n",
    "Of course connection problems need to be in the same year and within the same measurement project, so for now lets use the EandisVREG data of 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = 'EandisVREG'\n",
    "YEAR = 2016\n",
    "# get the right subset based on the info df\n",
    "info16_df = info_df.loc[idx[:, 2016],:]\n",
    "info16_df = info16_df[info16_df.data_source == 'EandisVREG']\n",
    "info16_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the corresponding data profiles \n",
    "data16_df = data_df.loc[info16_df.index, :]\n",
    "data16_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the amount of NaNs and Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb of zeros for each profile\n",
    "nb_of_na = (data16_df.isna()).sum(axis = 1)\n",
    "nb_of_zeros = (data16_df == 0).sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at profiles with potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data16_df= data16_df.loc[(nb_of_na>0)| (nb_of_zeros>0), :]\n",
    "data16_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the intervals\n",
    "So in the rest of this code we simply construct the intervals as a dataset and add different attributes/features and investigate whether they could be useful or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to find intervals with only zeros\n",
    "def value_interval(meterID, year, a, value):\n",
    "    \"\"\"\n",
    "        Makes a dataframe containing the start and end of each interval (only the longest intervals) that only contains value\n",
    "    \"\"\"\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    if np.isnan(value):\n",
    "        iszero = np.concatenate(([0], np.isnan(a).view(np.int8), [0]))\n",
    "    else: \n",
    "        iszero = np.concatenate(([0], np.equal(a, value).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    df = pd.DataFrame(ranges, columns = ['start', 'end'])\n",
    "    df['meterID'] = meterID\n",
    "    df['year'] = year\n",
    "    df['interval_value'] = value\n",
    "    return df.set_index(['meterID', 'year'])\n",
    "\n",
    "def zero_nan_intervals_df(data_df): \n",
    "    dfs = []\n",
    "    for (meterID, year), row in data_df.iterrows(): \n",
    "        nan_df = value_interval( meterID, year,row, np.NaN)\n",
    "        zero_df = value_interval( meterID, year, row, 0)\n",
    "        dfs.append(nan_df)\n",
    "        dfs.append(zero_df)\n",
    "    full_df = pd.concat(dfs, axis = 0)\n",
    "#     full_df['start_time'] = data14_df.columns[full_df['start']]\n",
    "#     full_df['end_time'] = data14_df.columns[full_df['end']-1]\n",
    "    return full_df\n",
    "\n",
    "profile_intervals = zero_nan_intervals_df(data16_df)\n",
    "profile_intervals['interval_length'] = profile_intervals.end - profile_intervals.start\n",
    "# start time and end time are exclusive! (this plots well with altair that why we do it this way)\n",
    "profile_intervals['start_time'] = data16_df.columns[profile_intervals['start']] - pd.Timedelta('15min')\n",
    "profile_intervals['end_time'] = data16_df.columns[profile_intervals['end']-1] # doing it this way because the timestamp we need might not exist in the columns\n",
    "profile_intervals['end_time'] += pd.Timedelta('15min')\n",
    "profile_intervals = profile_intervals.set_index(['start', 'end'], append = True)\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*notes:*  \n",
    "- start is inclusive, end is exclusive so the interval is $[start, end[$  \n",
    "- start_time and end_time are both exclusive $]start\\_time, end\\_time[$  \n",
    "This works better for plotting in altair\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the missing hour on march 27 due to change from winter to summer time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_intervals = profile_intervals[~((profile_intervals.start_time == '2016-03-27 02:00:00') & (profile_intervals.end_time == '2016-03-27 03:00:00'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add two next values after each interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile_intervals['value_after_interval'] =\n",
    "def values_after_end(row):\n",
    "    meterID, year, start, end = row.name\n",
    "    # if end is to large\n",
    "    try:\n",
    "        first_value = data16_df.at[(meterID,year), data16_df.columns[end]]\n",
    "    except: \n",
    "        first_value = 'end'\n",
    "    try:\n",
    "        second_value = data16_df.at[(meterID,year), data16_df.columns[end+1]]\n",
    "    except: \n",
    "        second_value = 'end'\n",
    "    return first_value, second_value\n",
    "\n",
    "if 'first_value_after_end' not in profile_intervals.columns:\n",
    "    after_values_df = profile_intervals.apply(values_after_end, axis = 1, result_type = 'expand').rename(columns = {0:'first_value_after_end', 1:'second_value_after_end'})\n",
    "    profile_intervals = pd.concat([profile_intervals, after_values_df], axis = 1)\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first focus on NaN intervals that start and end on the same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_day_intervals = profile_intervals.start_time.dt.date == profile_intervals.end_time.dt.date\n",
    "nan_intervals = profile_intervals.interval_value.isna()\n",
    "nan_intervals = profile_intervals[same_day_intervals & nan_intervals].copy()\n",
    "nan_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the missing day data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_intervals['date'] = nan_intervals.start_time.dt.date\n",
    "days_with_missing_data = nan_intervals.reset_index()[['meterID', 'year', 'date']].drop_duplicates()\n",
    "\n",
    "\n",
    "def get_data(row): \n",
    "    meterID, year, date = row\n",
    "    \n",
    "    return data16_df.loc[(meterID,year), data16_df.columns.date == date].values # for some reason loc directly using the date does not work\n",
    "    \n",
    "day_data = days_with_missing_data.apply(get_data, axis = 1, result_type = 'expand')\n",
    "day_data.columns = pd.date_range('2016-01-01', '2016-01-01 23:45', freq = '15min')\n",
    "day_data.index = pd.MultiIndex.from_frame(days_with_missing_data)\n",
    "days_with_missing_data = day_data \n",
    "days_with_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match these days with the most similar day based on a certain simularity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_as_real_measurement(full_day, missing_day): \n",
    "    iszero = np.concatenate(([0], np.isnan(missing_day).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    values_to_use = np.zeros(missing_day.shape).astype('int')\n",
    "    for start,end in ranges: \n",
    "        values_to_use[start:end] = 1\n",
    "    # euclidean distance of known values (without value after missing interval)\n",
    "    v1 = missing_day[values_to_use]\n",
    "    v2 = full_day[values_to_use]\n",
    "    euclidean = np.linalg.norm(v1-v2)\n",
    "    \n",
    "    # distances between values after missing intervals\n",
    "    other_vector = []\n",
    "    indices_to_use = ranges[:,1]\n",
    "    other_part = np.linalg.norm(missing_day[indices_to_use]- full_day[indices_to_use])\n",
    "    return euclidean + other_part\n",
    "\n",
    "def sim_as_cumulative_measurement(full_day, missing_day): \n",
    "    iszero = np.concatenate(([0], np.isnan(missing_day).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    values_to_use = np.zeros(missing_day.shape).astype('int')\n",
    "    for start,end in ranges: \n",
    "        values_to_use[start:end] = 1\n",
    "        \n",
    "    # euclidean distance of known part\n",
    "    v1 = missing_day[values_to_use]\n",
    "    v2 = full_day[values_to_use]\n",
    "    euclidean = np.linalg.norm(v1-v2)\n",
    "    \n",
    "    # distance between cumulative measurements and the sum of the measurement during missing interval \n",
    "    other_vector = []\n",
    "    for start, end in ranges: \n",
    "        consumption_during_missing = np.sum(full_day[start:end+1] )\n",
    "        cumulative_measurement = missing_day[end]\n",
    "        other_vector.append(consumption_during_missing - cumulative_measurement)\n",
    "    other_part = np.linalg.norm(other_vector)\n",
    "    return euclidean + other_part\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_day_row = days_with_missing_data.iloc[500,:]\n",
    "search_window = 90\n",
    "meterID, year, date = missing_day_row.name\n",
    "print(meterID)\n",
    "min_date, max_date = date - pd.Timedelta(f'{search_window//2}D'), date + pd.Timedelta(f'{search_window//2}D')\n",
    "full_profile = data16_df.loc[(meterID, year),:]\n",
    "dates_to_match = pd.date_range(min_date, max_date, freq = '1D')\n",
    "all_relevant_days = full_profile.loc[min_date:max_date].to_frame('value')\n",
    "all_relevant_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_days = all_relevant_days.copy()\n",
    "relevant_days['date'] = relevant_days.index.date\n",
    "relevant_days['time'] = relevant_days.index.time\n",
    "possible_days = pd.pivot_table(relevant_days, index = 'date', columns = 'time', values = 'value').dropna(axis = 0, how = 'any')\n",
    "possible_days.index = pd.to_datetime(possible_days.index)\n",
    "possible_days.columns = pd.to_datetime(possible_days.columns, format='%H:%M:%S')\n",
    "possible_days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_real_measurement = possible_days.apply(sim_as_real_measurement, axis = 1, missing_day = missing_day_row.to_numpy(), raw = True)\n",
    "distances_cum_measurement = possible_days.apply(sim_as_cumulative_measurement, axis = 1, missing_day = missing_day_row.to_numpy(), raw = True)\n",
    "distances = distances_real_measurement.to_frame('real')\n",
    "distances['cumulative'] = distances_cum_measurement\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_real_match = distances.index[np.argmin(distances['real'])]\n",
    "best_cumulative_match = distances.index[np.argmin(distances['cumulative'])]\n",
    "distances.loc[[best_real_match, best_cumulative_match],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = alt.Chart(missing_day_row.to_frame('value').reset_index(), title = 'missing day').mark_line().encode(\n",
    "    x= 'index:T',\n",
    "    y='value'\n",
    ")\n",
    "full_month = alt.Chart(all_relevant_days.reset_index(), width = 1600, title = 'days to match with').mark_line().encode(\n",
    "     x= 'timestamp:T',\n",
    "    y='value'\n",
    ").interactive(bind_y = False)\n",
    "best_day = alt.Chart(possible_days.loc[best_real_match].to_frame('value').reset_index(), title = 'best_real_match').mark_line().encode(\n",
    "     x= 'time:T',\n",
    "    y='value'\n",
    ")\n",
    "best_cumulative_day = alt.Chart(possible_days.loc[best_cumulative_match].to_frame('value').reset_index(), title = 'best_cumulative_match').mark_line().encode(\n",
    "     x= 'time:T',\n",
    "    y='value'\n",
    ")\n",
    "full_month & (missing_data & best_day & best_cumulative_day).resolve_scale(y='shared')\n",
    "# (missing_data & best_day & best_cumulative_day).resolve_scale(y='shared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
