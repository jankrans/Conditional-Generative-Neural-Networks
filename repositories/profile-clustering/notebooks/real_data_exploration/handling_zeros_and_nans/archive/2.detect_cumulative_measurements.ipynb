{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal here is to detect connection problems in the data such that we can handle these later\n",
    "So the main idea is to look for periods where multiple meters have zero measurements, these periods are called disconnection periods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Imports and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import datetime\n",
    "import tqdm\n",
    "idx = pd.IndexSlice\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/preprocessed/combined')\n",
    "info_path = PRE_PATH/'info.csv'\n",
    "data_path = PRE_PATH/'data.csv'\n",
    "assert info_path.exists() and data_path.exists(), 'These paths should exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = pd.read_csv(info_path, index_col = [0,1])\n",
    "info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(data_path, index_col = [0,1])\n",
    "data_df.columns = pd.to_datetime(data_df.columns)\n",
    "data_df.columns.name = 'timestamp'\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leap_years = [2012, 2016]\n",
    "non_leap_years = [year for year in info_df.index.levels[1] if year not in leap_years]\n",
    "print(f'leap years = {leap_years}')\n",
    "print(f'non leap years = {non_leap_years}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle all data sources and years seperately\n",
    "Of course connection problems need to be in the same year and within the same measurement project, so for now lets use the EandisVREG data of 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = 'EandisVREG'\n",
    "YEAR = 2016\n",
    "# get the right subset based on the info df\n",
    "info16_df = info_df.loc[idx[:, 2016],:]\n",
    "info16_df = info16_df[info16_df.data_source == 'EandisVREG']\n",
    "info16_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the corresponding data profiles \n",
    "data16_df = data_df.loc[info16_df.index, :]\n",
    "data16_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the amount of NaNs and Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb of zeros for each profile\n",
    "nb_of_na = (data16_df.isna()).sum(axis = 1)\n",
    "nb_of_zeros = (data16_df == 0).sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at profiles with potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data16_df= data16_df.loc[(nb_of_na>0)| (nb_of_zeros>0), :]\n",
    "data16_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the intervals\n",
    "So in the rest of this code we simply construct the intervals as a dataset and add different attributes/features and investigate whether they could be useful or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to find intervals with only zeros\n",
    "def value_interval(meterID, year, a, value):\n",
    "    \"\"\"\n",
    "        Makes a dataframe containing the start and end of each interval (only the longest intervals) that only contains value\n",
    "    \"\"\"\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    if np.isnan(value):\n",
    "        iszero = np.concatenate(([0], np.isnan(a).view(np.int8), [0]))\n",
    "    else: \n",
    "        iszero = np.concatenate(([0], np.equal(a, value).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    df = pd.DataFrame(ranges, columns = ['start', 'end'])\n",
    "    df['meterID'] = meterID\n",
    "    df['year'] = year\n",
    "    df['interval_value'] = value\n",
    "    return df.set_index(['meterID', 'year'])\n",
    "\n",
    "def zero_nan_intervals_df(data_df): \n",
    "    dfs = []\n",
    "    for (meterID, year), row in data_df.iterrows(): \n",
    "        nan_df = value_interval( meterID, year,row, np.NaN)\n",
    "        zero_df = value_interval( meterID, year, row, 0)\n",
    "        dfs.append(nan_df)\n",
    "        dfs.append(zero_df)\n",
    "    full_df = pd.concat(dfs, axis = 0)\n",
    "#     full_df['start_time'] = data14_df.columns[full_df['start']]\n",
    "#     full_df['end_time'] = data14_df.columns[full_df['end']-1]\n",
    "    return full_df\n",
    "\n",
    "profile_intervals = zero_nan_intervals_df(data16_df)\n",
    "profile_intervals['interval_length'] = profile_intervals.end - profile_intervals.start\n",
    "# start time and end time are exclusive! (this plots well with altair that why we do it this way)\n",
    "profile_intervals['start_time'] = data16_df.columns[profile_intervals['start']] - pd.Timedelta('15min')\n",
    "profile_intervals['end_time'] = data16_df.columns[profile_intervals['end']-1] # doing it this way because the timestamp we need might not exist in the columns\n",
    "profile_intervals['end_time'] += pd.Timedelta('15min')\n",
    "profile_intervals = profile_intervals.set_index(['start', 'end'], append = True)\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*notes:*  \n",
    "- start is inclusive, end is exclusive so the interval is $[start, end[$  \n",
    "- start_time and end_time are both exclusive $]start\\_time, end\\_time[$  \n",
    "This works better for plotting in altair\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the missing hour on march 27 due to change from winter to summer time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_intervals = profile_intervals[~((profile_intervals.start_time == '2016-03-27 02:00:00') & (profile_intervals.end_time == '2016-03-27 03:00:00'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add two next values after each interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile_intervals['value_after_interval'] =\n",
    "def values_after_end(row):\n",
    "    meterID, year, start, end = row.name\n",
    "    # if end is to large\n",
    "    try:\n",
    "        first_value = data16_df.at[(meterID,year), data16_df.columns[end]]\n",
    "    except: \n",
    "        first_value = 'end'\n",
    "    try:\n",
    "        second_value = data16_df.at[(meterID,year), data16_df.columns[end+1]]\n",
    "    except: \n",
    "        second_value = 'end'\n",
    "    return first_value, second_value\n",
    "\n",
    "if 'first_value_after_end' not in profile_intervals.columns:\n",
    "    after_values_df = profile_intervals.apply(values_after_end, axis = 1, result_type = 'expand').rename(columns = {0:'first_value_after_end', 1:'second_value_after_end'})\n",
    "    profile_intervals = pd.concat([profile_intervals, after_values_df], axis = 1)\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add connection capacity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'connection_power' not in profile_intervals.columns:\n",
    "    connection_power = info16_df[['connection_power']]\n",
    "    profile_intervals = profile_intervals.join(connection_power)\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check peaks due to connection_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'is_connection_power_peak' not in profile_intervals.columns: \n",
    "    # a value is a connection power peak if the first or the second value after the interval is higher than the peak\n",
    "    profile_intervals['is_connection_power_peak'] = (profile_intervals['first_value_after_end'].replace({'end': np.NaN}) > profile_intervals['connection_power'].astype('float'))|(profile_intervals['second_value_after_end'].replace({'end': np.NaN}) > profile_intervals['connection_power'].astype('float'))\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_intervals.is_connection_power_peak.value_counts().to_frame('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly this rule only helps to detect very few peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting for profiles with periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_profile_with_intervals(meterID, year, period_type_column = None, data = None, daterange = None):\n",
    "    # plots the profile, using the period data in data \n",
    "    # the color can be determined using the period_type_column\n",
    "    if data is None : \n",
    "        data = profile_intervals\n",
    "    if daterange is not None: \n",
    "        start_time =  f'2016-{daterange[0]}-1 00:00:00'\n",
    "        end_time = f'2016-{daterange[1]}-1 00:00:00'\n",
    "        profile_df = data16_df.loc[(meterID, year),start_time:end_time]\n",
    "        periods_for_profile =data.loc[(meterID,year), :]\n",
    "        periods_for_profile = periods_for_profile[(periods_for_profile['end_time'] > start_time ) & (periods_for_profile['start_time'] < end_time)]\n",
    "    else: \n",
    "        profile_df = data16_df.loc[(meterID, year),:]\n",
    "        periods_for_profile =data.loc[(meterID,year), :]\n",
    "        \n",
    "#     print(periods_for_profile[['start_time', 'end_time']])\n",
    "#     print(zero_periods_for_profile[['start_time', 'end_time', 'is_disconnection_period']])\n",
    "    line = alt.Chart(profile_df.to_frame('value').reset_index()).mark_line().encode(\n",
    "        x = alt.X('timestamp:T'), \n",
    "        y = alt.Y('value:Q')\n",
    "    )\n",
    "    if period_type_column is None: \n",
    "        color_encoding = alt.ColorValue('blue') \n",
    "    else: \n",
    "        color_encoding = alt.Color(f'{period_type_column}:N')\n",
    "    plot_df =periods_for_profile.reset_index(drop=True)\n",
    "    rect = alt.Chart(plot_df).mark_rect(opacity = 0.6).encode(\n",
    "        x = 'start_time:T',\n",
    "        x2 = 'end_time:T', \n",
    "        color = color_encoding\n",
    "    ) + alt.Chart(plot_df).mark_circle(opacity = 0.6).encode(\n",
    "        x = 'start_time:T',\n",
    "        y = alt.YValue(profile_df.max()),\n",
    "#         x2 = 'end_time:T', \n",
    "        color = color_encoding\n",
    "    )\n",
    "    chart = rect + line\n",
    "    if 'connection_power' in periods_for_profile.columns: \n",
    "        connection_power = float(periods_for_profile.connection_power.iat[0])\n",
    "\n",
    "        connection_power_line = alt.Chart(periods_for_profile.reset_index()).mark_rule(color = 'black', opacity = 0.8).encode(\n",
    "            y =  'mean(connection_power):Q'\n",
    "        )\n",
    "        chart += connection_power_line\n",
    "    return chart.properties(width = 2200, title = f\"{meterID} in {year}\").interactive()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a normal distribution\n",
    "For each profile figure out thresholds based on the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thresholds(row, prominence = 0.5): \n",
    "    X = row.fillna(0).to_numpy()\n",
    "    peaks, _ = find_peaks(X, prominence = prominence)\n",
    "    x_to_use = X[peaks]\n",
    "    mu, std = norm.fit(x_to_use)\n",
    "    _, max_thres = norm.interval(0.99, mu, std)\n",
    "    min_thres = np.nan\n",
    "    if profile.min() < 0:\n",
    "        # this profile has injection so negative peaks are possible\n",
    "        inverse_peaks, _ = find_peaks(-X, prominence = prominence)\n",
    "        x_to_use = X[peaks]\n",
    "        mu, std = norm.fit(x_to_use)\n",
    "        min_thres, _ = norm.interval(0.99, mu, std)\n",
    "    return min_thres ,max_thres\n",
    "if 'gauss_min_threshold' not in profile_intervals.columns: \n",
    "    thresholds = data16_df.apply(get_thresholds, axis = 1, result_type = 'expand', prominence = 0.3).rename(columns = {0:'gauss_min_threshold',1:'gauss_max_threshold'})\n",
    "    profile_intervals = profile_intervals.join(thresholds)\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_intervals['is_gauss_peak'] = profile_intervals.replace({'end': np.NaN}).eval('(first_value_after_end < gauss_min_threshold) | (first_value_after_end > gauss_max_threshold)')\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meterIDs = profile_intervals.index.levels[0]\n",
    "plot_profile_with_intervals(meterIDs[14], 2016, 'is_gauss_peak')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
