{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify peaks as cumulative or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify the measurements after the NaN and zero periods as cumulative or not based on peak detection**\n",
    "\n",
    "- **#1**: For each NaN/zero interval, check whether the value that follows is greater than peak_factor times the average value of the next M-1 values (peak_factor and M are parameters)\n",
    "\n",
    "- **#2**: For each meter, check whether the values after the NaN/zero intervals are higher than the average peak length\n",
    "\n",
    "- **#3**: For each NaN/zero interval, check whether the value that follows is among the detected peaks\n",
    "\n",
    "- **#4**: For each meter, apply 2-medoids to the M samples that follow the NaN/zero intervals to classify cumulative measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import datetime\n",
    "import tqdm\n",
    "import random\n",
    "from scipy.signal import find_peaks, find_peaks_cwt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import warnings\n",
    "idx = pd.IndexSlice\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/preprocessed/combined')\n",
    "info_path = PRE_PATH/'info.csv'\n",
    "data_path = PRE_PATH/'data.csv'\n",
    "assert info_path.exists() and data_path.exists(), 'These paths should exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = pd.read_csv(info_path, index_col = [0,1])\n",
    "info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(data_path, index_col = [0,1])\n",
    "data_df.columns = pd.to_datetime(data_df.columns)\n",
    "data_df.columns.name = 'timestamp'\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle all data sources and years seperately\n",
    "Of course connection problems need to be in the same year and within the same measurement project, so for now lets use the EandisVREG data of 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = 'EandisVREG'\n",
    "YEAR = 2016\n",
    "# get the right subset based on the info df\n",
    "info16_df = info_df.loc[idx[:, 2016],:]\n",
    "info16_df = info16_df[info16_df.data_source == 'EandisVREG']\n",
    "info16_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the corresponding data profiles \n",
    "data16_df = data_df.loc[info16_df.index, :]\n",
    "data16_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a random subset of data (some of the meters) for efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(333)\n",
    "\n",
    "#meterIDs = data16_df.index.get_level_values(0).unique().values\n",
    "#meterIDs_selected = random.sample(meterIDs.tolist(), int(len(meterIDs)*0.01))\n",
    "\n",
    "#info16_df = info16_df.query(f'meterID in {meterIDs_selected}')\n",
    "#data16_df = data16_df.query(f'meterID in {meterIDs_selected}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the amount of NaNs and Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb of zeros for each profile\n",
    "nb_of_na = (data16_df.isna()).sum(axis = 1)\n",
    "nb_of_zeros = (data16_df == 0).sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at profiles with potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data16_df= data16_df.loc[(nb_of_na>0)| (nb_of_zeros>0), :]\n",
    "data16_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the intervals\n",
    "So in the rest of this code we simply construct the intervals as a dataset and add different attributes/features and investigate whether they could be useful or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to find intervals with only zeros\n",
    "def value_interval(meterID, year, a, value):\n",
    "    \"\"\"\n",
    "        Makes a dataframe containing the start and end of each interval (only the longest intervals) that only contains value\n",
    "    \"\"\"\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    if np.isnan(value):\n",
    "        iszero = np.concatenate(([0], np.isnan(a).view(np.int8), [0]))\n",
    "    else: \n",
    "        iszero = np.concatenate(([0], np.equal(a, value).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    df = pd.DataFrame(ranges, columns = ['start', 'end'])\n",
    "    df['meterID'] = meterID\n",
    "    df['year'] = year\n",
    "    df['interval_value'] = value\n",
    "    return df.set_index(['meterID', 'year'])\n",
    "\n",
    "def zero_nan_intervals_df(data_df): \n",
    "    dfs = []\n",
    "    for (meterID, year), row in data_df.iterrows(): \n",
    "        nan_df = value_interval( meterID, year,row, np.NaN)\n",
    "        zero_df = value_interval( meterID, year, row, 0)\n",
    "        dfs.append(nan_df)\n",
    "        dfs.append(zero_df)\n",
    "    full_df = pd.concat(dfs, axis = 0)\n",
    "#     full_df['start_time'] = data14_df.columns[full_df['start']]\n",
    "#     full_df['end_time'] = data14_df.columns[full_df['end']-1]\n",
    "    return full_df\n",
    "\n",
    "profile_intervals = zero_nan_intervals_df(data16_df)\n",
    "profile_intervals['interval_length'] = profile_intervals.end - profile_intervals.start\n",
    "# start time and end time are exclusive! (this plots well with altair that why we do it this way)\n",
    "profile_intervals['start_time'] = data16_df.columns[profile_intervals['start']] - pd.Timedelta('15min')\n",
    "profile_intervals['end_time'] = data16_df.columns[profile_intervals['end']-1] # doing it this way because the timestamp we need might not exist in the columns\n",
    "profile_intervals['end_time'] += pd.Timedelta('15min')\n",
    "profile_intervals = profile_intervals.set_index(['start', 'end'], append = True)\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*notes:*  \n",
    "- start is inclusive, end is exclusive so the interval is $[start, end[$  \n",
    "- start_time and end_time are both exclusive $]start\\_time, end\\_time[$  \n",
    "This works better for plotting in altair\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the missing hour on march 27 due to change from winter to summer time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_intervals = profile_intervals[~((profile_intervals.start_time == '2016-03-27 02:00:00') & (profile_intervals.end_time == '2016-03-27 03:00:00'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add M values before and M values after each interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile_intervals['value_after_interval'] =\n",
    "def values_after_end(row, M):\n",
    "    meterID, year, start, end = row.name\n",
    "    # if end is to large\n",
    "    \n",
    "    # slow:\n",
    "    #val = []\n",
    "    #for m in range(M):\n",
    "    #    try:\n",
    "    #        val.append(data16_df.at[(meterID,year), data16_df.columns[end+m]])\n",
    "    #    except: \n",
    "    #        val.append('end')\n",
    "    \n",
    "    # fast:\n",
    "    #time_inds = pd.date_range(start=data16_df.columns[end-1] + datetime.timedelta(minutes=15), freq='15min', periods=M)\n",
    "    #val = data16_df.loc[(meterID,year)].reindex(time_inds, axis=1, fill_value='end').values.tolist()\n",
    "    \n",
    "    # faster:\n",
    "    val = data16_df.loc[(meterID,year)].iloc[end:end+M].values.tolist() # may return a shorter array if the index exceeds the length\n",
    "    if len(val) < M:\n",
    "        val = val + ['end']*(M - len(val))\n",
    "    \n",
    "    return val\n",
    "\n",
    "def values_before_start(row, M):\n",
    "    meterID, year, start, end = row.name\n",
    "\n",
    "    # fast:\n",
    "    #time_inds_prev = pd.date_range(end=data16_df.columns[start] - datetime.timedelta(minutes=15), freq='15min', periods=M)\n",
    "    #val = data16_df.loc[(meterID,year)].reindex(time_inds_prev, axis=1, fill_value='start').values.tolist()\n",
    "    \n",
    "    # faster:\n",
    "    val = data16_df.loc[(meterID,year)].iloc[max(start-M, 0):start].values.tolist() # may return a shorter array if the index exceeds starts before 0\n",
    "    if len(val) < M:\n",
    "        val = ['start']*(M - len(val)) + val\n",
    "    \n",
    "    return val\n",
    "\n",
    "M = 8\n",
    "\n",
    "if f'value{M}_before_start' not in profile_intervals.columns:\n",
    "    print('Adding values before start...')\n",
    "    before_values_df = profile_intervals.apply(lambda o: values_before_start(o, M), axis = 1, result_type = 'expand').rename(columns = lambda o: f'value{o+1}_before_start')\n",
    "    profile_intervals = pd.concat([profile_intervals, before_values_df], axis = 1)\n",
    "\n",
    "if f'value{M}_after_end' not in profile_intervals.columns:\n",
    "    print('Adding values after end...')\n",
    "    after_values_df = profile_intervals.apply(lambda o: values_after_end(o, M), axis = 1, result_type = 'expand').rename(columns = lambda o: f'value{o+1}_after_end')\n",
    "    profile_intervals = pd.concat([profile_intervals, after_values_df], axis = 1)\n",
    "\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add connection capacity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'connection_power' not in profile_intervals.columns:\n",
    "    connection_power = info16_df[['connection_power']]\n",
    "    profile_intervals = profile_intervals.join(connection_power)\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check peaks due to connection_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'is_connection_power_peak' not in profile_intervals.columns: \n",
    "    # a value is a connection power peak if the first or the second value after the interval is higher than the peak\n",
    "    profile_intervals['is_connection_power_peak'] = (profile_intervals['value1_after_end'].astype(object).replace({'end': np.NaN}) > profile_intervals['connection_power'].astype('float'))|(profile_intervals['value2_after_end'].astype(object).replace({'end': np.NaN}) > profile_intervals['connection_power'].astype('float'))\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_intervals.is_connection_power_peak.value_counts().to_frame('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly this rule only helps to detect very few peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect peaks in the whole data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select one of different methods&parameters to find peaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#peaks16_df = data16_df.apply(lambda o: find_peaks(o.values, \n",
    "#                                                  height=0, \n",
    "#                                                  width=[1,1], \n",
    "#                                                  plateau_size=[1,1], \n",
    "#                                                  distance=4\n",
    "#                                                 )[0], axis=1) \\\n",
    "#    .to_frame().rename(columns={0:'peak_locations'})\n",
    "\n",
    "# finds only sharp peaks, may not find peaks after NaNs!\n",
    "#peaks16_df = data16_df.join(info16_df) \\\n",
    "#    .apply(lambda o: find_peaks(o.values[0:data16_df.shape[1]], \n",
    "#                                #height=0, \n",
    "#                                #threshold=float(o['connection_power'])*0.01,\n",
    "#                                prominence=float(o['connection_power'])*0.05,\n",
    "#                                width=[1,4], \n",
    "#                                #distance=4\n",
    "#                               )[0], axis=1) \\\n",
    "#    .to_frame().rename(columns={0:'peak_locations'})\n",
    "\n",
    "# finds only sharp peaks, NaNs are filled with the preceding value:\n",
    "peaks16_df = data16_df.fillna(method='ffill', axis=1).join(info16_df) \\\n",
    "    .apply(lambda o: find_peaks(o.values[0:data16_df.shape[1]], \n",
    "                                #height=0, \n",
    "                                #threshold=float(o['connection_power'])*0.01,\n",
    "                                prominence=float(o['connection_power'])*0.05,\n",
    "                                width=[1,4], \n",
    "                                #distance=4\n",
    "                               )[0], axis=1) \\\n",
    "    .to_frame().rename(columns={0:'peak_locations'})\n",
    "\n",
    "# slow!:\n",
    "#peaks16_df = data16_df.join(info16_df) \\\n",
    "#    .apply(lambda o: find_peaks_cwt(o.values[0:data16_df.shape[1]], \n",
    "#                                    widths=np.arange(1,40),\n",
    "#                                   ), axis=1) \\\n",
    "#    .to_frame().rename(columns={0:'peak_locations'})\n",
    "\n",
    "peaks16_df['nb_of_peaks'] = peaks16_df.apply(lambda o: len(o['peak_locations']), axis=1)\n",
    "peaks16_df = peaks16_df.join(data16_df.join(peaks16_df).apply(\n",
    "    lambda o: np.array(o[o['peak_locations']]), axis=1).to_frame().rename(columns={0:'peak_values'}))\n",
    "peaks16_df['average_peak_value'] = peaks16_df.apply(lambda o: o['peak_values'].mean() if len(o['peak_values']) > 0 else np.nan, axis=1)\n",
    "\n",
    "peaks16_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_peaks = peaks16_df['peak_values'].values\n",
    "all_peaks = np.concatenate(all_peaks)\n",
    "print('Number of total peaks:', sum(peaks16_df['nb_of_peaks']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(pd.DataFrame({'peak_values':all_peaks}), title='histogram').mark_bar().encode(\n",
    "    alt.X('peak_values:Q', bin=alt.Bin(step=0.1), axis=alt.Axis(title='peak values')),\n",
    "    alt.Y('count()', scale=alt.Scale(type='log'), axis=alt.Axis(title='number of peaks'))\n",
    ").properties(width = 750).interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile_intervals.filter(regex='value[0-9]+_before_start|value[0-9]+_after_end', axis=1).iloc[2:3]\n",
    "#profile_intervals.filter(regex='value[0-9]+_after_end', axis=1).iloc[2].values\n",
    "\n",
    "# Peaks cannot be found like this!:\n",
    "#value_after_end_columns = np.where([bool(re.compile('value[0-9]+_after_end').match(i)) for i in profile_intervals.columns.values])[0]\n",
    "#is_first_sample_peak = lambda o: 0 in find_peaks(o[value_after_end_columns].replace({'end':np.nan}).values)[0]\n",
    "#profile_intervals['interval_after_end_starts_with_peak'] = profile_intervals.apply(is_first_sample_peak, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to detect whether the values are cumulative or not:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **#1**: For each NaN/zero interval, check whether the value that follows is greater than peak_factor times the average value of the next M-1 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "peak_factor = 5\n",
    "\n",
    "value_after_end_columns = np.where([bool(re.compile('value[0-9]+_after_end').match(i)) for i in profile_intervals.columns.values])[0]\n",
    "def is_cumulative_based_on_factor(row):\n",
    "    val1 = row['value1_after_end']\n",
    "    if val1 == 'end' or not np.isfinite(val1):\n",
    "        return False\n",
    "    else:\n",
    "        #return val1 > peak_factor * row.filter(regex='value[0-9]+_after_end').replace({'end':np.nan})[1:].mean() #slow\n",
    "        return val1 > peak_factor * row[value_after_end_columns].replace({'end':np.nan})[1:].mean() #fast\n",
    "\n",
    "profile_intervals['is_cumulative_based_on_factor'] = profile_intervals.apply(is_cumulative_based_on_factor, axis=1)\n",
    "\n",
    "print(f'Out of {profile_intervals.shape[0]} measurements, {profile_intervals[\"is_cumulative_based_on_factor\"].sum()} ({profile_intervals[\"is_cumulative_based_on_factor\"].mean()*10}%) are considered as peaks.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **#2**: For each meter, check whether the values after the NaN/zero intervals are higher than the average peak length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_intervals['is_cumulative_based_on_average_peak_value'] = profile_intervals.join(peaks16_df).apply(lambda o: o['value1_after_end'] > o['average_peak_value'] if not o['value1_after_end'] == 'end' else False, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **#3**: For each NaN/zero interval, check whether the value that follows is among the detected peaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_first_sample_global_peak = lambda o: (o['end'] in peaks16_df.loc[o['meterID'], o['year']]['peak_locations'])\n",
    "profile_intervals['is_first_sample_global_peak'] = profile_intervals.reset_index().apply(is_first_sample_global_peak, axis=1).values\n",
    "profile_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **#4**: For each meter, apply 2-medoids to the M samples that follow the NaN/zero intervals to classify cumulative measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in tqdm.tqdm(profile_intervals.groupby(['meterID', 'year'])):\n",
    "    X = group.iloc[:, value_after_end_columns].copy()\n",
    "    X.replace('end', np.nan, inplace=True) # treat 'end's as missing values\n",
    "    Xn = X.dropna() # clustering methods don't like missing values so omit these rows\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        clu = KMedoids(n_clusters=2, random_state=222).fit(Xn.values)\n",
    "    X.loc[Xn.index, 'is_cumulative_based_on_clustering'] = clu.labels_\n",
    "    cumulative_cluster = np.argmax(list(map(lambda o: o[0], clu.cluster_centers_))) # cluster with the largest first measurement\n",
    "    X['is_cumulative_based_on_clustering'] = X['is_cumulative_based_on_clustering'] \\\n",
    "        .map({cumulative_cluster:True, int(not cumulative_cluster):False}) # change cluster numbers to True and False\n",
    "    X['is_cumulative_based_on_clustering'].fillna(False, inplace=True) # classify the ones with missing values as 'real'\n",
    "    profile_intervals.loc[X.index, 'is_cumulative_based_on_clustering'] = X['is_cumulative_based_on_clustering']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the intervals and the measurements after them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_dataframe(df, lst_cols, fill_value=''):\n",
    "    # make sure `lst_cols` is a list\n",
    "    if lst_cols and not isinstance(lst_cols, list):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "\n",
    "    if (lens > 0).all():\n",
    "        # ALL lists in cells aren't empty\n",
    "        return pd.DataFrame({\n",
    "            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n",
    "            for col in idx_cols\n",
    "        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n",
    "          .loc[:, df.columns]\n",
    "    else:\n",
    "        # at least one list in cells is empty\n",
    "        return pd.DataFrame({\n",
    "            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n",
    "            for col in idx_cols\n",
    "        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n",
    "          .append(df.loc[lens==0, idx_cols]).fillna(fill_value) \\\n",
    "          .loc[:, df.columns]\n",
    "\n",
    "peaks16_df_exploded = explode_dataframe(peaks16_df.reset_index(), ['peak_locations', 'peak_values'], np.nan).set_index(['meterID', 'year'])\n",
    "\n",
    "peaks16_df_exploded['peak_time'] = peaks16_df_exploded.apply(\n",
    "    lambda o: data16_df.columns[int(o['peak_locations'])] if not np.isnan(o['peak_locations']) else np.nan, axis=1)\n",
    "\n",
    "peaks16_df_exploded.rename(columns={'peak_locations':'peak_location', 'peak_values':'peak_value', 'nb_of_peaks':'nb_of_peaks_for_meter', 'average_peak_value':'average_peak_value_for_meter'}, inplace=True)\n",
    "\n",
    "peaks16_df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_profile_with_intervals_and_peaks(meterID, year, period_type_column = None, data = None, peak_data = None, peak_type_column = None, daterange = None):\n",
    "    # plots the profile, using the period data in data and peak data in peak_data\n",
    "    # the color can be determined using the period_type_column\n",
    "    if data is None : \n",
    "        data = profile_intervals\n",
    "    if peak_data is None:\n",
    "        peak_data = peaks16_df_exploded.loc[(meterID, year)].copy()\n",
    "    if daterange is not None: \n",
    "        start_time =  f'2016-{daterange[0]}-1 00:00:00'\n",
    "        end_time = f'2016-{daterange[1]}-1 00:00:00'\n",
    "        profile_df = data16_df.loc[(meterID, year),start_time:end_time]\n",
    "        periods_for_profile =data.loc[(meterID,year), :]\n",
    "        periods_for_profile = periods_for_profile[(periods_for_profile['end_time'] > start_time ) & (periods_for_profile['start_time'] < end_time)]\n",
    "    else: \n",
    "        profile_df = data16_df.loc[(meterID, year),:]\n",
    "        periods_for_profile =data.loc[(meterID,year), :]\n",
    "        \n",
    "#     print(periods_for_profile[['start_time', 'end_time']])\n",
    "#     print(zero_periods_for_profile[['start_time', 'end_time', 'is_disconnection_period']])\n",
    "    line = alt.Chart(profile_df.to_frame('value').reset_index()).mark_line().encode(\n",
    "        x = alt.X('timestamp:T'), \n",
    "        y = alt.Y('value:Q')\n",
    "    )\n",
    "    \n",
    "    if peak_type_column is None:\n",
    "        peak_color_encoding = alt.ColorValue('red')\n",
    "    else:\n",
    "        peak_color_encoding = alt.Color(f'{peak_type_column}:N')\n",
    "    \n",
    "    peak_points = alt.Chart(peak_data.reset_index()).mark_point(filled=True, size=100).encode(\n",
    "        x=alt.X('peak_time:T'),\n",
    "        y=alt.Y('peak_value:Q'),\n",
    "        color=peak_color_encoding\n",
    "    )\n",
    "    \n",
    "    if period_type_column is None: \n",
    "        color_encoding = alt.ColorValue('blue') \n",
    "    else: \n",
    "        color_encoding = alt.Color(f'{period_type_column}:N')\n",
    "    plot_df =periods_for_profile.reset_index(drop=True)\n",
    "    rect = alt.Chart(plot_df).mark_rect(opacity = 0.6).encode(\n",
    "        x = 'start_time:T',\n",
    "        x2 = 'end_time:T', \n",
    "        color = color_encoding\n",
    "    ) + alt.Chart(plot_df).mark_circle(opacity = 0.6).encode(\n",
    "        x = 'start_time:T',\n",
    "        y = alt.YValue(profile_df.max()),\n",
    "#         x2 = 'end_time:T', \n",
    "        color = color_encoding\n",
    "    )\n",
    "    \n",
    "    chart = rect + line + peak_points\n",
    "    \n",
    "    if 'connection_power' in periods_for_profile.columns: \n",
    "        connection_power = float(periods_for_profile.connection_power.iat[0])\n",
    "\n",
    "        connection_power_line = alt.Chart(periods_for_profile.reset_index()).mark_rule(color = 'black', opacity = 0.8).encode(\n",
    "            y =  'mean(connection_power):Q'\n",
    "        )\n",
    "        chart += connection_power_line\n",
    "    return chart.properties(width = 750, title = f\"{meterID} in {year}\").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_no = 4\n",
    "meter = peaks16_df_exploded.index.get_level_values(0).unique()[meter_no]\n",
    "#plot_profile_with_intervals_and_peaks(meter, 2016, period_type_column='interval_value')\n",
    "plot_profile_with_intervals_and_peaks(meter, 2016, period_type_column='is_connection_power_peak').display()\n",
    "plot_profile_with_intervals_and_peaks(meter, 2016, period_type_column='is_cumulative_based_on_factor').display()\n",
    "plot_profile_with_intervals_and_peaks(meter, 2016, period_type_column='is_cumulative_based_on_average_peak_value').display()\n",
    "plot_profile_with_intervals_and_peaks(meter, 2016, period_type_column='is_first_sample_global_peak').display()\n",
    "plot_profile_with_intervals_and_peaks(meter, 2016, period_type_column='is_cumulative_based_on_clustering').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_measurements_after_intervals(meterID, year, profile_intervals, cumulative_classification_column='is_cumulative_based_on_clustering'):\n",
    "    # Plot the measurements after the NaN/zero intervals, colored according to cumulative_classification_column.\n",
    "    # The number of samples is determined by the number of columns named as 'valuei_after_end' for i=1,2,...,M where M is provided above.\n",
    "    value_after_end_columns_ = np.where([bool(re.compile('value[0-9]+_after_end').match(i)) \n",
    "                                         for i in profile_intervals.reset_index().columns.values])[0]\n",
    "    other_columns_ = [profile_intervals.reset_index().columns[i] \n",
    "                      for i in range(len(profile_intervals.reset_index().columns)) if i not in value_after_end_columns_]\n",
    "    plot_profile_intervals = profile_intervals.reset_index().set_index(other_columns_) \\\n",
    "                                              .rename_axis('time_sample', axis=1).stack().to_frame('value').reset_index()\n",
    "    plot_profile_intervals['time_sample'] = plot_profile_intervals['time_sample'].map(lambda o: int(re.search(r'\\d+', o).group())) # convert string values to numbers\n",
    "    #plot_profile_intervals['start_end'] = list(zip(plot_profile_intervals['start'], plot_profile_intervals['end'])) # altair doesn't like tuples\n",
    "    plot_profile_intervals['start_end'] = plot_profile_intervals['start'].astype(str) + '-' + plot_profile_intervals['end'].astype(str)\n",
    "    return alt.Chart(plot_profile_intervals.query(f'meterID == \"{meterID}\" & year == {year}')) \\\n",
    "              .mark_line().encode(x='time_sample:Q', y='value:Q', color=f'{cumulative_classification_column}:N', detail='start_end') \\\n",
    "              .properties(width = 500, title = f\"{meterID} in {year}\").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_no = 4\n",
    "meter = peaks16_df_exploded.index.get_level_values(0).unique()[meter_no]\n",
    "plot_measurements_after_intervals(meter, 2016, profile_intervals, 'is_connection_power_peak').display()\n",
    "plot_measurements_after_intervals(meter, 2016, profile_intervals, 'is_cumulative_based_on_factor').display()\n",
    "plot_measurements_after_intervals(meter, 2016, profile_intervals, 'is_cumulative_based_on_average_peak_value').display()\n",
    "plot_measurements_after_intervals(meter, 2016, profile_intervals, 'is_first_sample_global_peak').display()\n",
    "plot_measurements_after_intervals(meter, 2016, profile_intervals, 'is_cumulative_based_on_clustering').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
