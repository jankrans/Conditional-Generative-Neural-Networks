{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New similarity based cumulative value approach \n",
    "This approach first searched the nearest neighbors using the context as a guideline.  \n",
    "Each nearest neighbor checks which assumption fits the best.  \n",
    "If most nearest neighbors vote for the same assumption we mark the value as this.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from dask.distributed import Client\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import datetime\n",
    "import random\n",
    "from scipy.signal import find_peaks, find_peaks_cwt\n",
    "from kde_diffusion import kde1d\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import warnings\n",
    "from scipy.stats import norm\n",
    "idx = pd.IndexSlice\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this reloads code from external modules automatically if it is changed (without having to restart the kernel)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from interval_information import get_interval_df\n",
    "from peak_detection import (\n",
    "    get_cumulative_value_detections,\n",
    "    get_connection_and_pv_power_peaks, \n",
    "    get_model_based_global_peaks,\n",
    "    get_similarity_based_peaks, \n",
    "    construct_search_intervals, \n",
    "    add_data_to_search_intervals, \n",
    "    sim_known_data, \n",
    "    match_knn_then_assumption, \n",
    "    get_knn_similarity_based_peaks\n",
    "    \n",
    ")\n",
    "from statistical_models import (\n",
    "    NormalDistribution, \n",
    "    AutoKDEDistribution, \n",
    "    KDEDistribution,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/preprocessed/combined')\n",
    "RESULT_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/error_detection')\n",
    "RESULT_PATH.mkdir(mode = 0o770, parents = True, exist_ok=True)\n",
    "result_path = RESULT_PATH / 'cumulative_value_detection.csv' \n",
    "zero_path = RESULT_PATH / 'zero_interval_is_error.csv'\n",
    "info_path = PRE_PATH/'info.csv'\n",
    "data_path = PRE_PATH/'data.csv'\n",
    "assert info_path.exists() and data_path.exists() and zero_path.exists(), 'These paths should exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "Small table for convenience comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(name1, series1, name2, series2): \n",
    "    return pd.crosstab(series1, series2, rownames = [name1], colnames =[name2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "info_df = pd.read_csv(info_path, dtype={'meterID':'str'}).set_index(['meterID', 'year'], drop=True)\n",
    "print(f'#profiles = {info_df.shape[0]}')\n",
    "data_df = pd.read_csv(data_path, dtype={'meterID':'str'}).set_index(['meterID', 'year'], drop=True)\n",
    "data_df.columns = pd.to_datetime(data_df.columns)\n",
    "data_df.columns.name = 'timestamp'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For development look at subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = 'EandisVREG'\n",
    "YEAR = 2016\n",
    "# get the right subset based on the info df\n",
    "info16_df = info_df.loc[idx[:, 2016],:]\n",
    "info16_df = info16_df[info16_df.data_source == 'EandisVREG']\n",
    "info16_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the corresponding data profiles \n",
    "data16_df = data_df.loc[info16_df.index, :]\n",
    "data16_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the zero error detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_detections = pd.read_csv(zero_path).set_index(['meterID', 'year', 'start', 'end'], drop = True)\n",
    "zero_detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the intervals with additional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df = get_interval_df(data16_df, info16_df, keep_zero = True, keep_nan = True)\n",
    "interval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only look at the NaN intervals and the zero intervals detected as error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_intervals = interval_df.interval_value.isna()\n",
    "zero_error_intervals = (zero_detections == True).reindex(interval_df.index).is_error\n",
    "\n",
    "interval_df = interval_df[nan_intervals | zero_error_intervals]\n",
    "\n",
    "data_subset = data16_df.loc[interval_df.index.get_level_values(0).unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do cumulative value detection on the known error intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Client(n_workers = 4) as client: # local client with 4 processes\n",
    "    cumulative_value_detection = get_cumulative_value_detections(data_subset, interval_df, client = client)\n",
    "cumulative_value_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_value_detection.to_frame('followed_by_cumulative_value').to_csv(result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result summary\n",
    "## Zero detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_detections.is_error.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_value_detection.value_counts(dropna = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
