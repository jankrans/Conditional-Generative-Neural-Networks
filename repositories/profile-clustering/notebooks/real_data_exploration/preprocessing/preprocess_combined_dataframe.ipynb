{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined database of profiles\n",
    "So the idea is to make one database of profiles that is easy to load and use. \n",
    "\n",
    "\n",
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_DIR = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/new_preprocessed/') #Jonas\n",
    "# PREPROCESSED_DIR = Path('/Users/lolabotman/PycharmProjects/FluviusFullData/profiles/preprocessed') #Lola\n",
    "\n",
    "infrax_path = PREPROCESSED_DIR / 'infrax'\n",
    "eandis_vreg_path = PREPROCESSED_DIR / 'eandis2017'\n",
    "eandis_amr_path = PREPROCESSED_DIR / 'eandis_AMR'\n",
    "result_path = PREPROCESSED_DIR / 'combined'\n",
    "result_path.mkdir(parents = True, exist_ok = True, mode = 0o770)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_float_field(series, nan_allowed = True):\n",
    "    if not nan_allowed: \n",
    "        assert ~series.isna().any(), 'there is at least one NaN value!'\n",
    "    series = series.dropna()\n",
    "    try: \n",
    "        return series.astype('float')\n",
    "    except: \n",
    "        print('converting to float failed!')\n",
    "        \n",
    "    # check for ',' instead of '.'\n",
    "    has_komma = series.str.contains(',', regex = False, na = False)\n",
    "    print(series[has_komma[has_komma.isna()].index])\n",
    "    if has_komma.any(): \n",
    "        print(f\"Found komma ',' instead of '.' replacing... ({has_komma.sum()} times)\")\n",
    "        series[has_komma] = series[has_komma].str.replace(',','.')\n",
    "   \n",
    "    has_placeholders = series == '/'\n",
    "    if has_placeholders.any(): \n",
    "        print(f\"Found placeholder '/' replacing with NaN... ({has_placeholders.sum()} times)\")\n",
    "        series= series.replace('/', np.NAN)\n",
    "    converted = series.astype('float')\n",
    "    print(\"SUCCES\")\n",
    "    return converted\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infrax_info_df = pd.read_csv(infrax_path / 'clean_info.csv', index_col = [0,1], dtype={'meterID':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infrax_data_df = pd.read_csv(infrax_path / 'clean_data.csv', index_col = [0,1], dtype={'meterID':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eandis_info_df = pd.read_csv(eandis_vreg_path / 'clean_info_no_night.csv', index_col = [0,1])\n",
    "eandis_data_df = pd.read_csv(eandis_vreg_path / 'clean_data_no_night.csv', index_col = [0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amr_info_df = pd.read_csv(eandis_amr_path / 'clean_info.csv', index_col = [0,1])\n",
    "amr_data_df = pd.read_csv(eandis_amr_path /'clean_data.csv', index_col = [0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make info df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine info dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amr_info_df = amr_info_df.reset_index().astype({'meterID':'str'}).set_index(['meterID', 'year'])\n",
    "amr_data_df = amr_data_df.reset_index().astype({'meterID':'str'}).set_index(['meterID', 'year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infrax_info_df.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eandis_info_df.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amr_info_df.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_info_df = pd.concat([infrax_info_df, eandis_info_df, amr_info_df])\n",
    "combined_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_info_df.town.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_info_df['#family_members'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_info_df.consumer_type.value_counts().to_frame('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_info_df.PV.value_counts(dropna=False).to_frame('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_info_df.SLP_cat.value_counts(dropna=False).to_frame('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_info_df.heatpump.value_counts(dropna=False).to_frame('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection power and PV power need some processing\n",
    "There are '/' and sometimes a komma is used instead of a point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_info_df['connection_power'] = check_float_field(combined_info_df.connection_power)\n",
    "combined_info_df['PV_power'] = check_float_field(combined_info_df.PV_power)\n",
    "combined_info_df['connection_power'] = combined_info_df['connection_power'].replace({0:np.NAN})\n",
    "combined_info_df[['connection_power','PV_power']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_info_df.to_csv(result_path/'info.csv')\n",
    "combined_info_df.to_pickle(result_path/'info.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make data df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine data dfs\n",
    "There is a problem with leap years, so use columns of a leap year and a non-leap year will simply have NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infrax_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eandis_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amr_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_dt_replace(series, year=None, month=None, day=None):\n",
    "    return pd.to_datetime(\n",
    "        {'year': series.year if year is None else year,\n",
    "         'month': series.month if month is None else month,\n",
    "         'day': series.day if day is None else day, \n",
    "        'hour': series.hour,\n",
    "        'minute': series.minute})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2016 = pd.to_datetime(eandis_data_df.columns)\n",
    "t2017 = pd.to_datetime(amr_data_df.columns)\n",
    "t2017_2016 = vec_dt_replace(t2017, year = 2016)\n",
    "t2017_2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex the amr data correctly\n",
    "amr_data_16_df = amr_data_df.copy()\n",
    "amr_data_16_df.columns = t2017_2016\n",
    "amr_data_16_df = amr_data_16_df.reindex(t2016, axis = 1)\n",
    "amr_data_16_df.loc[:, amr_data_16_df.columns.month == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infrax contains some non-round timestamps so round them to correct his issue! \n",
    "infrax_data_df.columns = pd.to_datetime(infrax_data_df.columns, exact = False).round('min')\n",
    "eandis_data_df.columns = pd.to_datetime(eandis_data_df.columns)\n",
    "\n",
    "# take mean of the duplicate timestamps\n",
    "infrax_data_df = infrax_data_df.groupby(axis = 1, level = 0).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data_df = pd.concat([infrax_data_df, eandis_data_df, amr_data_16_df])\n",
    "combined_data_df.to_csv(result_path/'data.csv')\n",
    "combined_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks if indexes in data and info are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_no_data = combined_info_df.index.difference(combined_data_df.index)\n",
    "data_no_info = combined_data_df.index.difference(combined_info_df.index)\n",
    "assert len(info_no_data) == 0, info_no_data\n",
    "assert len(data_no_info) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New indexing (more uniform and remove EANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_info_df = pd.read_csv(result_path/'info.csv', dtype={'meterID':'str'})\n",
    "# combined_info_df = combined_info_df.set_index(['meterID','year'])\n",
    "# combined_data_df = pd.read_csv(result_path/'data.csv', dtype={'meterID':'str'})\n",
    "# combined_data_df = combined_data_df.set_index(['meterID','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dubble check for issues in the indexes of the info and data file)\n",
    "info_no_data = combined_info_df.index.difference(combined_data_df.index)\n",
    "data_no_info = combined_data_df.index.difference(combined_info_df.index)\n",
    "print(len(info_no_data))\n",
    "print(len(data_no_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change index of the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_indexes = list( str(item) for item in combined_data_df.index.levels[0])\n",
    "original_indexes_info = list( str(item) for item in combined_info_df.index.levels[0])\n",
    "new_indexes = ['smartmeter_'+str(integer) for integer in list(range(0,len(original_indexes)))]\n",
    "print(\"Original indexes :\")\n",
    "print(original_indexes[0:5])\n",
    "print('New indexes :')\n",
    "print(new_indexes[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meterID_old_to_new = dict(zip(original_indexes,new_indexes))\n",
    "meterID_new_to_old = dict(zip(new_indexes,original_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_level(df, dct, level=0):\n",
    "    index = df.index\n",
    "    index.set_levels([[dct.get(item, item) for item in names] if i==level else names\n",
    "                      for i, names in enumerate(index.levels)], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE = True\n",
    "if not (result_path/'reindexed_data.csv').exists() or OVERWRITE: \n",
    "    new_index_combined_data_df = combined_data_df.copy()\n",
    "    map_level(new_index_combined_data_df, meterID_old_to_new, level=0)\n",
    "    combined_data_df.to_csv(result_path/'reindexed_data.csv')\n",
    "else:\n",
    "    print(\"It has already been previously saved in your folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index_combined_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index_combined_data_df.index.levels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFO FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE = True\n",
    "if not (result_path/'reindexed_info.csv').exists() or OVERWRITE: \n",
    "    new_index_combined_info_df = combined_info_df.copy()\n",
    "    map_level(new_index_combined_info_df, meterID_old_to_new, level=0)\n",
    "#     specials = new_index_combined_info_df.connection_power[new_index_combined_info_df.connection_power.astype('str').str.contains(',')].str.replace(',', '.').astype('float')\n",
    "#     new_index_combined_info_df.loc[specials.index, 'connection_power'] = specials\n",
    "#     specials = new_index_combined_info_df.connection_power[new_index_combined_info_df.PV_power.astype('str').str.contains(',')].str.replace(',', '.').astype('float')\n",
    "#     new_index_combined_info_df.loc[specials.index, 'PV_power'] = specials\n",
    "    new_index_combined_info_df.to_csv(result_path/'reindexed_info.csv')\n",
    "    new_index_combined_info_df.to_pickle(result_path/'reindexed_info.pkl')\n",
    "else:\n",
    "    print(\"It has already been previously saved in your folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index_combined_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index_combined_info_df.query('consumer_type==\"0\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put everything in daylight savings time\n",
    "So the data following datasets are in UTC: \n",
    "- infrax (all but appartement)\n",
    "- eandis AMR  \n",
    "\n",
    "And these are in UTC: \n",
    "- eandis2017\n",
    "- infrax appartement\n",
    "(the duplicate hour is resolved by taking the average of both hours)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dates for winter summer time changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mar_2010, oct_2010 = pd.to_datetime('2016-03-28 02:00:00') , pd.to_datetime('2016-10-31 02:00:00')\n",
    "mar_2011, oct_2011 = pd.to_datetime('2016-03-27 02:00:00') , pd.to_datetime('2016-10-30 02:00:00')\n",
    "mar_2012, oct_2012 = pd.to_datetime('2016-03-25 02:00:00') , pd.to_datetime('2016-10-28 02:00:00')\n",
    "mar_2013, oct_2013 = pd.to_datetime('2016-03-31 02:00:00') , pd.to_datetime('2016-10-27 02:00:00')\n",
    "mar_2014, oct_2014 = pd.to_datetime('2016-03-30 02:00:00') , pd.to_datetime('2016-10-26 02:00:00')\n",
    "mar_2015, oct_2015 = pd.to_datetime('2016-03-29 02:00:00') , pd.to_datetime('2016-10-25 02:00:00')\n",
    "mar_2016, oct_2016 = pd.to_datetime('2016-03-27 02:00:00') , pd.to_datetime('2016-10-30 02:00:00')\n",
    "mar_2017, oct_2017 = pd.to_datetime('2016-03-26 02:00:00') , pd.to_datetime('2016-10-29 02:00:00')\n",
    "DST_times = pd.DataFrame(\n",
    "    [\n",
    "        [mar_2010, oct_2010],\n",
    "        [mar_2011, oct_2011],\n",
    "        [mar_2012, oct_2012],\n",
    "        [mar_2013, oct_2013],\n",
    "        [mar_2014, oct_2014],\n",
    "        [mar_2015, oct_2015], \n",
    "        [mar_2016, oct_2016], \n",
    "        [mar_2017, oct_2017]\n",
    "    ], index = range(2010, 2018),\n",
    "    columns = ['DST_start', 'DST_end']).rename_axis(index = 'year')\n",
    "DST_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = new_index_combined_data_df\n",
    "info_df = new_index_combined_info_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a reference some data in DST \n",
    "This is what we want to recreate, missing data from 2:00 to 2:45 (inclusive) because of the shifted hour \n",
    "and in october the duplicate hour (2:00 to 2:45) resolved by taking the mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_data = data_df.loc[info_df.query('data_source == \"EandisVREG\"').index]\n",
    "DST_start = DST_times.loc[2016, 'DST_start']\n",
    "dst_data.loc[:, DST_start - pd.Timedelta(hours = 1): DST_start + pd.Timedelta(hours = 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take all the data that is in UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "utc_info = info_df.query('data_source == \"EandisAMR\" | (data_source == \"Infrax\" & consumer_type!= \"app1\" & consumer_type != \"app2\")')\n",
    "utc_data = data_df.loc[utc_info.index, :]\n",
    "utc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert it to DST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice \n",
    "\n",
    "# to store the transformed utc data\n",
    "new_utc_data = utc_data.copy() \n",
    "for year in utc_data.index.get_level_values('year').unique():     \n",
    "    # goal:\n",
    "    # - have missing values from march 2:00 to march 2:45 inclusive\n",
    "    # - have duplicate values from october 2:00 to october 2:45\n",
    "    \n",
    "    # start and end dates \n",
    "    dst_start, dst_end = DST_times.loc[year, 'DST_start'], DST_times.loc[year, 'DST_end'] - pd.Timedelta(minutes = 15)\n",
    "    new_start, new_end = dst_start + pd.Timedelta(hours = 1), dst_end + pd.Timedelta(hours = 1)\n",
    "    \n",
    "    # Move march 2:00 - october 1:45 to march 3:00 - october 2:45\n",
    "    new_utc_data.loc[idx[:,year], new_start:new_end] = utc_data.loc[idx[:,year], dst_start: dst_end]\n",
    "\n",
    "    # take average of duplicate hour \n",
    "    new_utc_data.loc[idx[:,year], dst_end:new_end] = (new_utc_data.loc[idx[:,year], dst_end:new_end]  + utc_data.loc[idx[:,year], dst_end:new_end] )/2\n",
    "\n",
    "    # make the missing hour missing \n",
    "    new_utc_data.loc[idx[:,year], dst_start:new_start-pd.Timedelta(minutes = 15)] = np.NaN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the converted data back into the full dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc[new_utc_data.index] = new_utc_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save this already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv(result_path/'reindexed_DST_data.csv')\n",
    "data_df.to_pickle(result_path/'reindexed_DST_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
