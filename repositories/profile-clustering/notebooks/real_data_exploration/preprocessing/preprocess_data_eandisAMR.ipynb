{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of data eandis AMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt #Altair is a declarative statistical visualization library for Python.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path #Offers classes representing filesystem paths with semantics appropriate for different OS.\n",
    "import datetime\n",
    "from tqdm import tqdm #Instantly make your loops show a smart progress meter. How? wrap any iterable with tqdm(it).\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path to the AMR folder (Jonas)\n",
    "DATA_PATH = Path(\"/cw/dtaiproj/ml/2020-FLAIR-VITO/Data-2020-11/FluviusData/profiles/data eandis 20180822 AMR\") \n",
    "# Path to the AMR folder (Lola)\n",
    "# DATA_PATH = Path(\"/Users/lolabotman/PycharmProjects/FluviusFullData/profiles/data eandis 20180822 AMR\") \n",
    "\n",
    "# folder with all the .txt's\n",
    "data_dir = DATA_PATH / 'Kwartuurwaarden AMR'\n",
    "master_file_path = DATA_PATH/ 'E7856 Master data.xlsx'\n",
    "\n",
    "# location to store preprocessed files (Jonas)\n",
    "PREPROCESSED_PATH = Path(\"/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/new_preprocessed/eandis_AMR\")\n",
    "# location to store preprocessed files (Lola)\n",
    "# PREPROCESSED_PATH = Path(\"/Users/lolabotman/PycharmProjects/FluviusFullData/profiles/preprocessed/eandis_AMR\")\n",
    "\n",
    "\n",
    "# intermediate results\n",
    "PREPROCESSED_PER_ID = PREPROCESSED_PATH / 'per_id'\n",
    "PREPROCESSED_PER_ID.mkdir(mode = 0o770, parents= True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the txt's to something workeable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Master table\n",
    "This one is also parsed to something a little bit more readeable later but is necessary to parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(master_file_path)\n",
    "sheets = xls.sheet_names\n",
    "print(f\"available sheets: {sheets}\")\n",
    "print(f\"reading {sheets[0]}\")\n",
    "print()\n",
    "master_df = xls.parse(sheet_name = 0).set_index('EAN')\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make new master table and parse txt's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming master df from previous section \n",
    "master_table_df = master_df \n",
    "# creating empty df using the right index and columns\n",
    "master_df_entry = pd.DataFrame(index = master_table_df.index, columns = ['ID', 'yearly_offtake', 'yearly_injection', 'yearly_net_offtake', 'PV', 'PV_power', 'connection_power', 'residential', 'ex_night', 'HP', 'startDate', 'endDate', 'data_source', 'ean'])\n",
    "# listing all the .txt files in the chosen folder \n",
    "data_files = list(data_dir.glob('*.txt'))\n",
    "# convert list to set (interesting mathematical properties different than lists)\n",
    "unread_data_files = set(data_files)\n",
    "\n",
    "# loop though all EANs from the master table\n",
    "for ean_to_check in tqdm(master_df.index):\n",
    "    \n",
    "    # get all files that have ean in the filename (so all the files that contain info about this EAN)\n",
    "    ean_data_files = [f  for f in data_files if str(ean_to_check) in f.stem] # .stem = gets the final path component, without its suffix (in this case .txt)\n",
    "    unread_data_files.difference_update(ean_data_files) #unread_data_files becomes the difference (unread_data_files - ean_data_files)\n",
    "    \n",
    "\n",
    "    new_reading_df = pd.DataFrame(columns=['ID', 'offtake', 'injection']) #create empty df\n",
    "    offtake = []\n",
    "    injection = []\n",
    "    date_index_off = []\n",
    "    date_index_inj = []\n",
    "    \n",
    "    #loop through each of the selected .txt files in main loop (many files about this one EAN)\n",
    "    # extract data in each .txt as dataframe & parse the data to make it useable \n",
    "    for ean_file in ean_data_files:\n",
    "        data = pd.read_csv(ean_file, sep=';', skiprows=[0,1,2,3,4,5,6,7,8], decimal=',', header=None, skipfooter=2, engine='python')\n",
    "\n",
    "        # can do this jointly using iloc \n",
    "        for c in range(9,9+96):\n",
    "            data[c] = pd.to_numeric(data[c])\n",
    "\n",
    "        # parse the dates\n",
    "        data[0] = pd.to_datetime(data[0], format='%d%m%Y %H:%M')\n",
    "        data[1] = pd.to_datetime(data[1], format='%d%m%Y %H:%M')\n",
    "\n",
    "        # offtake rows\n",
    "        offtake_rows = data[(data[6]=='E12-E17') & (data[7]=='KWT')]\n",
    "        for ind in offtake_rows.index:\n",
    "            date_index_off.extend(pd.date_range(offtake_rows.loc[ind,0], offtake_rows.loc[ind,1], freq = '15min', closed='left').values)\n",
    "            nr_data_points = len(pd.date_range(offtake_rows.loc[ind,0], offtake_rows.loc[ind,1], freq = '15min', closed='left').values)\n",
    "            offtake.extend(offtake_rows.loc[ind,9:9+nr_data_points-1].values)\n",
    "       \n",
    "        # injection rows \n",
    "        injection_rows = data[(data[6]=='E12-E18') & (data[7]=='KWT')]\n",
    "        for ind in injection_rows.index:\n",
    "            date_index_inj.extend(pd.date_range(injection_rows.loc[ind,0], injection_rows.loc[ind,1], freq = '15min', closed='left').values)\n",
    "            nr_data_points = len(pd.date_range(injection_rows.loc[ind,0], injection_rows.loc[ind,1], freq = '15min', closed='left').values)\n",
    "            injection.extend(injection_rows.loc[ind,9:9+nr_data_points-1].values)\n",
    "\n",
    "    # does this mean when there is no data for offtake/inj you put zero values ?\n",
    "    if len(offtake) ==0:\n",
    "        offtake = np.zeros(len(date_index_inj))\n",
    "    if len(injection) ==0:\n",
    "        injection = np.zeros(len(date_index_off))\n",
    "\n",
    "    new_reading_df['offtake'] = np.array(offtake)/4 #values in df should be in kWh, source data is in kW\n",
    "    new_reading_df['injection'] = injection\n",
    "    new_reading_df['ID'] = ean_to_check\n",
    "    \n",
    "    if len(date_index_off)>0:\n",
    "        new_reading_df.index = date_index_off\n",
    "    else:\n",
    "        new_reading_df.index = date_index_inj\n",
    "\n",
    "    # add info to the new master table\n",
    "    total_yearly_consumption = new_reading_df['offtake'].sum()/len(new_reading_df)*(365*96) #new_reading_df[year_mask]['offtake'].sum()   #scaled to year ==> might not be correct because some months have different consumption than other months\n",
    "    total_yearly_injection = new_reading_df['injection'].sum()/len(new_reading_df)*(365*96) #no data available in source data\n",
    "    yearly_net_consumption = total_yearly_consumption\n",
    "\n",
    "    master_df_entry.loc[ean_to_check,'ID'] = str(ean_to_check)\n",
    "    master_df_entry.loc[ean_to_check,'yearly_offtake'] = total_yearly_consumption\n",
    "    master_df_entry.loc[ean_to_check,'yearly_injection'] = total_yearly_injection\n",
    "    master_df_entry.loc[ean_to_check,'yearly_net_offtake'] = yearly_net_consumption\n",
    "    master_df_entry.loc[ean_to_check,'PV'] = master_table_df.loc[ean_to_check,'DCP']\n",
    "    master_df_entry.loc[ean_to_check,'PV_power'] = master_table_df.loc[ean_to_check,'INSTALLED_POWER_DCP']\n",
    "    master_df_entry.loc[ean_to_check,'connection_power'] = master_table_df.loc[ean_to_check,'PHY_CON_CAP']\n",
    "    master_df_entry.loc[ean_to_check,'residential'] = 0\n",
    "    master_df_entry.loc[ean_to_check,'ex_night'] = 0\n",
    "    master_df_entry.loc[ean_to_check,'HP'] = 0\n",
    "    master_df_entry.loc[ean_to_check,'startDate'] = new_reading_df.index.min()\n",
    "    master_df_entry.loc[ean_to_check,'endDate'] = new_reading_df.index.max()\n",
    "    master_df_entry.loc[ean_to_check,'data_source'] = 'EandisAMR'\n",
    "    master_df_entry.loc[ean_to_check,'ean'] = str(ean_to_check)\n",
    "\n",
    "    if not (PREPROCESSED_PER_ID / (str(ean_to_check)+\".csv\")).exists():\n",
    "        new_reading_df.to_csv(PREPROCESSED_PER_ID / (str(ean_to_check)+\".csv\"))\n",
    "\n",
    "print(f'there are {len(unread_data_files)} unread files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### yearly total \n",
    "NB : if we just make the total (in kWh), we sum up more than a year of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just give easier names \n",
    "new_master_df = master_df_entry\n",
    "master_df = master_table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make one big dataframe of all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in PREPROCESSED_PER_ID.iterdir():\n",
    "    data_df = pd.read_csv(file)\n",
    "    dfs.append(data_df)\n",
    "raw_data_df = pd.concat(dfs, axis = 0).rename(columns = {'Unnamed: 0': 'timestamp', 'ID':'meterID'}).reset_index(drop = True)\n",
    "raw_data_df['timestamp'] = pd.to_datetime(raw_data_df.timestamp)\n",
    "raw_data_df.set_index(['meterID', 'timestamp'], inplace = True)\n",
    "raw_data_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df.reset_index().dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_extra_hour(ind):\n",
    "    if ind < pd.to_datetime('2017-10-30 03:00:00') and ind > pd.to_datetime('2017-10-30 01:45:00'):\n",
    "        color = 'yellow'\n",
    "    else:\n",
    "        color = ''\n",
    "    return 'background-color: %s' % color\n",
    "\n",
    "def color_missing_hour(ind):\n",
    "    if ind < pd.to_datetime('2017-03-26 03:00:00') and ind > pd.to_datetime('2017-03-26 01:45:00'):\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = ''\n",
    "    return 'background-color: %s' % color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicate/missing measurements due to summer-winter time\n",
    "- In <b> March </b> clocks go forward (at 2 am, we say it is 3 am), causing <b> 4 missing values </b> corresponding to that one hour (which may have been interpolated somehow)\n",
    "- In <b> October </b> clocks go back (at 3am, we say it is 2 am) so there should be <b> 4 duplicate measurements </b> (which may have already been averaged or the duplicates may have been omitted).\n",
    "\n",
    "> NB. Only a few test are done here, the more exaustive work is at the end of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOKING FOR DUPLICATES (OCTOBER)\n",
    "Conclusion : There are no duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurence_count = raw_data_df.reset_index().groupby('meterID')['timestamp'].value_counts()\n",
    "duplicate_values = occurence_count[occurence_count > 1]\n",
    "duplicate_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for the first smart meter \n",
    "sm_id = raw_data_df.index.get_level_values(0)[0]\n",
    "print(f'HEAD : {raw_data_df.loc[[sm_id],:].sort_index().head()}\\n\\n')\n",
    "print(f'TAIL : {raw_data_df.loc[[sm_id],:].sort_index().tail()}')\n",
    "\n",
    "##this profiles happens to start in september 2017 and go until january 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can observe that there doesn't seem to be duplicate data in octobre \n",
    "\n",
    "all_timestamps = raw_data_df.loc[raw_data_df.index.get_level_values(0)[0]]['2017-10-30 1:00':'2017-10-30 4:00']\n",
    "all_timestamps.reset_index().style.applymap(color_extra_hour, subset=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick another smart meter to look at \n",
    "\n",
    "sm_id_2 = 541448810000108927\n",
    "#print(f'HEAD : {raw_data_df.loc[[sm_id_2],:].sort_index().head()}\\n\\n')\n",
    "#print(f'TAIL : {raw_data_df.loc[[sm_id_2],:].sort_index().tail()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timestamps = raw_data_df.loc[sm_id_2]['2017-10-30 1:00':'2017-10-30 4:00']\n",
    "all_timestamps.reset_index().style.applymap(color_extra_hour, subset=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOKING FOR MISSING TIME STAMPS (MARCH)\n",
    "So there does not seem to be any correction for winter/summer time in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First smart meter : nothing can be observed in march (no data at all)\n",
    "all_timestamps = raw_data_df.loc[sm_id]['2017-03-26 1:00':'2017-03-26 4:00']\n",
    "all_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second smart meter : we have data in march, we can observe that there are no missing data\n",
    "all_timestamps = raw_data_df.loc[sm_id_2]['2017-03-26 01:00':'2017-03-26 04:00']\n",
    "all_timestamps.reset_index().style.applymap(color_missing_hour, subset=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check if the potentially missing hour due to DST has been interpolated \n",
    "We want to find out if some techniques have been applied to the data in order to stay coherent with the other datasets. It could be that these profiles are in UTC. This is what we try to check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop injections\n",
    "temp_off_val = all_timestamps.drop(columns=['injection'])\n",
    "\n",
    "#make a column where we copy the offtake and then replace the potential originally missing hour with nan\n",
    "temp_off_val['offtake_with_nan'] = temp_off_val.offtake\n",
    "temp_off_val.at[pd.to_datetime('2017-03-26 02:00:00'):pd.to_datetime('2017-03-26 02:45:00'),'offtake_with_nan']=np.nan\n",
    "temp_off_val.reset_index().style.applymap(color_missing_hour, subset=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(temp_off_val.offtake)\n",
    "plt.plot(temp_off_val.offtake_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply interpolation \n",
    "interp_method = 'spline'\n",
    "temp_off_val_interp = temp_off_val.offtake_with_nan.interpolate(method=interp_method,order=3)\n",
    "temp_off_val[\"filled_nan\"] = temp_off_val_interp\n",
    "temp_off_val.reset_index().style.applymap(color_missing_hour, subset=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(temp_off_val.offtake)\n",
    "plt.plot(temp_off_val.filled_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if values are the same by computing the difference \n",
    "diff_original_interp = (temp_off_val['offtake']-temp_off_val['filled_nan'])\n",
    "diff_original_interp = pd.DataFrame(diff_original_interp, columns = ['diff'])\n",
    "equal_interp = (diff_original_interp['diff'] == 0).all() #This will return True if all values are 0 otherwise it will return false\n",
    "\n",
    "if equal_interp == True:\n",
    "    print(f'The interpolated potential missing hour gives the same result as the given data : the hypothesis is true, the data has been interpolated using the method \"{interp_method}\"')\n",
    "else:\n",
    "    print(f'The interpolated potential missing hour gives a different result than the given data : the hypothesis is denied OR the interpolation method is not \"{interp_method}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the hypothesis can be confirmed for all of the smartmeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_interp_oct(sm_id, interp_method):\n",
    "    \n",
    "    #potential 'missing hour' timestamps due to DST\n",
    "    start_missing_hour = pd.to_datetime('2017-03-26 02:00:00')\n",
    "    end_missing_hour = pd.to_datetime('2017-03-26 02:45:00')\n",
    "    \n",
    "    #time samples before and after the 'missing hour'\n",
    "    start_sample = start_missing_hour - 4*pd.DateOffset(minutes=15)\n",
    "    end_sample = end_missing_hour + 5*pd.DateOffset(minutes=15)\n",
    "    \n",
    "    #select the sample of time in the dataframe \n",
    "    sample = pd.DataFrame(raw_data_df.loc[sm_id][start_sample:end_sample].offtake)\n",
    "    \n",
    "    #duplicate offtake column and replace 'missing hour' with nans\n",
    "    sample['offtake_with_nan'] = sample.offtake\n",
    "    sample.at[start_missing_hour:end_missing_hour,'offtake_with_nan']=np.nan\n",
    "    \n",
    "    #interpolate\n",
    "    sample['interp'] = sample.offtake_with_nan.interpolate(method=interp_method).values\n",
    "    \n",
    "    #check if values are the same by computing the difference \n",
    "    sample['diff'] = sample['offtake']-sample['interp']\n",
    "    equal_interp = (sample['diff'] == 0).all() \n",
    "        #If equal_interp = True : all differences are zero --> hypothesis confirmed. This 'missing hour' has been interpolated\n",
    "        #If equal_interp = False --> hypothesis denied. This 'missing hour' has not been interpolated with the chosen method\n",
    "    \n",
    "    return sample, equal_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_interp = []\n",
    "method_interp_x = 'cubic'\n",
    "\n",
    "for sm_id in raw_data_df.index.levels[0]:\n",
    "    idx = raw_data_df.loc[sm_id].index\n",
    "    if pd.to_datetime('2017-03-26 02:00:00') in idx:\n",
    "        sample_x, equal_interp_x = check_interp_oct(sm_id,method_interp_x)\n",
    "        equal_interp.append(equal_interp_x)\n",
    "    \n",
    "check_all = all(equal_interp)\n",
    "\n",
    "if check_all == True:\n",
    "    print(f'✓ The interpolated potential missing hour gives the same result as the given data for all the smart meters :\\n --> The hypothesis is true, the data has been interpolated using the method \"{interp_method}\"')\n",
    "else:\n",
    "    print(f'✘ The interpolated potential missing hour gives a different result than the given data for one of the smartmeters :\\n --> The hypothesis is denied OR the interpolation method is not \"{interp_method}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make into a nice pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note: there are no Nan values here so the calculation of consumption is correct!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df[raw_data_df.offtake.isna() | raw_data_df.injection.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = raw_data_df.copy().reset_index()\n",
    "data_df['consumption'] = data_df.offtake - data_df.injection\n",
    "data_df = data_df.drop(columns = ['offtake', 'injection'])\n",
    "data_df = pd.pivot_table(data_df, index = 'meterID', columns = 'timestamp', values = 'consumption')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc[[541448860013438368],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take subset of 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2017_df = data_df.loc[:,pd.to_datetime(data_df.columns).year == 2017].copy()\n",
    "data_2017_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove profiles with only injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eans_to_drop = list(data_2017_df[data_2017_df.isna().any(axis = 1)].index)\n",
    "eans_with_injection = list(raw_data_df.index.get_level_values(0)[raw_data_df.injection>0].unique())\n",
    "eans_to_drop.extend(eans_with_injection)\n",
    "print(f\"dropping eans: {eans_to_drop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_2017_df = data_2017_df.loc[~data_2017_df.index.isin(eans_to_drop)].copy()\n",
    "\n",
    "# add 2017 to the index\n",
    "data_2017_df['year'] = 2017\n",
    "data_2017_df = data_2017_df.set_index('year', append=True)\n",
    "data_2017_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up the master table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_master_2_df = new_master_df.drop(columns = ['ID', 'ean','startDate', 'endDate', 'HP', 'yearly_offtake', 'yearly_injection', 'yearly_net_offtake'])\n",
    "new_master_2_df = new_master_2_df[~new_master_2_df.index.get_level_values(0).isin(eans_to_drop)]\n",
    "new_master_2_df.rename_axis(index = {'EAN':'meterID'}, inplace = True)\n",
    "new_master_2_df = new_master_2_df.astype({'PV':'boolean', 'residential': 'boolean', 'ex_night':'boolean'})\n",
    "new_master_2_df['year'] = 2017\n",
    "new_master_2_df.set_index('year', append=True, inplace = True)\n",
    "new_master_2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At this time residential is false for all profiles and ex_night as well so drop these as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_master_2_df.drop(columns = ['residential', 'ex_night'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_master_2_df['consumer_type'] = 'professional'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_master_2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save it all to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_master_2_df.to_csv(PREPROCESSED_PATH/ 'clean_info.csv') #only relevant meta data for AMR profiles\n",
    "new_master_df.to_csv(PREPROCESSED_PATH/'full_info.csv') #all relevant meta data for all profiles (when everything is merged)\n",
    "data_df.to_csv(PREPROCESSED_PATH/'data.csv') #all consumption values (offtake-inj) for the entire time inluding january 2018\n",
    "data_2017_df.to_csv(PREPROCESSED_PATH/'clean_data.csv') #cleaned consumption values (offtake-inj) for 2017\n",
    "raw_data_df.to_csv(PREPROCESSED_PATH/'raw_data.csv') #all 'raw data' from .txt file (with additinal month of january 2018) including injection and offtae values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for injection values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for positive injection values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_injection_values = (raw_data_df.injection> 0).sum()\n",
    "print(f'there are {positive_injection_values} positive injection values!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So indeed there are some positive injection values  \n",
    "Let's save these for reference such that we can check with the eans in the master table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for eans with positive injection values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eans_with_injection = list(raw_data_df.index.get_level_values(0)[raw_data_df.injection>0].unique())\n",
    "print('eans_with_injection')\n",
    "print(eans_with_injection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_master_df.loc[eans_with_injection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these profiles only have injection and no offtake but there are no PV panels according to the master table! (in the next plot this is the DCP column) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.loc[eans_with_injection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eans with DCP \n",
    "There is only one ean with PV panels according to the master table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.loc[master_df.DCP == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_master_df.loc[new_master_df.PV == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eans_with_injection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if I understand well, these 8 EANS have injection values but according to the master table they have no PV pannels installed ? Could they have another source of energy generations ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK FOR DAYLIGHT SAVING TIME ARTEFACT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data to be checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2017_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work on a copy \n",
    "pivot_table_or = data_2017_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pivot_table_or.index.levels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 'year' indexes are 2017 > remove them to not have to handle them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table = pivot_table_or.droplevel(level=1)\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table.columns = pd.to_datetime(pivot_table.columns)\n",
    "pivot_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_ids = list(pivot_table.index)\n",
    "sm_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define potentially originally missing hour\n",
    "start_missing_hour = pd.to_datetime('2017-03-26 02:00:00')\n",
    "end_missing_hour = pd.to_datetime('2017-03-26 02:45:00')\n",
    "\n",
    "index_start_missingh,index_end_missingh =  pivot_table.columns.get_indexer([start_missing_hour, end_missing_hour])\n",
    "\n",
    "print(f'Start missing hour : {start_missing_hour}, ind : {index_start_missingh}\\nEnd missing hour   : {end_missing_hour}, ind :{index_end_missingh}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define time samples before and after the 'missing hour' (arbitrary chosen > one hour before and one hour after) \n",
    "#subset of the full df \n",
    "start_sample = start_missing_hour - 4*pd.DateOffset(minutes=15)\n",
    "end_sample = end_missing_hour + 4*pd.DateOffset(minutes=15)\n",
    "\n",
    "index_start_sample,index_end_sample =  pivot_table.columns.get_indexer([start_sample, end_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sample = pivot_table.iloc[:,index_start_sample:index_end_sample+1].copy()\n",
    "original_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing hour with nan (to be interpolated) in the pivot table - working on a copy \n",
    "pivot_copy = pivot_table.copy()\n",
    "pivot_copy.at[:,start_missing_hour:end_missing_hour]=np.nan\n",
    "nan_pivot_table = pivot_copy.copy()\n",
    "nan_pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nan sample (to compare with original sample )\n",
    "nan_sample = nan_pivot_table.iloc[:,index_start_sample:index_end_sample+1].copy()\n",
    "nan_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPOTHESIS : missing hour = (h-1 + h+1)/2\n",
    "\n",
    "interp_sample = pd.DataFrame()\n",
    "for sm_id in nan_sample.index:\n",
    "    sm_serie = nan_sample.loc[sm_id].copy()\n",
    "    for i in range(0,4):\n",
    "        sm_serie.iat[i+4] = (sm_serie[i]+sm_serie[i+8])/2\n",
    "        sm_df = pd.DataFrame(sm_serie).T\n",
    "    interp_sample = interp_sample.append(sm_df)\n",
    "\n",
    "bool_output = interp_sample == original_sample\n",
    "unique, counts = np.unique(bool_output, return_counts=True)\n",
    "original_nb_of_false = 4*89\n",
    "print(f'remaining errors : {counts[0]} out of {original_nb_of_false}\\n')\n",
    "bool_output\n",
    "\n",
    "## it is clear that the results are not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPOTHESIS : interpolation\n",
    "\n",
    "chosen_meth = 'nearest'\n",
    "#chose method from ‘linear’, ‘time’, ‘index’, ‘values’, 'pad’, ‘nearest’, \n",
    "# ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘spline’, ‘barycentric’, ‘polynomial’\n",
    "# ‘krogh’, ‘piecewise_polynomial’, ‘pchip’, ‘akima’, ‘cubicspline’\n",
    "\n",
    "interp_sample = pd.DataFrame()\n",
    "for sm_id in nan_sample.index:\n",
    "    sm_serie = nan_sample.loc[sm_id].copy()\n",
    "    sm_interp = sm_serie.interpolate(method = chosen_meth, axis=0) ##add order depending on the chosen method\n",
    "    sm_df = pd.DataFrame(sm_interp).T\n",
    "    interp_sample = interp_sample.append(sm_df)\n",
    "    \n",
    "bool_output = interp_sample == original_sample\n",
    "unique, counts = np.unique(bool_output, return_counts=True)\n",
    "original_nb_of_false = 4*89\n",
    "print(f'Interp method \"{chosen_meth}\" yields : {counts[0]} wrongly guessed values out of {original_nb_of_false}\\n')\n",
    "bool_output\n",
    "\n",
    "#nearest 286/356\n",
    "#linear 323/356\n",
    "#pad 295/356\n",
    "#slinear 326/356\n",
    "#cubic 340/356\n",
    "#quadratic 340/356\n",
    "#spline 340/356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPOTHESIS : missing hour = (h-1)\n",
    "\n",
    "interp_sample = pd.DataFrame()\n",
    "for sm_id in nan_sample.index:\n",
    "    sm_serie = nan_sample.loc[sm_id].copy()\n",
    "    for i in range(0,4):\n",
    "        sm_serie.iat[i+4] = sm_serie[i]\n",
    "        sm_df = pd.DataFrame(sm_serie).T\n",
    "    interp_sample = interp_sample.append(sm_df)\n",
    "\n",
    "bool_output = interp_sample == original_sample\n",
    "unique, counts = np.unique(bool_output, return_counts=True)\n",
    "original_nb_of_false = 4*89\n",
    "print(f'remaining errors : {counts[0]}  wrongly guessed values out of {original_nb_of_false}\\n')\n",
    "bool_output\n",
    "\n",
    "## it is clear that the results are not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPOTHESIS : missing hour = (h+1)\n",
    "\n",
    "interp_sample = pd.DataFrame()\n",
    "for sm_id in nan_sample.index:\n",
    "    sm_serie = nan_sample.loc[sm_id].copy()\n",
    "    for i in range(0,4):\n",
    "        sm_serie.iat[i+4] = sm_serie[i+8]\n",
    "        sm_df = pd.DataFrame(sm_serie).T\n",
    "    interp_sample = interp_sample.append(sm_df)\n",
    "\n",
    "bool_output = interp_sample == original_sample\n",
    "unique, counts = np.unique(bool_output, return_counts=True)\n",
    "original_nb_of_false = 4*89\n",
    "print(f'remaining errors : {counts[0]}  wrongly guessed values out of  {original_nb_of_false}\\n')\n",
    "bool_output\n",
    "\n",
    "## it is clear that the results are not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPOTHESIS : missing hour = same hour previous day\n",
    "\n",
    "interp_table = pd.DataFrame()\n",
    "for sm_id in nan_pivot_table.index:\n",
    "    sm_serie = nan_pivot_table.loc[sm_id].copy()\n",
    "    for i in range(index_start_missingh,index_end_missingh+1):\n",
    "        sm_serie.iat[i] = sm_serie[i-(4*24)]\n",
    "        sm_df = pd.DataFrame(sm_serie).T\n",
    "    interp_table = interp_table.append(sm_df)\n",
    "\n",
    "bool_output = interp_table == pivot_table\n",
    "unique, counts = np.unique(bool_output, return_counts=True)\n",
    "original_nb_of_false = 4*89\n",
    "print(f'remaining errors : {counts[0]}  wrongly guessed values out of  {original_nb_of_false}\\n')\n",
    "bool_output.iloc[:,index_start_sample:index_end_sample]\n",
    "\n",
    "## it is clear that the results are not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same hour previous day \n",
    "interp_table.iloc[:,index_start_missingh-(4*24):index_end_missingh-(4*24)+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpolated hour \n",
    "interp_table.iloc[:,index_start_missingh:index_end_missingh+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original hour \n",
    "pivot_table.iloc[:,index_start_missingh:index_end_missingh+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPOTHESIS : missing hour = same hour next day\n",
    "\n",
    "interp_table = pd.DataFrame()\n",
    "for sm_id in nan_pivot_table.index:\n",
    "    sm_serie = nan_pivot_table.loc[sm_id].copy()\n",
    "    for i in range(index_start_missingh,index_end_missingh+1):\n",
    "        sm_serie.iat[i] = sm_serie[i+(4*24)]\n",
    "        sm_df = pd.DataFrame(sm_serie).T\n",
    "    interp_table = interp_table.append(sm_df)\n",
    "\n",
    "bool_output = interp_table == pivot_table\n",
    "unique, counts = np.unique(bool_output, return_counts=True)\n",
    "original_nb_of_false = 4*89\n",
    "print(f'remaining errors : {counts[0]}  wrongly guessed values out of  {original_nb_of_false}\\n')\n",
    "bool_output.iloc[:,index_start_sample:index_end_sample]\n",
    "\n",
    "## it is clear that the results are not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPOTHESIS : missing hour = same hour, same day previous week \n",
    "\n",
    "interp_table = pd.DataFrame()\n",
    "for sm_id in nan_pivot_table.index:\n",
    "    sm_serie = nan_pivot_table.loc[sm_id].copy()\n",
    "    for i in range(index_start_missingh,index_end_missingh+1):\n",
    "        sm_serie.iat[i] = sm_serie[i-(4*24*7)]\n",
    "        sm_df = pd.DataFrame(sm_serie).T\n",
    "    interp_table = interp_table.append(sm_df)\n",
    "\n",
    "bool_output = interp_table == pivot_table\n",
    "unique, counts = np.unique(bool_output, return_counts=True)\n",
    "original_nb_of_false = 4*89\n",
    "print(f'remaining errors : {counts[0]}  wrongly guessed values out of  {original_nb_of_false}\\n')\n",
    "bool_output.iloc[:,index_start_sample:index_end_sample]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPOTHESIS : missing hour = same hour, same day next week \n",
    "\n",
    "interp_table = pd.DataFrame()\n",
    "for sm_id in nan_pivot_table.index:\n",
    "    sm_serie = nan_pivot_table.loc[sm_id].copy()\n",
    "    for i in range(index_start_missingh,index_end_missingh+1):\n",
    "        sm_serie.iat[i] = sm_serie[i+(4*24*7)]\n",
    "        sm_df = pd.DataFrame(sm_serie).T\n",
    "    interp_table = interp_table.append(sm_df)\n",
    "\n",
    "bool_output = interp_table == pivot_table\n",
    "unique, counts = np.unique(bool_output, return_counts=True)\n",
    "original_nb_of_false = 4*89\n",
    "print(f'remaining errors : {counts[0]}  wrongly guessed values out of  {original_nb_of_false}\\n')\n",
    "bool_output.iloc[:,index_start_sample:index_end_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPOTHESIS : missing hour = average of the same hour, same day previous and next week \n",
    "\n",
    "interp_table = pd.DataFrame()\n",
    "for sm_id in nan_pivot_table.index:\n",
    "    sm_serie = nan_pivot_table.loc[sm_id].copy()\n",
    "    for i in range(index_start_missingh,index_end_missingh+1):\n",
    "        sm_serie.iat[i] = (sm_serie[i-(4*24*7)] + sm_serie[i+(4*24*7)])/2\n",
    "        sm_df = pd.DataFrame(sm_serie).T\n",
    "    interp_table = interp_table.append(sm_df)\n",
    "\n",
    "bool_output = interp_table == pivot_table\n",
    "unique, counts = np.unique(bool_output, return_counts=True)\n",
    "original_nb_of_false = 4*89\n",
    "print(f'remaining errors : {counts[0]}  wrongly guessed values out of  {original_nb_of_false}\\n')\n",
    "bool_output.iloc[:,index_start_sample:index_end_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### HYPOTHESIS : The same sequence of four value is taken from somwhere else in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(a, size):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - size + 1, size)\n",
    "    strides = a.strides + (a. strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_1_series = pivot_table.loc[541448810000108927]\n",
    "sm_1_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_hour_sm_id_1 = pivot_table.iloc[0,index_start_missingh:index_end_missingh+1]\n",
    "missing_hour_sm_id_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_roll_w = rolling_window(sm_1_series.values, 4) == missing_hour_sm_id_1.values\n",
    "df_output_roll_w = pd.DataFrame(output_roll_w)\n",
    "# df_output_roll_w['timestamps']=sm_1_series.index\n",
    "# df_output_roll_w = df_output_roll_w.set_index('timestamps')\n",
    "df_output_roll_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which row has 4 Trues\n",
    "df_output_roll_w.all(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count how many rows has 4 Trues \n",
    "df_output_roll_w.all(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is somewhere else in the table a sequence of this four value (it appears twice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find index of the 4 Trues\n",
    "df_trues = pd.DataFrame(df_output_roll_w.all(axis=1))\n",
    "df_trues['timetamps'] = sm_1_series.index[0:len(df_trues)]\n",
    "df_trues = df_trues.set_index('timetamps')\n",
    "for index in df_trues.index:\n",
    "    if df_trues.loc[index].values[0] == True:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and check it out for all the smart meters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TAKES A WHILE TO RUN >> check attached .txt file for output \n",
    "\n",
    "# #save all sm ids in a list \n",
    "# smartmeters_ids = list(pivot_table.index)\n",
    "\n",
    "# for sm_id in smartmeters_ids:\n",
    "    \n",
    "#     sm_series = pivot_table.loc[sm_id]\n",
    "\n",
    "#     missing_hour_sm_id = pivot_table.loc[sm_id,start_missing_hour:end_missing_hour]\n",
    "\n",
    "#     output_roll_w = rolling_window(sm_series.values, 4) == missing_hour_sm_id.values\n",
    "#     df_output_roll_w = pd.DataFrame(output_roll_w)\n",
    "\n",
    "#     #which row has 4 Trues ?\n",
    "#     all_true = df_output_roll_w.all(axis=1)\n",
    "\n",
    "#     #count how many rows has 4 Trues \n",
    "#     all_true_count = df_output_roll_w.all(axis=1).value_counts()\n",
    "\n",
    "\n",
    "#     #find index of the 4 Trues\n",
    "#     df_trues = pd.DataFrame(df_output_roll_w.all(axis=1))\n",
    "#     df_trues['timetamps'] = sm_1_series.index[0:len(df_trues)]\n",
    "#     df_trues = df_trues.set_index('timetamps')\n",
    "#     print(sm_id)\n",
    "#     for index in df_trues.index:\n",
    "#         if df_trues.loc[index].values[0] == True:\n",
    "#             print(index)\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
