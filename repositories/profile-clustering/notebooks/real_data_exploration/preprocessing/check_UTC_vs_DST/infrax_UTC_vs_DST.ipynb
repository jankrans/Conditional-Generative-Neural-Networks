{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First look at the data\n",
    "DONE: \n",
    "- Preprocessed all files to a csv with info and data \n",
    "- make a combined file\n",
    "- added year to the energy profiles \n",
    "\n",
    "Remaining problems: \n",
    "- Somewhere there are two rows with NaN's remove these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import tqdm\n",
    "import pyxlsb\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH to the profile directory in the fluvius data\n",
    "# DATA_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/Data-2020-11/FluviusData/profiles')\n",
    "DATA_PATH = Path('/Users/lolabotman/PycharmProjects/FluviusFullData/profiles') #Path Lola\n",
    "\n",
    "# PATH to where the preprocessed files should be appear\n",
    "# PREPROCESSED_PATH = Path('/cw/dtaiproj/ml/2020-FLAIR-VITO/profile-clustering/preprocessed_test/infrax')\n",
    "PREPROCESSED_PATH = Path('/Users/lolabotman/PycharmProjects/FluviusFullData/profiles/preprocessed/infrax')#Path Lola\n",
    "PREPROCESSED_PATH.mkdir(mode = 0o770, parents = True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse code\n",
    "This is simply all the code to parse every kind of dataset (not so clean I know)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting a value to a datetime format  \n",
    "def to_timestamp(index): \n",
    "    return [pyxlsb.convert_date(value) if not np.isnan(value) else value for value in index]\n",
    "\n",
    "# transform the data and save the transformed data using the functions according to the parse data dict\n",
    "def transform_and_save(source_path, name, parse_function): \n",
    "    info_path = PREPROCESSED_PATH / f\"{name}_info.csv\"\n",
    "    data_path = PREPROCESSED_PATH / f\"{name}_data.csv\"\n",
    "    if not( info_path.exists() and data_path.exists()):\n",
    "        try:\n",
    "            info_df, data_df = parse_function(source_path)\n",
    "            info_df.to_csv(info_path)\n",
    "            data_df.to_csv(data_path)\n",
    "            assert info_path.exists() \n",
    "            assert data_path.exists()\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    else: \n",
    "        print(f'{name} already preprocessed')\n",
    "\n",
    "# In these files, the 24 first rows are meta data and the time series starts on row 26\n",
    "# There are 30 EANs and hourly measurements for a year (2015 file and 2014 file)\n",
    "# Hypothesis : measurement values are in kW\n",
    "def read_infrax_gas(path): \n",
    "    df = pd.read_excel(path, header = None, parse_dates = True)\n",
    "    df = df.set_index(df.columns[0])\n",
    "    smart_meter_df = df.T\n",
    "    smart_meter_df = smart_meter_df.set_index('EAN_coded')\n",
    "    \n",
    "    # info_df (we get the meta data from the first 24 columns - table has been transposed)\n",
    "    info_df = smart_meter_df.iloc[:,:24]\n",
    "    info_df = info_df.loc[:, ~ info_df.columns.isna()]\n",
    "    info_df = info_df.dropna(how='all', axis = 1) # drop columns with all NaN's\n",
    "    info_df = info_df.set_index('Jaar', append=True)\n",
    "    \n",
    "    #data_df (we get the actual value data from after column 24 )\n",
    "    data_df = smart_meter_df.iloc[:,24:].copy()\n",
    "    data_df.columns = pd.to_datetime(data_df.columns).round('1min')\n",
    "    data_df['Jaar'] = info_df.Jaar\n",
    "    data_df = data_df.set_index(['Jaar'], append = True)\n",
    "    data_df = data_df.sort_index()\n",
    "    \n",
    "    assert info_df.index.is_unique, 'info_df index should be unique'\n",
    "    assert data_df.index.is_unique, 'data_df index should be unique'\n",
    "\n",
    "    # sort on index\n",
    "    info_df = info_df.sort_index()\n",
    "    data_df = data_df.sort_index()\n",
    "    \n",
    "    \n",
    "    return info_df, data_df \n",
    "\n",
    "# In these files, the 24 first rows are meta data and the time series starts on row 27\n",
    "# there are 8 smart meter ids (EAN)\n",
    "# yearly total is in kWh \n",
    "# measurement values are in kW\n",
    "def read_infrax_heatpump(path): \n",
    "    df = pd.read_excel(path, header = None, parse_dates = True)\n",
    "    df = df.set_index(df.columns[0])\n",
    "    smart_meter_df = df.T\n",
    "    smart_meter_df = smart_meter_df.set_index('EAN_coded')\n",
    "\n",
    "    # info_df \n",
    "    info_df = smart_meter_df.iloc[:,:25]\n",
    "    info_df = info_df.loc[:, ~ info_df.columns.isna()]\n",
    "    info_df = info_df.dropna(how='all', axis = 1) # drop columns with all NaN's\n",
    "    info_df = info_df.set_index('Jaar', append=True)\n",
    "    \n",
    "    #data_df \n",
    "    data_df = smart_meter_df.iloc[:,25:].copy()\n",
    "    data_df.columns = pd.to_datetime(data_df.columns).round('1min')\n",
    "    data_df['Jaar'] = info_df.index.get_level_values('Jaar')\n",
    "    data_df = data_df.set_index(['Jaar'], append = True)\n",
    "    data_df = data_df.sort_index()\n",
    "    \n",
    "    assert info_df.index.is_unique, 'info_df index should be unique'\n",
    "    assert data_df.index.is_unique, 'data_df index should be unique'\n",
    "\n",
    "    # sort on index\n",
    "    info_df = info_df.sort_index()\n",
    "    data_df = data_df.sort_index()\n",
    "    \n",
    "    return info_df, data_df \n",
    "\n",
    "# Yearly total in kwh \n",
    "# hyp : Measurement values in kw >> deduced from the fact that (sum of the measurement) = 4*(jaarverbuik in kwh) + max value given in kW\n",
    "def read_infrax_app_xlsb(path): \n",
    "    # no useful index! \n",
    "    df = pd.read_excel(path, header = None, engine='pyxlsb')\n",
    "    df.set_index(df.columns[0], inplace = True)\n",
    "    smart_meter_df = df.T\n",
    "   \n",
    "    #info df \n",
    "    info_df = (\n",
    "    smart_meter_df\n",
    "        .iloc[:,:5] # info columns\n",
    "        .loc[:,~smart_meter_df.columns[:5].isna()] # drop nan columns\n",
    "    )\n",
    "    info_df = info_df.rename(index=lambda s: 'app2_'+ str(s)) #changing to 'unique' index to not mix up with the app1\n",
    "    info_df['Jaar']=[2014]*len(info_df) ##extract 2014 anoher way ? more generic ?\n",
    "    info_df = info_df.set_index('Jaar', append=True)\n",
    "    info_df = info_df.dropna(how = 'all') # for some reason there are some NaN rows\n",
    "    info_df = info_df.rename(columns={'Max (kW)':'Piek P (kW)'}) #rename such as to have the same column title as the other dfs\n",
    "    \n",
    "    # data_df\n",
    "    data_df = smart_meter_df.iloc[:,5:].copy()\n",
    "    data_df.columns = to_timestamp(data_df.columns)\n",
    "    data_df = data_df.dropna(how = 'all') # for some reason there are some NaN rows\n",
    "    data_df.columns = data_df.columns.round('1min')\n",
    "    data_df = data_df.rename(index=lambda s: 'app2_'+ str(s))\n",
    "    data_df['Jaar'] = [int(2014)]*len(data_df)\n",
    "    data_df = data_df.set_index(['Jaar'], append = True)  \n",
    "    data_df = data_df.loc[:,pd.to_datetime(data_df.columns).year == 2014] #there is one day of 2015 that we don't want to keep\n",
    "    data_df.columns = data_df.columns.map(lambda t: t.replace(year=2016)) #set the columns to 2016 for the final merge \n",
    "    \n",
    "    assert info_df.index.is_unique, 'info_df index should be unique'\n",
    "    assert data_df.index.is_unique, 'data_df index should be unique'\n",
    "\n",
    "    # sort on index\n",
    "    info_df = info_df.sort_index()\n",
    "    data_df = data_df.sort_index()\n",
    "    return info_df, data_df\n",
    "\n",
    "# hyp : all measurement values in kW >> deduced from the fact that (sum of the measurement) = 4*(jaarverbuik in kwh) \n",
    "# jaar verbruik in kWh\n",
    "def read_infrax_app_xlsx(path): \n",
    "    df = pd.read_excel(path, header = None, parse_dates = True)\n",
    "    df = df.set_index(df.columns[0])\n",
    "    smart_meter_df = df.T\n",
    "\n",
    "    # info_df\n",
    "    info_df = smart_meter_df.iloc[:,:7]\n",
    "    info_df = info_df.loc[:, ~ info_df.columns.isna()]\n",
    "    info_df = info_df.rename(index=lambda s: 'app1_'+ str(s)) #changing to 'unique' index to not mix up with the app1\n",
    "    info_df['Jaar']=[int(2014)]*len(info_df) ##extract 2014 anoher way ? more generic ?\n",
    "    info_df = info_df.set_index('Jaar', append=True)\n",
    "    info_df = info_df.rename(columns={'Max (kW)':'Piek P (kW)'})#rename such as to have the same column title as the other dfs\n",
    "    \n",
    "    # data_df\n",
    "    data_df = smart_meter_df.iloc[:,7:].copy()\n",
    "    data_df = data_df.dropna(how = 'all')\n",
    "    data_df.columns = pd.to_datetime(data_df.columns).round('1min')\n",
    "    data_df = data_df.rename(index=lambda s: 'app1_'+ str(s))\n",
    "    data_df['Jaar'] = [2014]*len(data_df)\n",
    "    data_df = data_df.set_index(['Jaar'], append = True)\n",
    "    data_df = data_df.loc[:,pd.to_datetime(data_df.columns).year == 2014] #there is one day of 2015 that we don't want to keep\n",
    "    data_df.columns = data_df.columns.map(lambda t: t.replace(year=2016)) #set the columns to 2016 for the final merge \n",
    "    \n",
    "    assert info_df.index.is_unique, 'info_df index should be unique'\n",
    "    assert data_df.index.is_unique, 'data_df index should be unique'\n",
    "\n",
    "    # sort on index\n",
    "    info_df = info_df.sort_index()\n",
    "    data_df = data_df.sort_index()\n",
    "    return info_df, data_df\n",
    "\n",
    "# all other files ending in _coded.xlsb \n",
    "# measurment values in kW\n",
    "# yearly total in kWh\n",
    "def read_infrax_data(path):\n",
    "    df = pd.read_excel(path, engine='pyxlsb')\n",
    "    df.set_index(df.columns[0], inplace = True)\n",
    "    smart_meter_df = df.T\n",
    "    smart_meter_df.set_index('EAN_coded', inplace = True)\n",
    "\n",
    "\n",
    "    # info df \n",
    "    info_df = (\n",
    "    smart_meter_df\n",
    "        .iloc[:,:21] # info columns\n",
    "        .loc[:,~smart_meter_df.columns[:21].isna()] # drop nan columns\n",
    "        .drop(columns = ['Info installatie', 'Info profiel'])\n",
    "    )\n",
    "\n",
    "    info_df['PV vermogen (kW)'] = info_df['PV vermogen (kW)'].replace('/', np.nan)\n",
    "    info_df = info_df[~ info_df.index.isna()] #remove row with nan index\n",
    "    info_df = info_df.reset_index()\n",
    "    info_df['EAN_coded'] = info_df['EAN_coded'].astype('int')\n",
    "    info_df = info_df.set_index(['EAN_coded', 'Jaar'])\n",
    "\n",
    "    # data df \n",
    "\n",
    "    data_df = smart_meter_df.iloc[:,23:].copy()\n",
    "    data_df.columns = to_timestamp(data_df.columns)\n",
    "    # drop the columns with NaT\n",
    "    data_df = data_df.loc[:,~data_df.columns.isna()]\n",
    "    data_df.columns = data_df.columns.round('1min')\n",
    "    data_df = data_df[~ data_df.index.isna()] #remove row with nan index\n",
    "    data_df = data_df.reset_index()\n",
    "    data_df['EAN_coded'] = data_df['EAN_coded'].astype('int')\n",
    "    data_df['Jaar'] = info_df.index.get_level_values(1)\n",
    "    data_df = data_df.set_index(['EAN_coded','Jaar'])\n",
    "\n",
    "    # Handle the ids 1290 en 1299 that have year 2013 twice \n",
    "    if (1290, 2013) in info_df.index: \n",
    "        new_info_df = info_df.reset_index()\n",
    "        new_info_df.loc[new_info_df['EAN_coded'].isin([1290,1299]) & new_info_df.duplicated(subset = ['EAN_coded', 'Jaar'], keep = 'first'), 'Jaar'] = 2012\n",
    "        info_df = new_info_df.set_index(['EAN_coded', 'Jaar'])\n",
    "        \n",
    "        new_data_df = data_df.reset_index()\n",
    "        new_data_df.loc[new_data_df['EAN_coded'].isin([1290,1299]) & new_data_df.duplicated(subset = ['EAN_coded', 'Jaar'], keep = 'first'), 'Jaar'] = 2012\n",
    "        data_df = new_data_df.set_index(['EAN_coded', 'Jaar'])\n",
    "        \n",
    "   \n",
    "    assert info_df.index.is_unique, 'info_df index should be unique'\n",
    "    assert data_df.index.is_unique, 'data_df index should be unique'\n",
    "\n",
    "    # sort on index\n",
    "    info_df = info_df.sort_index()\n",
    "    data_df = data_df.sort_index()\n",
    "    \n",
    "    return info_df, data_df \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# file information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is which parser function to use for which file \n",
    "parser_functions = {   \n",
    "    'Appartement1': read_infrax_app_xlsx,\n",
    "    'Appartement2': read_infrax_app_xlsb,\n",
    "#     'SLP_profiel S41 2014 (30)_coded': read_infrax_gas,\n",
    "#     'SLP_profiel S41 2015 (30)_coded': read_infrax_gas,\n",
    "    'SLPs_professionelen(348)_coded': read_infrax_data,\n",
    "    'SLPs_residentielen(1675)_coded': read_infrax_data,\n",
    "    'SLPs_residentiëlen(1675)_coded': read_infrax_data,\n",
    "    'Slimme meters met WP (en eventueel PV)_coded': read_infrax_heatpump,\n",
    "    'Slimme meters_professionelen(141)_coded': read_infrax_data,\n",
    "    'Slimme meters_prosumers(123)_coded': read_infrax_data,\n",
    "    'Slimme meters_residentielen(1080)_coded': read_infrax_data, \n",
    "    'Slimme meters_residentiëlen(1080)_coded': read_infrax_data\n",
    "}\n",
    "\n",
    "# this is which preprocessed file name to use \n",
    "new_filename = { \n",
    "    'Appartement1': 'app1',\n",
    "    'Appartement2': 'app2',\n",
    "    'SLP_profiel S41 2014 (30)_coded': 'SLP_gas_2014',\n",
    "    'SLP_profiel S41 2015 (30)_coded': 'SLP_gas_2015',\n",
    "    'SLPs_professionelen(348)_coded': 'SLP_prof',\n",
    "    'SLPs_residentielen(1675)_coded': 'SLP_resid',\n",
    "    'SLPs_residentiëlen(1675)_coded': 'SLP_resid',\n",
    "    'Slimme meters met WP (en eventueel PV)_coded': 'M_heatpump',\n",
    "    'Slimme meters_professionelen(141)_coded': 'M_prof',\n",
    "    'Slimme meters_prosumers(123)_coded': 'M_prosumers',\n",
    "    'Slimme meters_residentielen(1080)_coded': 'M_resid', \n",
    "    'Slimme meters_residentiëlen(1080)_coded': 'M_resid'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse it all :D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infrax_path = DATA_PATH/ \"20171219 Profielen Infrax\"\n",
    "translate = dict()\n",
    "for path in tqdm.tqdm(list(infrax_path.glob('**/*.xlsb'))+ list(infrax_path.glob('**/*.xlsx'))):\n",
    "    print(path)\n",
    "    if path.stem in parser_functions:\n",
    "        new_name = new_filename[path.stem]\n",
    "        parser = parser_functions[path.stem]\n",
    "        transform_and_save(path, new_name, parser)\n",
    "    else:\n",
    "        print('error:'+path.stem)\n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make combined dataframe of relevant profiles\n",
    "So these profiles are all in the same format so we can easily combine these!  \n",
    "I add some extra columns to the info dataframe to ensure that we can later recover the different groups if necessary.  \n",
    "Appartement is excluded and the gas information is excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_info_df():\n",
    "    files = ['M_resid_info.csv', 'SLP_resid_info.csv', 'M_prof_info.csv', 'SLP_prof_info.csv', 'M_prosumers_info.csv', 'M_heatpump_info.csv', 'app1_info.csv', 'app2_info.csv']\n",
    "    files = [PREPROCESSED_PATH/file for file in files]\n",
    "\n",
    "\n",
    "    M_heatpump = pd.read_csv(files[5], index_col = [0,1])\n",
    "    M_heatpump['heatpump'] = True\n",
    "\n",
    "\n",
    "    M_prosumers = pd.read_csv(files[4], index_col = [0,1])\n",
    "    M_prosumers = M_prosumers.dropna(how='all')\n",
    "    M_prosumers['prosumer'] = True\n",
    "\n",
    "    \n",
    "    M_prof_df = pd.read_csv(files[2], index_col = [0,1])\n",
    "    SLP_prof_df = pd.read_csv(files[3], index_col = [0,1])\n",
    "\n",
    "\n",
    "    M_resid_df = pd.read_csv(files[0], index_col = [0,1])\n",
    "    SLP_resid_df = pd.read_csv(files[1], index_col = [0,1])\n",
    "    \n",
    "    app1_df = pd.read_csv(files[6], index_col =[0,1])\n",
    "    app1_df['R/P']=['app1']*len(app1_df.index)\n",
    "    \n",
    "    app2_df = pd.read_csv(files[7], index_col = [0,1])\n",
    "    app2_df['R/P']=['app2']*len(app2_df.index)\n",
    "\n",
    "    infrax = pd.concat([M_resid_df, SLP_resid_df, M_prof_df, SLP_prof_df, M_heatpump, M_prosumers, app1_df, app2_df]).sort_index()\n",
    "    infrax.to_csv(PREPROCESSED_PATH/'combined_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (PREPROCESSED_PATH/'combined_info.csv').exists(): \n",
    "    combined_info_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = pd.read_csv(PREPROCESSED_PATH/'combined_info.csv')\n",
    "comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE = True\n",
    "if not (PREPROCESSED_PATH/'combined_data.csv').exists() or OVERWRITE: \n",
    "    files = ['M_resid_info.csv', 'SLP_resid_info.csv', 'M_prof_info.csv', 'SLP_prof_info.csv', 'M_prosumers_info.csv', 'M_heatpump_info.csv','app1_info.csv', 'app2_info.csv']\n",
    "    profile_files = [PREPROCESSED_PATH/ f'{file[:-8]}data.csv' for file in files]\n",
    "    combined_data_df = pd.concat([pd.read_csv(file, index_col = [0,1]) for file in profile_files] )\n",
    "    combined_data_df = combined_data_df.dropna(how='all', axis = 0).sort_index()\n",
    "    combined_data_df = combined_data_df.reset_index()\n",
    "    combined_data_df['Jaar'] = combined_data_df['Jaar'].astype('int')\n",
    "    combined_data_df = combined_data_df.set_index(['EAN_coded','Jaar'])\n",
    "    #combined_data_df.to_csv(PREPROCESSED_PATH/'combined_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data_df.loc[1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data_df.reset_index().sort_values('Jaar')['Jaar'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how the DST has been treated \n",
    "\n",
    "* 2010 : '2010-03-28 00:02:00' & '2010-10-31 00:02:00'\n",
    "* 2011 : '2011-03-27 00:02:00' & '2011-10-30 00:02:00'\n",
    "* 2012 : '2012-03-25 00:02:00' & '2012-10-28 00:02:00'\n",
    "* 2013 : '2013-03-31 00:02:00' & '2013-10-27 00:02:00'\n",
    "* 2014 : '2014-03-30 00:02:00' & '2014-10-26 00:02:00'\n",
    "* 2015 : '2015-03-29 00:02:00' & '2015-10-25 00:02:00'\n",
    "* 2016 : '2016-03-27 00:02:00' & '2016-10-30 00:02:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the 2 appartment files have not been treated, because we can still notice the artefacts (duplicate values in october and missing values in march) >> let's remove this before checking the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app1_file = pd.read_csv(PREPROCESSED_PATH/'app1_info.csv', index_col = 0)\n",
    "app2_file = pd.read_csv(PREPROCESSED_PATH/'app2_info.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_id_apps = list(app1_file.index) + list(app2_file.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_combined_data_df = combined_data_df.drop(index=sm_id_apps).dropna(axis='columns', how = 'all', inplace=False)\n",
    "new_combined_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(data_df):\n",
    "    data_df.columns = pd.to_datetime(data_df.columns).round('1min')\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make df per year >> DST changes on different times. \n",
    "\n",
    "sm_2010 = parse_dates(new_combined_data_df.reset_index()[new_combined_data_df.reset_index().Jaar == 2010].drop(columns='Jaar').set_index('EAN_coded'))\n",
    "sm_2011 = parse_dates(new_combined_data_df.reset_index()[new_combined_data_df.reset_index().Jaar == 2011].drop(columns='Jaar').set_index('EAN_coded'))\n",
    "sm_2012 = parse_dates(new_combined_data_df.reset_index()[new_combined_data_df.reset_index().Jaar == 2012].drop(columns='Jaar').set_index('EAN_coded'))\n",
    "sm_2013 = parse_dates(new_combined_data_df.reset_index()[new_combined_data_df.reset_index().Jaar == 2013].drop(columns='Jaar').set_index('EAN_coded'))\n",
    "sm_2014 = parse_dates(new_combined_data_df.reset_index()[new_combined_data_df.reset_index().Jaar == 2014].drop(columns='Jaar').set_index('EAN_coded'))\n",
    "sm_2015 = parse_dates(new_combined_data_df.reset_index()[new_combined_data_df.reset_index().Jaar == 2015].drop(columns='Jaar').set_index('EAN_coded'))\n",
    "sm_2016 = parse_dates(new_combined_data_df.reset_index()[new_combined_data_df.reset_index().Jaar == 2016].drop(columns='Jaar').set_index('EAN_coded'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#year is set to 2016 because we used this year to be coherent even with differnt profiles in different years\n",
    "#Lets define the change of date in each year \n",
    "mar_2010, oct_2010 = pd.to_datetime('2016-03-28 02:00:00') , pd.to_datetime('2016-10-31 02:00:00')\n",
    "mar_2011, oct_2011 = pd.to_datetime('2016-03-27 02:00:00') , pd.to_datetime('2016-10-30 02:00:00')\n",
    "mar_2012, oct_2012 = pd.to_datetime('2016-03-25 02:00:00') , pd.to_datetime('2016-10-28 02:00:00')\n",
    "mar_2013, oct_2013 = pd.to_datetime('2016-03-31 02:00:00') , pd.to_datetime('2016-10-27 02:00:00')\n",
    "mar_2014, oct_2014 = pd.to_datetime('2016-03-30 02:00:00') , pd.to_datetime('2016-10-26 02:00:00')\n",
    "mar_2015, oct_2015 = pd.to_datetime('2016-03-29 02:00:00') , pd.to_datetime('2016-10-25 02:00:00')\n",
    "mar_2016, oct_2016 = pd.to_datetime('2016-03-27 02:00:00') , pd.to_datetime('2016-10-30 02:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONGOING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_selection(df,start_missing_hour):\n",
    "    \"\"\"\n",
    "        INPUT \n",
    "        df = dataframe with date as columns (in datetime format) and smart meter id as rows \n",
    "        date = timestamp of the start of the missing hour in datetime format\n",
    "\n",
    "        OUTPUT\n",
    "        the start timestamp and end timestamp of the missing hour \n",
    "        df with NaNs instead of the originally potentially missing hour\n",
    "        original sample df, englobing the potentially originally missing hour and one extra hour before and after\n",
    "        nan sample df, englobing the missing hour (replaced with NaNs) and one extra hour before and after\n",
    "    \"\"\"\n",
    "    #potentially missing hour\n",
    "    end_missing_hour = start_missing_hour+pd.Timedelta(minutes=45)\n",
    "    \n",
    "    #full df with the potentially missing hour replaced with NaNs\n",
    "    df_copy = df.copy()\n",
    "    df_copy.at[:,start_missing_hour:end_missing_hour]=np.nan\n",
    "    df_nan = df_copy.copy()\n",
    "    \n",
    "    #sample one hour before and after the potentially missing hour >> Original + with NaN\n",
    "    start_sample = start_missing_hour - 4*pd.DateOffset(minutes=15)\n",
    "    end_sample = end_missing_hour + 4*pd.DateOffset(minutes=15)\n",
    "    original_sample_df = df.loc[:,start_sample: end_sample].copy()\n",
    "    nan_sample_df = df_nan.loc[:,start_sample: end_sample].copy()\n",
    "    \n",
    "    return df_nan, original_sample_df, nan_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan, original_sample_df, nan_sample_df = sample_selection(sm_2010, mar_2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_1(original_df, start_missing_hour):\n",
    "    \n",
    "    #FUNCTION : checks if missing hour = (h-1 + h+1)/2\n",
    "    \n",
    "    #INPUT : \n",
    "    # original sample df, englobing the potentially originally missing hour and one extra hour before and after\n",
    "    # nan sample df, englobing the missing hour (replaced with NaNs) and one extra hour before and after\n",
    "    # the two dfs should have the same size !! \n",
    "    \n",
    "    #OUTPUT : \n",
    "    # df of percentage error in interpolation \n",
    "    \n",
    "    df_nan, original_sample_df, nan_sample_df = sample_selection(original_df, start_missing_hour)\n",
    "    \n",
    "    \n",
    "    result = pd.DataFrame(index= ['(h-1 + h+1)/2'],columns = ['is_correct', '%error'])\n",
    "    \n",
    "    interp_sample = pd.DataFrame()\n",
    "    for sm_id in nan_sample_df.index:\n",
    "        sm_serie = nan_sample_df.loc[sm_id].copy()\n",
    "        for i in range(0,4):\n",
    "            sm_serie.iat[i+4] = (sm_serie[i]+sm_serie[i+8])/2\n",
    "            sm_df = pd.DataFrame(sm_serie).T\n",
    "        interp_sample = interp_sample.append(sm_df)\n",
    "\n",
    "    bool_output = interp_sample == original_sample_df\n",
    "    unique, counts = np.unique(bool_output, return_counts=True)\n",
    "    nb_nans_to_guess = 4*len(original_sample_df.index)\n",
    "    false_guess = counts[0]\n",
    "    percentage_error = false_guess/nb_nans_to_guess\n",
    "    if percentage_error == 0 :\n",
    "        #print('All NaNs have been interpolated correctly. This hypothesis is confirmed. The missing hour has been interpolated using an everage of the hour before and after.')\n",
    "        result.at['(h-1 + h+1)/2','is_correct'] = True\n",
    "        result.at['(h-1 + h+1)/2','%error'] = percentage_error\n",
    "    else :\n",
    "        #print(f'There is {round(percentage_error*100,2)} % of falsly guessed NaNs. This hypothesis is rejected. The missing hour has not been interpolated using an average of the hour before and after.')\n",
    "        result.at['(h-1 + h+1)/2','is_correct'] = False\n",
    "        result.at['(h-1 + h+1)/2','%error'] = percentage_error\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = hyp_1(sm_2010, mar_2010)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_2(original_df, start_missing_hour):\n",
    "    \n",
    "    \n",
    "    #FUNCTION : checks if missing hour = interpolated using one of the chosen methods :\n",
    "    # ‘linear’, ‘time’, ‘index’, ‘values’, 'pad’, ‘nearest’, \n",
    "    # ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘spline’, ‘barycentric’, ‘polynomial’\n",
    "    # ‘krogh’, ‘piecewise_polynomial’, ‘pchip’, ‘akima’, ‘cubicspline’\n",
    "    \n",
    "    #INPUT : \n",
    "    # original sample df, englobing the potentially originally missing hour and one extra hour before and after\n",
    "    # nan sample df, englobing the missing hour (replaced with NaNs) and one extra hour before and after\n",
    "    # the two dfs should have the same size !! \n",
    "    \n",
    "    #OUTPUT : \n",
    "    # percentage error in interpolation according to the method used \n",
    "    \n",
    "    df_nan, original_sample_df, nan_sample_df = sample_selection(original_df, start_missing_hour)\n",
    "    \n",
    "    \n",
    "    method_list = ['linear', 'time', 'index', 'values', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'barycentric', 'polynomial', 'krogh', 'piecewise_polynomial', 'pchip', 'akima', 'spline', 'from_derivatives']\n",
    "    method_order = [0,0,0,0,0,0,0,0,0,0,2,0,2,0,0,2,0]\n",
    "    dict_method_order = dict(zip(method_list,method_order))\n",
    "    \n",
    "    result = pd.DataFrame(index= method_list,columns = ['is_correct', '%error'])\n",
    "    \n",
    "    for chosen_meth in method_list:\n",
    "        \n",
    "        chosen_order = dict_method_order[chosen_meth]\n",
    "        #print(chosen_meth+'_'+str(chosen_order))\n",
    "        \n",
    "        interp_sample = pd.DataFrame()\n",
    "        for sm_id in nan_sample_df.index:\n",
    "            sm_serie = nan_sample_df.loc[sm_id].copy()\n",
    "            if chosen_order == 0:\n",
    "                sm_interp = sm_serie.interpolate(method = chosen_meth, axis=0)\n",
    "            else :\n",
    "                sm_interp = sm_serie.interpolate(method = chosen_meth, axis=0, order = chosen_order)\n",
    "\n",
    "            sm_df = pd.DataFrame(sm_interp).T\n",
    "            interp_sample = interp_sample.append(sm_df)\n",
    "\n",
    "        bool_output = interp_sample == original_sample_df\n",
    "        unique, counts = np.unique(bool_output, return_counts=True)\n",
    "        nb_nans_to_guess = 4*len(original_sample_df.index)\n",
    "        false_guess = counts[0]\n",
    "        percentage_error = false_guess/nb_nans_to_guess\n",
    "        \n",
    "        if percentage_error == 0 :\n",
    "            result.at[chosen_meth,'is_correct'] = True\n",
    "            result.at[chosen_meth,'%error'] = percentage_error\n",
    "            #print(f'All NaNs have been interpolated correctly. This hypothesis is confirmed. The missing hour has been interpolated using the method \"{chosen_meth}\" and order \"{chosen_order}\".')\n",
    "            \n",
    "        else :\n",
    "            result.at[chosen_meth,'is_correct'] = False\n",
    "            result.at[chosen_meth,'%error'] = percentage_error\n",
    "            #print(f'There is {round(percentage_error*100,2)} % of falsly guessed NaNs. \\nThis hypothesis is rejected. \\nThe missing hour has not been interpolated using the method : \"{chosen_meth}\" and order \"{chosen_order}\".\\n') \n",
    "\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = hyp_2(sm_2010, mar_2010)\n",
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_3(original_df, start_missing_hour):\n",
    "\n",
    "    #HYPOTHESIS : missing hour = (h-1)\n",
    "    \n",
    "    #INPUT : \n",
    "    # original sample df, englobing the potentially originally missing hour and one extra hour before and after\n",
    "    # nan sample df, englobing the missing hour (replaced with NaNs) and one extra hour before and after\n",
    "    # the two dfs should have the same size !! \n",
    "    \n",
    "    #OUTPUT : \n",
    "    # percentage error in interpolation according to the method used \n",
    "    \n",
    "    df_nan, original_sample_df, nan_sample_df = sample_selection(original_df, start_missing_hour)\n",
    "\n",
    "    result = pd.DataFrame(index= ['(h-1)'],columns = ['is_correct', '%error'])\n",
    "    \n",
    "    interp_sample = pd.DataFrame()\n",
    "    for sm_id in nan_sample_df.index:\n",
    "        sm_serie = nan_sample_df.loc[sm_id].copy()\n",
    "        for i in range(0,4):\n",
    "            sm_serie.iat[i+4] = sm_serie[i]\n",
    "            sm_df = pd.DataFrame(sm_serie).T\n",
    "        interp_sample = interp_sample.append(sm_df)\n",
    "\n",
    "    bool_output = interp_sample == original_sample_df\n",
    "    unique, counts = np.unique(bool_output, return_counts=True)\n",
    "    nb_nans_to_guess = 4*len(original_sample_df.index)\n",
    "    false_guess = counts[0]\n",
    "    percentage_error = false_guess/nb_nans_to_guess\n",
    "\n",
    "    if percentage_error == 0 :\n",
    "        result.at['(h-1)','is_correct'] = True\n",
    "        result.at['(h-1)','%error'] = percentage_error\n",
    "        #print(f'All NaNs have been interpolated correctly. This hypothesis is confirmed. The missing hour has been filled using the previous hour.')\n",
    "\n",
    "    else :\n",
    "        result.at['(h-1)','is_correct'] = False\n",
    "        result.at['(h-1)','%error'] = percentage_error\n",
    "        #print(f'There is {round(percentage_error*100,2)} % of falsly guessed NaNs. \\nThis hypothesis is rejected. \\nThe missing hour has not been filled using the previous hour\\n') \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output3 = hyp_3(sm_2010, mar_2010)\n",
    "output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_4(original_df, start_missing_hour):\n",
    "    \n",
    "    #HYPOTHESIS : missing hour = (h+1)\n",
    "    \n",
    "    #INPUT : \n",
    "    # original sample df, englobing the potentially originally missing hour and one extra hour before and after\n",
    "    # nan sample df, englobing the missing hour (replaced with NaNs) and one extra hour before and after\n",
    "    # the two dfs should have the same size !! \n",
    "    \n",
    "    #OUTPUT : \n",
    "    # percentage error in interpolation according to the method used \n",
    "    \n",
    "    df_nan, original_sample_df, nan_sample_df = sample_selection(original_df, start_missing_hour)\n",
    "\n",
    "\n",
    "    result = pd.DataFrame(index= ['(h+1)'],columns = ['is_correct', '%error'])\n",
    "    \n",
    "\n",
    "    interp_sample = pd.DataFrame()\n",
    "    for sm_id in nan_sample_df.index:\n",
    "        sm_serie = nan_sample_df.loc[sm_id].copy()\n",
    "        for i in range(0,4):\n",
    "            sm_serie.iat[i+4] = sm_serie[i+8]\n",
    "            sm_df = pd.DataFrame(sm_serie).T\n",
    "        interp_sample = interp_sample.append(sm_df)\n",
    "\n",
    "    bool_output = interp_sample == original_sample_df\n",
    "    unique, counts = np.unique(bool_output, return_counts=True)\n",
    "\n",
    "    nb_nans_to_guess = 4*len(original_sample_df.index)\n",
    "    false_guess = counts[0]\n",
    "    percentage_error = false_guess/nb_nans_to_guess\n",
    "\n",
    "    if percentage_error == 0 :\n",
    "        result.at['(h+1)','is_correct'] = True\n",
    "        result.at['(h+1)','%error'] = percentage_error\n",
    "        #print(f'All NaNs have been interpolated correctly. This hypothesis is confirmed. The missing hour has been filled using the next hour.')\n",
    "\n",
    "    else :\n",
    "        result.at['(h+1)','is_correct'] = False\n",
    "        result.at['(h+1)','%error'] = percentage_error\n",
    "        #print(f'There is {round(percentage_error*100,2)} % of falsly guessed NaNs. \\nThis hypothesis is rejected. \\nThe missing hour has not been filled using the next hour\\n') \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output4 = hyp_4(sm_2010, mar_2010)\n",
    "output4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_5(original_df, start_missing_hour):\n",
    "    \n",
    "    #HYPOTHESIS : missing hour = same hour previous week\n",
    "    \n",
    "    # INPUT:\n",
    "    # original_df = dataframe with date as columns (in datetime format) and smart meter id as rows \n",
    "    # df_nan = same as original_df but the columns of the potentially missing hour have been replaced with nans\n",
    "    # using function : \n",
    "    \n",
    "    df_nan, original_sample_df, nan_sample_df = sample_selection(original_df, start_missing_hour)\n",
    "    \n",
    "    result = pd.DataFrame(index= ['(h-7d)'],columns = ['is_correct', '%error'])\n",
    "    \n",
    "    interp_table = pd.DataFrame()\n",
    "    for sm_id in df_nan.index:\n",
    "        sm_serie = df_nan.loc[sm_id].copy()\n",
    "        for date in pd.date_range(start = start_missing_hour, periods=4,freq='15min'):\n",
    "            sm_serie.at[date] = sm_serie[date-pd.Timedelta(days=7)]\n",
    "            sm_df = pd.DataFrame(sm_serie).T\n",
    "        interp_table = interp_table.append(sm_df)\n",
    "\n",
    "    bool_output = interp_table == original_df\n",
    "    unique, counts = np.unique(bool_output, return_counts=True)\n",
    "    \n",
    "    nb_nans_to_guess = 4*len(original_sample_df.index)\n",
    "    false_guess = counts[0]\n",
    "    percentage_error = false_guess/nb_nans_to_guess\n",
    "\n",
    "    if percentage_error == 0 :\n",
    "        result.at['(h-7d)','is_correct'] = True\n",
    "        result.at['(h-7d)','%error'] = percentage_error\n",
    "        #print(f'All NaNs have been interpolated correctly. This hypothesis is confirmed. The missing hour has been filled using the same hour, one week back.')\n",
    "\n",
    "    else :\n",
    "        result.at['(h-7d)','is_correct'] = False\n",
    "        result.at['(h-7d)','%error'] = percentage_error\n",
    "        #print(f'There is {round(percentage_error*100,2)} % of falsly guessed NaNs. \\nThis hypothesis is rejected. \\nThe missing hour has not been interpolated using the same hour, one week back\\n') \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output5 = hyp_5(sm_2010, mar_2010)\n",
    "output5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_6(original_df, start_missing_hour):\n",
    "    \n",
    "    #HYPOTHESIS : missing hour = same hour next week\n",
    "    \n",
    "    # INPUT:\n",
    "    # original_df = dataframe with date as columns (in datetime format) and smart meter id as rows \n",
    "    # df_nan = same as original_df but the columns of the potentially missing hour have been replaced with nans\n",
    "    # using function : \n",
    "    \n",
    "    df_nan, original_sample_df, nan_sample_df = sample_selection(original_df, start_missing_hour)\n",
    "    \n",
    "    result = pd.DataFrame(index= ['(h+7d)'],columns = ['is_correct', '%error'])\n",
    "    \n",
    "    interp_table = pd.DataFrame()\n",
    "    for sm_id in df_nan.index:\n",
    "        sm_serie = df_nan.loc[sm_id].copy()\n",
    "        for date in pd.date_range(start = start_missing_hour, periods=4,freq='15min'):\n",
    "            sm_serie.at[date] = sm_serie[date+pd.Timedelta(days=7)]\n",
    "            sm_df = pd.DataFrame(sm_serie).T\n",
    "        interp_table = interp_table.append(sm_df)\n",
    "\n",
    "    bool_output = interp_table.loc[:,start_missing_hour:start_missing_hour+pd.Timedelta(minutes=45)] == original_df.loc[:,start_missing_hour:start_missing_hour+pd.Timedelta(minutes=45)]\n",
    "    unique, counts = np.unique(bool_output, return_counts=True)\n",
    "    \n",
    "    nb_nans_to_guess = 4*len(original_sample_df.index)\n",
    "    false_guess = counts[0]\n",
    "    percentage_error = false_guess/nb_nans_to_guess\n",
    "\n",
    "    if percentage_error == 0 :\n",
    "        result.at['(h+7d)','is_correct'] = True\n",
    "        result.at['(h+7d)','%error'] = percentage_error\n",
    "        #print(f'All NaNs have been interpolated correctly. This hypothesis is confirmed. The missing hour has been filled using the same hour, one week ahead.')\n",
    "\n",
    "    else :\n",
    "        result.at['(h+7d)','is_correct'] = False\n",
    "        result.at['(h+7d)','%error'] = percentage_error\n",
    "        #print(f'There is {round(percentage_error*100,2)} % of falsly guessed NaNs. \\nThis hypothesis is rejected. \\nThe missing hour has not been interpolated using the same hour, one week ahead\\n') \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output6 = hyp_6(sm_2010, mar_2010)\n",
    "output6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_7(original_df, start_missing_hour):\n",
    "    \n",
    "    #HYPOTHESIS : missing hour = same hour previous day\n",
    "    \n",
    "    # INPUT:\n",
    "    # original_df = dataframe with date as columns (in datetime format) and smart meter id as rows \n",
    "    # df_nan = same as original_df but the columns of the potentially missing hour have been replaced with nans\n",
    "    # using function : \n",
    "    \n",
    "    df_nan, original_sample_df, nan_sample_df = sample_selection(original_df, start_missing_hour)\n",
    "    \n",
    "    result = pd.DataFrame(index= ['(h-1d)'],columns = ['is_correct', '%error'])\n",
    "    \n",
    "    interp_table = pd.DataFrame()\n",
    "    for sm_id in df_nan.index:\n",
    "        sm_serie = df_nan.loc[sm_id].copy()\n",
    "        for date in pd.date_range(start = start_missing_hour, periods=4,freq='15min'):\n",
    "            sm_serie.at[date] = sm_serie[date-pd.Timedelta(days=1)]\n",
    "            sm_df = pd.DataFrame(sm_serie).T\n",
    "        interp_table = interp_table.append(sm_df)\n",
    "\n",
    "    bool_output = interp_table.loc[:,start_missing_hour:start_missing_hour+pd.Timedelta(minutes=45)] == original_df.loc[:,start_missing_hour:start_missing_hour+pd.Timedelta(minutes=45)]\n",
    "    unique, counts = np.unique(bool_output, return_counts=True)\n",
    "    \n",
    "    nb_nans_to_guess = 4*len(original_sample_df.index)\n",
    "    false_guess = counts[0]\n",
    "    percentage_error = false_guess/nb_nans_to_guess\n",
    "\n",
    "    if percentage_error == 0 :\n",
    "        result.at['(h-1d)','is_correct'] = True\n",
    "        result.at['(h-1d)','%error'] = percentage_error\n",
    "        #print(f'All NaNs have been interpolated correctly. This hypothesis is confirmed. The missing hour has been filled using the same hour, one day back.')\n",
    "\n",
    "    else :\n",
    "        result.at['(h-1d)','is_correct'] = False\n",
    "        result.at['(h-1d)','%error'] = percentage_error\n",
    "        #print(f'There is {round(percentage_error*100,2)} % of falsly guessed NaNs. \\nThis hypothesis is rejected. \\nThe missing hour has not been interpolated using the same hour, one day back\\n') \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output7, interp = hyp_7(sm_2010, mar_2010)\n",
    "output7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_8(original_df, start_missing_hour):\n",
    "    \n",
    "    #HYPOTHESIS : missing hour = same hour next day\n",
    "    \n",
    "    # INPUT:\n",
    "    # original_df = dataframe with date as columns (in datetime format) and smart meter id as rows \n",
    "    # df_nan = same as original_df but the columns of the potentially missing hour have been replaced with nans\n",
    "    # using function : \n",
    "    \n",
    "    df_nan, original_sample_df, nan_sample_df = sample_selection(original_df, start_missing_hour)\n",
    "    \n",
    "    result = pd.DataFrame(index= ['(h+1d)'],columns = ['is_correct', '%error'])\n",
    "    \n",
    "    interp_table = pd.DataFrame()\n",
    "    for sm_id in df_nan.index:\n",
    "        sm_serie = df_nan.loc[sm_id].copy()\n",
    "        for date in pd.date_range(start = start_missing_hour, periods=4,freq='15min'):\n",
    "            sm_serie.at[date] = sm_serie[date+pd.Timedelta(days=1)]\n",
    "            sm_df = pd.DataFrame(sm_serie).T\n",
    "        interp_table = interp_table.append(sm_df)\n",
    "\n",
    "    bool_output = interp_table.loc[:,start_missing_hour:start_missing_hour+pd.Timedelta(minutes=45)] == original_df.loc[:,start_missing_hour:start_missing_hour+pd.Timedelta(minutes=45)]\n",
    "    unique, counts = np.unique(bool_output, return_counts=True)\n",
    "    \n",
    "    nb_nans_to_guess = 4*len(original_sample_df.index)\n",
    "    false_guess = counts[0]\n",
    "    percentage_error = false_guess/nb_nans_to_guess\n",
    "\n",
    "    if percentage_error == 0 :\n",
    "        result.at['(h+1d)','is_correct'] = True\n",
    "        result.at['(h+1d)','%error'] = percentage_error\n",
    "        #print(f'All NaNs have been interpolated correctly. This hypothesis is confirmed. The missing hour has been filled using the same hour, one day ahead.')\n",
    "\n",
    "    else :\n",
    "        result.at['(h+1d)','is_correct'] = False\n",
    "        result.at['(h+1d)','%error'] = percentage_error\n",
    "        #print(f'There is {round(percentage_error*100,2)} % of falsly guessed NaNs. \\nThis hypothesis is rejected. \\nThe missing hour has not been interpolated using the same hour, one day ahead\\n') \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output8 = hyp_8(sm_2010, mar_2010)\n",
    "output8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_9(original_df, start_missing_hour):\n",
    "    \n",
    "    #HYPOTHESIS : missing hour = average same hour next week + same hour last week \n",
    "    \n",
    "    # INPUT:\n",
    "    # original_df = dataframe with date as columns (in datetime format) and smart meter id as rows \n",
    "    # df_nan = same as original_df but the columns of the potentially missing hour have been replaced with nans\n",
    "    # using function : \n",
    "    \n",
    "    df_nan, original_sample_df, nan_sample_df = sample_selection(original_df, start_missing_hour)\n",
    "    \n",
    "    result = pd.DataFrame(index= ['((h+7d)+(h-7d))/2'],columns = ['is_correct', '%error'])\n",
    "    \n",
    "    interp_table = pd.DataFrame()\n",
    "    for sm_id in df_nan.index:\n",
    "        sm_serie = df_nan.loc[sm_id].copy()\n",
    "        for date in pd.date_range(start = start_missing_hour, periods=4,freq='15min'):\n",
    "            sm_serie.at[date] = (sm_serie[date+pd.Timedelta(days=7)] + sm_serie[date-pd.Timedelta(days=7)])/2\n",
    "            sm_df = pd.DataFrame(sm_serie).T\n",
    "        interp_table = interp_table.append(sm_df)\n",
    "\n",
    "    bool_output = interp_table.loc[:,start_missing_hour:start_missing_hour+pd.Timedelta(minutes=45)] == original_df.loc[:,start_missing_hour:start_missing_hour+pd.Timedelta(minutes=45)]\n",
    "    unique, counts = np.unique(bool_output, return_counts=True)\n",
    "    \n",
    "    nb_nans_to_guess = 4*len(original_sample_df.index)\n",
    "    false_guess = counts[0]\n",
    "    percentage_error = false_guess/nb_nans_to_guess\n",
    "\n",
    "    if percentage_error == 0 :\n",
    "        result.at['((h+7d)+(h-7d))/2','is_correct'] = True\n",
    "        result.at['((h+7d)+(h-7d))/2','%error'] = percentage_error\n",
    "        #print(f'All NaNs have been interpolated correctly. This hypothesis is confirmed. The missing hour has been filled using the average of same hour, one week ahead and one week back.')\n",
    "\n",
    "    else :\n",
    "        result.at['((h+7d)+(h-7d))/2','is_correct'] = False\n",
    "        result.at['((h+7d)+(h-7d))/2','%error'] = percentage_error\n",
    "        #print(f'There is {round(percentage_error*100,2)} % of falsly guessed NaNs. \\nThis hypothesis is rejected. \\nThe missing hour has not been filled using the average of same hour, one week ahead and one week back\\n') \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output9 = hyp_9(sm_2010, mar_2010)\n",
    "output9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_10(original_df, start_missing_hour):\n",
    "    \n",
    "    #HYPOTHESIS : missing hour = average same hour next day + same hour previous day \n",
    "    \n",
    "    # INPUT:\n",
    "    # original_df = dataframe with date as columns (in datetime format) and smart meter id as rows \n",
    "    # df_nan = same as original_df but the columns of the potentially missing hour have been replaced with nans\n",
    "    # using function : \n",
    "    \n",
    "    df_nan, original_sample_df, nan_sample_df = sample_selection(original_df, start_missing_hour)\n",
    "    \n",
    "    result = pd.DataFrame(index= ['((h+1d)+(h-1d))/2'],columns = ['is_correct', '%error'])\n",
    "    \n",
    "    interp_table = pd.DataFrame()\n",
    "    for sm_id in df_nan.index:\n",
    "        sm_serie = df_nan.loc[sm_id].copy()\n",
    "        for date in pd.date_range(start = start_missing_hour, periods=4,freq='15min'):\n",
    "            sm_serie.at[date] = (sm_serie[date+pd.Timedelta(days=1)] + sm_serie[date-pd.Timedelta(days=1)])/2\n",
    "            sm_df = pd.DataFrame(sm_serie).T\n",
    "        interp_table = interp_table.append(sm_df)\n",
    "\n",
    "    bool_output = interp_table.loc[:,start_missing_hour:start_missing_hour+pd.Timedelta(minutes=45)] == original_df.loc[:,start_missing_hour:start_missing_hour+pd.Timedelta(minutes=45)]\n",
    "    unique, counts = np.unique(bool_output, return_counts=True)\n",
    "    \n",
    "    nb_nans_to_guess = 4*len(original_sample_df.index)\n",
    "    false_guess = counts[0]\n",
    "    percentage_error = false_guess/nb_nans_to_guess\n",
    "\n",
    "    if percentage_error == 0 :\n",
    "        result.at['((h+1d)+(h-1d))/2','is_correct'] = True\n",
    "        result.at['((h+1d)+(h-1d))/2','%error'] = percentage_error\n",
    "        #print(f'All NaNs have been interpolated correctly. This hypothesis is confirmed. The missing hour has been filled using the average of same hour, one day ahead and one day back.')\n",
    "\n",
    "    else :\n",
    "        result.at['((h+1d)+(h-1d))/2','is_correct'] = False\n",
    "        result.at['((h+1d)+(h-1d))/2','%error'] = percentage_error\n",
    "        #print(f'There is {round(percentage_error*100,2)} % of falsly guessed NaNs. \\nThis hypothesis is rejected. \\nThe missing hour has not been filled using the average of same hour, one day ahead and one day back\\n') \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output10 = hyp_10(sm_2010, mar_2010)\n",
    "output10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_hyp(original_df, start_missing_hour):\n",
    "    glob_result = pd.concat([hyp_1(original_df, start_missing_hour), hyp_2(original_df, start_missing_hour), hyp_3(original_df, start_missing_hour), hyp_4(original_df, start_missing_hour), hyp_5(original_df, start_missing_hour), hyp_6(original_df, start_missing_hour), hyp_7(original_df, start_missing_hour), hyp_8(original_df, start_missing_hour), hyp_9(original_df, start_missing_hour), hyp_10(original_df, start_missing_hour)])\n",
    "    return glob_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2010 = all_hyp(sm_2010,mar_2010)\n",
    "result_2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2011 = all_hyp(sm_2011,mar_2011)\n",
    "result_2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2012 = all_hyp(sm_2012,mar_2012)\n",
    "result_2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2013 = all_hyp(sm_2013,mar_2013)\n",
    "result_2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2014 = all_hyp(sm_2014,mar_2014)\n",
    "result_2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2015 = all_hyp(sm_2015,mar_2015)\n",
    "result_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2016 = all_hyp(sm_2016,mar_2016)\n",
    "result_2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
