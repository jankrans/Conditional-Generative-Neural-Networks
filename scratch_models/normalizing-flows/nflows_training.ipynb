{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HYYoQ4WBpjUr"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o344x9rX-_gX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "import sys\n",
        "sys.path.append('../..')\n",
        "from nflows.flows.base import Flow\n",
        "from nflows.distributions.normal import ConditionalDiagonalNormal, StandardNormal\n",
        "from nflows.transforms.base import CompositeTransform\n",
        "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n",
        "from nflows.transforms.permutations import ReversePermutation\n",
        "from nflows.transforms.base import CompositeTransform\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import scripts.utils as utils"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2abgDK-tp-jF"
      },
      "source": [
        "Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8HaM-XoiKhKU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([45656, 10]) torch.Size([45656, 96]) torch.Size([11414, 10]) torch.Size([11414, 96])\n"
          ]
        }
      ],
      "source": [
        "train_y, train_x = utils.get_training_data()\n",
        "test_y, test_x = utils.get_test_data()\n",
        "train_y = utils.normalize_numpy(train_y, False, True)\n",
        "test_y = utils.normalize_numpy(test_y, False, True)\n",
        "train_y = torch.tensor(train_y).float()\n",
        "train_x = torch.tensor(train_x).float()\n",
        "test_y = torch.tensor(test_y).float()\n",
        "test_x = torch.tensor(test_x).float()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
        "\n",
        "print(train_x.shape,train_y.shape,test_x.shape,test_y.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h-dWYECYqQN5"
      },
      "source": [
        "Parameter setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 50000\n",
        "\n",
        "num_layers = 5\n",
        "base_dist = StandardNormal([96])\n",
        "#base_dist = ConditionalDiagonalNormal([96],context_encoder=model)\n",
        "transforms = []\n",
        "\n",
        "for _ in range(num_layers):\n",
        "    transforms.append(ReversePermutation(features=96))\n",
        "    transforms.append(MaskedAffineAutoregressiveTransform(features=96,\n",
        "                                                          hidden_features=256,\n",
        "                                                          context_features=10))\n",
        "    \n",
        "transform = CompositeTransform(transforms)\n",
        "flow = Flow(transform, base_dist).cuda()\n",
        "optimizer = optim.Adam(flow.parameters())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uo6vIglb_MXh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration 0 : 262.7591552734375 : 262.76025390625\n",
            "iteration 100 : -182.32083129882812 : 58.32097244262695\n",
            "iteration 200 : -221.04750061035156 : 29.12255859375\n",
            "iteration 300 : -245.12103271484375 : -46.42838668823242\n",
            "iteration 400 : -238.00796508789062 : -46.597084045410156\n",
            "iteration 500 : -250.22068786621094 : 274.6990966796875\n",
            "iteration 600 : -234.407958984375 : -50.74391174316406\n",
            "iteration 700 : -255.08331298828125 : -9.94276237487793\n",
            "iteration 800 : -269.9414367675781 : -34.461421966552734\n",
            "iteration 900 : -274.62890625 : -52.26594543457031\n",
            "iteration 1000 : -263.5885314941406 : -71.35167694091797\n",
            "iteration 1100 : -274.20623779296875 : -63.89247131347656\n",
            "iteration 1200 : -276.9622497558594 : -40.918209075927734\n",
            "iteration 1300 : -273.0816650390625 : -21.534099578857422\n",
            "iteration 1400 : -278.00274658203125 : 170.95831298828125\n",
            "iteration 1500 : -277.7588806152344 : 1047.81640625\n",
            "iteration 1600 : -283.6842041015625 : 383.7525939941406\n",
            "iteration 1700 : -289.914306640625 : 4148.45361328125\n",
            "iteration 1800 : -295.0802307128906 : 215.31448364257812\n",
            "iteration 1900 : -291.14495849609375 : 2989.337890625\n",
            "iteration 2000 : -289.55450439453125 : 18560.197265625\n",
            "iteration 2100 : -298.9336242675781 : 8978.9580078125\n",
            "iteration 2200 : -301.2140197753906 : 5784.20263671875\n",
            "iteration 2300 : -296.4696350097656 : 1097.9210205078125\n",
            "iteration 2400 : -302.7611389160156 : 21510.328125\n",
            "iteration 2500 : -304.6073303222656 : 42178.359375\n",
            "iteration 2600 : -307.33148193359375 : 70059.8125\n",
            "iteration 2700 : -306.2939758300781 : 77273.640625\n",
            "iteration 2800 : -301.97845458984375 : 86181.71875\n",
            "iteration 2900 : -300.87835693359375 : 35104.7421875\n",
            "iteration 3000 : -306.88079833984375 : 59356.1875\n",
            "iteration 3100 : -302.3868408203125 : 139441.640625\n",
            "iteration 3200 : -306.4891357421875 : 4955.48681640625\n",
            "iteration 3300 : -309.9066467285156 : 10685.7587890625\n",
            "iteration 3400 : -311.7934265136719 : 35995.95703125\n",
            "iteration 3500 : -315.897705078125 : 2344.738525390625\n",
            "iteration 3600 : -311.44268798828125 : 5937.48291015625\n",
            "iteration 3700 : -311.9917297363281 : 12742.8046875\n",
            "iteration 3800 : -318.8433837890625 : 7739.20458984375\n",
            "iteration 3900 : -319.8349914550781 : 16544.6875\n",
            "iteration 4000 : -322.1513977050781 : 7500.6357421875\n",
            "iteration 4100 : -320.92657470703125 : 27159.162109375\n",
            "iteration 4200 : -319.18756103515625 : 69595.0234375\n",
            "iteration 4300 : -317.6310119628906 : 2543.402587890625\n",
            "iteration 4400 : -323.26483154296875 : 83082.359375\n",
            "iteration 4500 : -317.4668273925781 : 42538.3828125\n",
            "iteration 4600 : -321.7620849609375 : 183296.453125\n",
            "iteration 4700 : -319.5739440917969 : 3146285.25\n",
            "iteration 4800 : -320.4263610839844 : 28275.64453125\n",
            "iteration 4900 : -318.03125 : 5621.12646484375\n",
            "iteration 5000 : -330.4034423828125 : 23316.16796875\n",
            "iteration 5100 : -328.3390197753906 : 557249664.0\n",
            "iteration 5200 : -321.4969177246094 : 226596080.0\n",
            "iteration 5300 : -327.50347900390625 : 98853952.0\n",
            "iteration 5400 : -320.80926513671875 : 515666048.0\n",
            "iteration 5500 : -325.7724914550781 : 48882916.0\n",
            "iteration 5600 : -324.2190856933594 : 46935576.0\n",
            "iteration 5700 : -328.754638671875 : 3226943232.0\n",
            "iteration 5800 : -327.791015625 : 15349681152.0\n",
            "iteration 5900 : -330.6996154785156 : 146170160.0\n",
            "iteration 6000 : -330.1993713378906 : 2881952768.0\n",
            "iteration 6100 : -323.69549560546875 : 1124015104.0\n",
            "iteration 6200 : -330.7246398925781 : 20942280704.0\n",
            "iteration 6300 : -335.5856018066406 : 1880871936.0\n",
            "iteration 6400 : -332.6175231933594 : 26527721472.0\n",
            "iteration 6500 : -328.72528076171875 : 5800660.0\n",
            "iteration 6600 : -324.1138610839844 : 233835248.0\n",
            "iteration 6700 : -326.6380920410156 : 552868.0\n",
            "iteration 6800 : -334.023193359375 : 8119144448.0\n",
            "iteration 6900 : -332.7117004394531 : 154437088.0\n",
            "iteration 7000 : -335.7510986328125 : 78215094272.0\n",
            "iteration 7100 : -334.178955078125 : 99327992.0\n",
            "iteration 7200 : -327.74591064453125 : 432274944.0\n",
            "iteration 7300 : -329.5289611816406 : 315327040.0\n",
            "iteration 7400 : -338.2376403808594 : 707093888.0\n",
            "iteration 7500 : -331.2388916015625 : 271109.03125\n",
            "iteration 7600 : -329.76104736328125 : 13037362.0\n",
            "iteration 7700 : -328.1265869140625 : 478030.21875\n",
            "iteration 7800 : -333.4325256347656 : 7745561.0\n",
            "iteration 7900 : -341.3664245605469 : 20005591040.0\n",
            "iteration 8000 : -339.7449035644531 : 26297.169921875\n",
            "iteration 8100 : -336.96417236328125 : 32970714.0\n",
            "iteration 8200 : -338.2733154296875 : 1620797.625\n",
            "iteration 8300 : -338.110595703125 : 278312.34375\n",
            "iteration 8400 : -338.6913146972656 : 329522528.0\n",
            "iteration 8500 : -335.8831481933594 : 9614.5078125\n",
            "iteration 8600 : -322.7364501953125 : 19562.13671875\n",
            "iteration 8700 : -336.7723388671875 : 2470961.75\n",
            "iteration 8800 : -338.5068359375 : 10298100.0\n",
            "iteration 8900 : -337.3452453613281 : 3575123.75\n",
            "iteration 9000 : -342.299072265625 : 539801216.0\n",
            "iteration 9100 : -335.9922790527344 : 15801838.0\n",
            "iteration 9200 : -339.9580993652344 : 11227971.0\n",
            "iteration 9300 : -335.33038330078125 : 470.1632080078125\n",
            "iteration 9400 : -341.5653991699219 : 5480643.5\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mflow\u001b[39m.\u001b[39mlog_prob(inputs\u001b[39m=\u001b[39mtrain_y, context\u001b[39m=\u001b[39mtrain_x)\u001b[39m.\u001b[39mmean()\n\u001b[0;32m      9\u001b[0m test_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mflow\u001b[39m.\u001b[39mlog_prob(inputs\u001b[39m=\u001b[39mtest_y, context\u001b[39m=\u001b[39mtest_x)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m---> 10\u001b[0m writer\u001b[39m.\u001b[39;49madd_scalar(\u001b[39m\"\u001b[39;49m\u001b[39mLoss/train\u001b[39;49m\u001b[39m\"\u001b[39;49m,loss,i)\n\u001b[0;32m     11\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mLoss/test\u001b[39m\u001b[39m\"\u001b[39m,test_loss,i)\n\u001b[0;32m     12\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39m\u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\jankr\\miniconda3\\envs\\NF\\lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:388\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[1;34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mcaffe2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m workspace\n\u001b[0;32m    386\u001b[0m     scalar_value \u001b[39m=\u001b[39m workspace\u001b[39m.\u001b[39mFetchBlob(scalar_value)\n\u001b[1;32m--> 388\u001b[0m summary \u001b[39m=\u001b[39m scalar(\n\u001b[0;32m    389\u001b[0m     tag, scalar_value, new_style\u001b[39m=\u001b[39;49mnew_style, double_precision\u001b[39m=\u001b[39;49mdouble_precision\n\u001b[0;32m    390\u001b[0m )\n\u001b[0;32m    391\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_file_writer()\u001b[39m.\u001b[39madd_summary(summary, global_step, walltime)\n",
            "File \u001b[1;32mc:\\Users\\jankr\\miniconda3\\envs\\NF\\lib\\site-packages\\torch\\utils\\tensorboard\\summary.py:283\u001b[0m, in \u001b[0;36mscalar\u001b[1;34m(name, tensor, collections, new_style, double_precision)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscalar\u001b[39m(name, tensor, collections\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, new_style\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, double_precision\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    268\u001b[0m     \u001b[39m\"\"\"Outputs a `Summary` protocol buffer containing a single scalar value.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[39m    The generated Summary has a Tensor.proto containing the input Tensor.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[39m      ValueError: If tensor has the wrong shape or type.\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 283\u001b[0m     tensor \u001b[39m=\u001b[39m make_np(tensor)\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m    284\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m    285\u001b[0m         tensor\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    286\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTensor should contain one element (0 dimensions). Was given size: \u001b[39m\u001b[39m{\u001b[39;00mtensor\u001b[39m.\u001b[39msize\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mtensor\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m dimensions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     \u001b[39m# python float is double precision in numpy\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\jankr\\miniconda3\\envs\\NF\\lib\\site-packages\\torch\\utils\\tensorboard\\_convert_np.py:23\u001b[0m, in \u001b[0;36mmake_np\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([x])\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m _prepare_pytorch(x)\n\u001b[0;32m     24\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mGot \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, but numpy array, torch tensor, or caffe2 blob name are expected.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     26\u001b[0m         \u001b[39mtype\u001b[39m(x)\n\u001b[0;32m     27\u001b[0m     )\n\u001b[0;32m     28\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\jankr\\miniconda3\\envs\\NF\\lib\\site-packages\\torch\\utils\\tensorboard\\_convert_np.py:32\u001b[0m, in \u001b[0;36m_prepare_pytorch\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prepare_pytorch\u001b[39m(x):\n\u001b[1;32m---> 32\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "run = 'run3'\n",
        "output_path = r'C:\\Thesis\\scripts\\normalizing-flows\\runs'\n",
        "writer = SummaryWriter(os.path.join(output_path,run))\n",
        "\n",
        "for i in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = -flow.log_prob(inputs=train_y, context=train_x).mean()\n",
        "    test_loss = -flow.log_prob(inputs=test_y, context=test_x).mean()\n",
        "    writer.add_scalar(\"Loss/train\",loss,i)\n",
        "    writer.add_scalar(\"Loss/test\",test_loss,i)\n",
        "    if i%100 == 0:\n",
        "      print('iteration', i,':',loss.item(),':',test_loss.item())\n",
        "      output_model = os.path.join(output_path,run,f'model-{i}')\n",
        "      torch.save(flow,output_model)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "wind power - normalizing flow",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "NF",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "baa695504aaae1bc3cfc6a1d4d799bb53234c2ecf54ef761e4260958faee5131"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
